<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kohonen on GID Webpage</title>
    <link>https://gdetor.github.io/tags/kohonen/</link>
    <description>Recent content in Kohonen on GID Webpage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ⓒ  GID, 2021-2022</copyright>
    <lastBuildDate>Fri, 29 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gdetor.github.io/tags/kohonen/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>An introduction to self-organizing maps</title>
      <link>https://gdetor.github.io/posts/som/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/som/</guid>
      <description>This post presents the classical self-organizing map algorithm proposed by Grossberg [1] and Teuvo Kohonen [2]. We explain the fundamental aspects of the algorithm and its applications, and we offer a basic implementation in Pytorch.
Introduction Let us begin with a prevalent problem in science. We often have to deal with data that live in a high-dimensional space $\mathcal{X} \in \mathbb{R}^k$. When $k &amp;gt; 3$, things get rough for people who need help to visualize what&amp;rsquo;s happening.</description>
      <content>&lt;p&gt;This post presents the classical self-organizing map algorithm proposed by
Grossberg [1] and Teuvo Kohonen [2]. We explain the fundamental aspects of the
algorithm and its applications, and we offer a basic implementation in Pytorch.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let us begin with a prevalent problem in science. We often have to deal with
data that &lt;em&gt;live&lt;/em&gt; in a high-dimensional space $\mathcal{X} \in \mathbb{R}^k$.
When $k &amp;gt; 3$, things get rough for people who need help to visualize
what&amp;rsquo;s happening. Moreover, many algorithms we use daily cannot operate fast
enough in such high-dimensional spaces [3, 4]. Therefore, we rely on methods
that reduce the dimensionality without compromising or losing much information.&lt;/p&gt;
&lt;p&gt;One such method is a self-organizing map or SOM. SOM is an unsupervised
learning algorithm that maps a high-dimensional space into a lower-dimensional
one through an artificial neural network.
In a more mathematical context, the idea behind an unsupervised learning
problem and particularly a self-organizing map, is to learn the input
distribution, meaning we are looking at an approximation for the joint
distribution $p(x, y)$, where $x$ represents input samples and $y$ the output,
without making any assumptions about causality.&lt;/p&gt;
&lt;h3 id=&#34;applications&#34;&gt;Applications&lt;/h3&gt;
&lt;p&gt;We have already seen the dimensionality reduction problem, where the data we
handle are usually high-dimensional, rendering their analysis, processing, or
interpretation hard. Therefore, we can use a SOM to map the original input
space to a low-dimensional space and work with the reduced data [2].
Another feature of the SOM is that it retains the topographic properties of the
input distribution (space). The code words are topographically organized,
meaning that the neighboring units of the map represent data that are nearby in
the input space.
For instance, classifying world poverty is a problem where clusters (or groups
of units/neurons) represent poor, rich, or in-between countries [http://www.cis.hut.fi/research/som-research/worldmap.html].
There are many other applications, but providing an exhaustive list is out of
the scope of this post.&lt;/p&gt;
&lt;h3 id=&#34;notation-and-terminology&#34;&gt;Notation and Terminology&lt;/h3&gt;
&lt;p&gt;Before diving into the details and the implementation of the SOM algorithms,
we must define some mathematical quantities and determine the terminology we
will use. The SOM is implemented as a neural network $\mathcal{F}({\bf \theta})$
that maps a high-dimensional manifold $ \mathcal{X} \in \mathbb{R}^k $
to a low-dimensional predefined latice $\mathcal{Y} \in \mathbb{R}^m$, where
$ m \ll k $. The input to the SOM is a vector $ {\bf x} \in \mathcal{X} $ and
the weights, $ {\bf w} $ of the neural network that learn the representations
are called &lt;em&gt;code words&lt;/em&gt; and the set of the code words,
$ \mathcal{W} = {\bf w_1, \ldots, w_n} $, is called a &lt;em&gt;codebook&lt;/em&gt;.
The cardinal number of $\mathcal{W}$ is the number of units we use in the
low-level lattice or the number of neural network&amp;rsquo;s units.
We let $ d(\cdot, \cdot) $ be the Minkowski distance
defined as
$$ d({\bf x}, {\bf y}) = \Big( \sum_{i=1}^{k} |x_i - y_i|^p \Big)^{\frac{1}{p}}, \qquad (1) $$
where here ${\bf x}, {\bf y} \in \mathbb{R}^n $ and $ p \geq 1$. For different values of $ p $, we recover different known
distance functions such as the Euclidean distance ($p=2$) or the Manhattan ($p=1$).&lt;/p&gt;
&lt;h2 id=&#34;the-som-algorithm&#34;&gt;The SOM Algorithm&lt;/h2&gt;
&lt;p&gt;We are now ready to introduce the SOM algorithm. First, we will describe the
main idea behind the SOM algorithm in plain words, and then we will give the
pseudocode and its implementation along with some examples.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/somtraining.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. Illustration of SOM algorithm steps. At the beginning (t=0), we see that the winner unit (BMU, center of yellow disc) that is closer to the current sample (white disc) has been identified. The SOM algorithm adjusts the codebook based on a neighborhood function (yellow disc) such that the map will come closer to the sample. After all the iterations have been exhausted, the SOM has learned the representation (black grid) of the input distribution (blue cloud). This figure is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported (Wikipedia)&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;The first step is to initialize the codebook; we usually do that by assigning
random values to the code words. Once we initialize the codebook, we start the
learning iteration. At each iteration or epoch, we draw a sample ${\bf x}$ from
the input space $ \mathcal{X} $, and we compute the distance of that sample
from all the code words. We select the code word with the smallest distance
from ${\bf x}$ such that
$$ i_{\text{BMU}} = \text{BMU} = \text{arg}\min_{i\in \mathbb{N}^+} \left\{ d({\bf x}, {\bf w}^t_i) \right\}, \qquad (2) $$&lt;/p&gt;
&lt;p&gt;and we call that unit (neuron) the best matching unit (BMU) or the winner unit.&lt;/p&gt;
&lt;p&gt;Figure 1 shows a random initialization of the codebook (black grid) that contains
two-dimensional code words ($ \mathcal{X} \in \mathbb{R}^2$). The white disc
represents the sample ${\bf x}$ we draw. After we compute all the distances
between that sample and all the grid points, we observe that the closest point
of the grid to that sample is the one at the center of the yellow disc.&lt;/p&gt;
&lt;p&gt;So now the next step is to update the code words ${\bf w}_i$ ($i = 1, \ldots, n$)
by moving them towards the sample ${\bf x}$ according to the following equation:&lt;/p&gt;
&lt;p&gt;$$ {\bf w}_i^{t+1} = {\bf w}_i^t + \epsilon(t) h(t, \text{BMU}, \sigma(t))({\bf x} - {\bf w}_i^t), \qquad(3) $$&lt;/p&gt;
&lt;p&gt;and updating the code words that fall within a neighborhood defined by the function:&lt;/p&gt;
&lt;p&gt;$$ h(t, \text{BMU}; \sigma(t)) = \exp\Big( \Big( \frac{d({\bf w}^t_{\text{BMU}}, {\bf w}^t_i)}{\sigma(t)} \Big)^2 \Big), \qquad(4) $$&lt;/p&gt;
&lt;p&gt;where $t$ is the current iteration (or time step or epoch). $\epsilon(t)$ and
$\sigma(t)$ are the time-dependent learning rate and
neighborhood function&amp;rsquo;s variance, respectively.They both decay exponentially
based on $ z_i \Big( \frac{z_f}{z_i} \Big)^{\frac{t}{t_f}} $, where $z_i$ and
$z_f$ are the initial and final values, respectively ($z = {\epsilon, \sigma}$).
We assume that $z_i \gg z_f$ for the initial and final values. The idea behind
a time-dependent decaying learning rate is to freeze learning after some time,
and a decaying variance of the neighborhood function guarantees that as the
learning progresses, the number of units that get to update their weights
decreases. Moreover, at the beginning of learning the radius of the
neighborhood function should be large enough, and at the end of learning,
we should keep the radius small such that only one or zero neighbors are within
the immediate vicinity of the BMU.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the pseudocode of the SOM algorithm. The algorithm presented
here is an online learning algorithm (or sequential), meaning that one sample
of data is processed at each iteration. A different version of the SOM
algorithm runs offline and uses the entire data set at each iteration, called
batch SOM. In this post, we will implement the online algorithm used in our
examples.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/som_algo.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. Self-organizing map basic algorithm.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;h2 id=&#34;pytorch-implementation&#34;&gt;Pytorch Implementation&lt;/h2&gt;
&lt;p&gt;The source code below is the implementation of the SOM algorithm given in
Figure 2. The implementation is a simple &lt;em&gt;translation&lt;/em&gt; of the pseudocode to
Python. In this implementation, we contain all the parameters and the codebooks
in $[0, 1]^k$, where $k \in \mathbb{N}^+$. We also consider the neighborhood
function a Gaussian, and the final time $t_f = 1$. The metric $d(\cdot, \cdot)$
is the Euclidean distance ($p=2$ in Equation 1).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pylab &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; nn


&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;SOM&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self,
                 n_units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;,
                 dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,
                 iters&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,
                 lrate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;),
                 sigma&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;)):
        super()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n_units &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; n_units				&lt;span style=&#34;color:#75715e&#34;&gt;# Number of units (n in main text)&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dim						&lt;span style=&#34;color:#75715e&#34;&gt;# Code words dim (k in main text)&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; iters
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lrate[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lrate[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigma[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigma[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]

		&lt;span style=&#34;color:#75715e&#34;&gt;# Initialize the code words&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;initCodebook()
        
		&lt;span style=&#34;color:#75715e&#34;&gt;# Initialize a two-dimensional grid&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;initLatice()

		&lt;span style=&#34;color:#75715e&#34;&gt;# Pre-compute the time array, learning rate, and variance&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;t &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;iters)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate_i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate_f &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate_i)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;t
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma_i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma_f &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma_i)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;t

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;initCodebook&lt;/span&gt;(self):
    	&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34; Initialize the code words using random values drawn from a 
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	uniform distribution in [0, 0.01).
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;codebook &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones([self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n_units&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n_units, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dim])
        nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform_(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;codebook, a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;initLatice&lt;/span&gt;(self):
    	&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34; Initialize a two-dimensional grid (latice) on which units
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	(or neurons) are placed. Moreover, computes the distances between all
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	the pairs within the grid.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
        line &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n_units)
        grid_x, grid_y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;meshgrid(line, line, indexing&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ij&amp;#34;&lt;/span&gt;)
        p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat((grid_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatten()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),
                       grid_y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatten()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)), dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cdist(p, p)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fit&lt;/span&gt;(self, X):
    	&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34; Fit the input data X to a two-dimensional SOM.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    	&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; t &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iters):
        	
        	&lt;span style=&#34;color:#75715e&#34;&gt;# Sample the input space&lt;/span&gt;
        	x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X[np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])]			

			&lt;span style=&#34;color:#75715e&#34;&gt;# Find the BMU&lt;/span&gt;
            BMU &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmin((((self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;codebook &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x))&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(dim&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))

			&lt;span style=&#34;color:#75715e&#34;&gt;# Compute the neighborhood function&lt;/span&gt;
            h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dist[BMU]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigma[t])&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

			&lt;span style=&#34;color:#75715e&#34;&gt;# Update the codebook&lt;/span&gt;
            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;codebook &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lrate[t] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; h &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;codebook &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/som_maps.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 3. Self-organizing maps. (A) Shows a self-organizing map  of the unit square $[0, 1]^2$, and (B) shows a a map for a ring in $[-1, 1]^2$.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;h3 id=&#34;square-in--mathbbr2-&#34;&gt;Square in $ \mathbb{R}^2 $&lt;/h3&gt;
&lt;p&gt;Our first example is to map a two-dimensional rectangular distribution in the
$[0, 1] \times [0, 1]$. To this end, we randomly generate points
$(x_1, x_2) \in [0, 1] \times [0, 1]$ from a uniform distribution $\mathcal{U}$.
After running the code presented earlier for $3000$ iterations and with
$\epsilon_i = \sigma_i = 0.5$ and $\epsilon_f = \sigma_f = 0.01$, we obtain the
self-organized map illustrated in Figure 3A, where the red dots represent the
input space (and thus the samples we used to train our neural network). The
white discs are the units of our SOM.&lt;/p&gt;
&lt;h3 id=&#34;ring-in--mathbbr2-&#34;&gt;Ring in $ \mathbb{R}^2 $&lt;/h3&gt;
&lt;p&gt;The second example is again related to space $[0, 1] \times [0, 1] $, only this
time we draw uniformly points from a ring, rendering this problem a little a
bit harder for the SOM algorithm since there is a hole in the center of our
distribution. Figure 3B shows the input space distribution in red dots, and
again, we see that the SOM algorithm has captured the input space. The code
words (white discs) cover the space, and the lattice (black solid lines) are
well organized. However, we see some units representing the hole (void) in the
middle of the ring. That is a known problem with Kohonen&amp;rsquo;s SOM algorithm.
The parameters for this experiment are the same as before.&lt;/p&gt;
&lt;h2 id=&#34;evaluate-a-som&#34;&gt;Evaluate a SOM&lt;/h2&gt;
&lt;p&gt;When measuring the quality of a self-organized map, there is no one measure to
rule them all. People use many different measures in the literature to measure
how good or how bad a self-organized map is. Three well-known measures are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The quantization error (or distortion), which is the average distance of the
minimum of all the pairwise distances between the input space samples
$\mathcal{X}$ and the codebook $\mathcal{W}$,
$ \mathcal{E_{Q}} = \mathbb{E}[\min\left\{ \mathcal{D}(\mathcal{X}, \mathcal{W}) \right\} $.
The quantization error does not measure how well the map preserves the data&amp;rsquo;s
topology but instead measures the goodness-of-fit of the input distribution
by the map. The lower the $\mathcal{E_Q}$, the better.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The topographic error quantifies how well the map captures the shape of the
input data and how well the topology of the map preserves the input space.
The topographic error is computed as
$\mathcal{E_{T}} = \frac{1}{|\mathcal{X}|} \sum_{{\bf x} \in \mathcal{X}}^{}I({\bf x}) $,
where $I({\bf x}) = 1$ when the BMU and the second best matching unit are
neighbors, and zero otherwise. The lower the value of the $\mathcal{E_{T}}$,
the better.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our examples above, we measure the quantization and topographic errors.
As Table 1 shows, the errors for the square distribution are indeed minor,
meaning that the maps capture and preserve the topology of the input distribution.
However, the quantization error is not that small for the ring distribution,
meaning that the map preserves the topology since the topographic error is small.
Still, the map needs to fit the data distribution better. We can see that the
map also tries to find non-existent data in the ring&amp;rsquo;s hole.&lt;/p&gt;






  
    
  


  




&lt;div class=&#34;data-table&#34; role=&#34;region&#34; tabindex=&#34;0&#34; aria-labelledby=&#34;table-caption-table1&#34;&gt;
  &lt;table class=&#34;table optional CSS class declaration&#34; id=&#34;table1&#34; itemscope itemtype=&#34;https://schema.org/Table&#34;&gt;&lt;caption id=&#34;table-caption-table1&#34; itemprop=&#34;about&#34;&gt;&lt;b&gt;Table&lt;/b&gt; ${\bf 1}$ SOM Errors. Quantization $\mathcal{E_Q}$ and topographic $\mathcal{E_T}$ errors for the square and ring distributions.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Example&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$\mathcal{E_Q} $&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$\mathcal{E_{T}}$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Square&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$0.034$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$0.028$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Ring&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$0.255$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$0.04$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;{{/table}}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we introduced some fundamental ideas of self-organizing maps.
We tried to convey the gist of how the SOM algorithm works and provided a
simple Python implementation using Pytorch. Finally, we tested our implementation
on two basic examples: a two-dimensional distribution of a rectangle and one of
a two-dimensional disc. In both cases, we see how the SOM algorithm learns the
representations and correctly maps the input distribution.&lt;/p&gt;
&lt;h3 id=&#34;source-code&#34;&gt;Source Code&lt;/h3&gt;
&lt;p&gt;The entire source code for the experiments used in this post is freely available
&lt;a href=&#34;https://gist.github.com/gdetor/379c2e3897d474894f42735b5b1ba641&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;@article{detorakis2023som,
  title   = &amp;#34;An introduction to self-organizing maps&amp;#34;,
  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
  journal = &amp;#34;gdetor.github.io&amp;#34;,
  year    = &amp;#34;2023&amp;#34;,
  url     = &amp;#34;https://gdetor.github.io/posts/som&amp;#34;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Grossberg S., &lt;em&gt;Physiological interpretation of the self-organizing map algorithm&lt;/em&gt;, 1994.&lt;/li&gt;
&lt;li&gt;Kohonen T., &lt;em&gt;Self-organized formation of topologically correct feature maps&lt;/em&gt;, Biol Cybern., 1982;43(1):59–69.&lt;/li&gt;
&lt;li&gt;Bellman R. E., &lt;em&gt;Dynamic programming&lt;/em&gt;, Princeton University Press, Rand Corporation (1957).&lt;/li&gt;
&lt;li&gt;Bellman R. E., &lt;em&gt;Adaptive control processes: a guided tour&lt;/em&gt;, Princeton University Press. ISBN 9780691079011, 1961.&lt;/li&gt;
&lt;li&gt;Ponmalai, R., and Kamath, C., &lt;em&gt;Self-organizing maps and their applications to data analysis&lt;/em&gt;, Lawrence Livermore National Lab.(LLNL), Livermore, CA, 2019.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;



</content>
    </item>
    
  </channel>
</rss>
