<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Useful data transformations :: GID Webpage</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Blog post on useful data transformations" />
<meta name="keywords" content="[gdetor, Georgios Detorakis, Georgios Is. Detorakis, georgios, detorakis]" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://gdetor.github.io/posts/normalization/" />




<link rel="stylesheet" href="https://gdetor.github.io/assets/style.css">



<link rel="stylesheet" href="https://gdetor.github.io/style.css">


<link rel="apple-touch-icon" href="https://gdetor.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://gdetor.github.io/icons/favicon.ico">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:creator" content="Georgios Is. Detorakis" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Useful data transformations">
<meta property="og:description" content="Blog post on useful data transformations" />
<meta property="og:url" content="https://gdetor.github.io/posts/normalization/" />
<meta property="og:site_name" content="GID Webpage" />

  
    <meta property="og:image" content="https://gdetor.github.io/icons/favicon.ico">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2022-10-13 00:00:00 &#43;0000 UTC" />












<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
  </head>
</html>


</head>
<body class="orange">


<div class="container center">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    # GID
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/research">Research</a></li>
        
      
        
          <li><a href="/software">Software</a></li>
        
      
        
          <li><a href="/publications">Publications</a></li>
        
      
        
          <li><a href="/artwork">Artwork</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/research">Research</a></li>
      
    
      
        <li><a href="/software">Software</a></li>
      
    
      
        <li><a href="/publications">Publications</a></li>
      
    
      
        <li><a href="/artwork">Artwork</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://gdetor.github.io/posts/normalization/">Useful data transformations</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2022-10-13
        
      </span>
    
    
      <span class="post-author">:: Georgios Is. Detorakis</span>
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://gdetor.github.io/tags/normalization/">normalization</a>&nbsp;
    
    #<a href="https://gdetor.github.io/tags/standardize/">standardize</a>&nbsp;
    
    #<a href="https://gdetor.github.io/tags/scaling/">scaling</a>&nbsp;
    
    #<a href="https://gdetor.github.io/tags/centered/">centered</a>&nbsp;
    
    #<a href="https://gdetor.github.io/tags/difference-transform/">difference transform</a>&nbsp;
    
    #<a href="https://gdetor.github.io/tags/power-transform/">power transform</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>This post briefly introduces fundamental data transformations such as mean
subtraction (centering data), normalization, standardization, difference
transform, and power transform. Furthermore, we provide simple examples of
Python code on how to apply those transforms to real data. Moreover, we heavily
rely on the <em>sklearn</em> Python package [1].</p>
<h2 id="mean-subtraction">Mean subtraction<a href="#mean-subtraction" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Let&rsquo;s assume we have some data in a vector $ {\bf x} $, and we know that the
mean value of $ {\bf x} $ is not zero. We could force the mean to be zero by
subtracting the mean from each element in the vector $ {\bf x} $. Thus we center
the data when we apply the following transform:</p>
<p>$$ {\bf z} = {\bf x} - \bar{x}, \quad (1) $$</p>
<p>where $ \bar{x} $ is the mean of $ {\bf x} $. Another important use of this
transform is this: When we would like to compare data sets of different scales,
such as temperatures in Celcius and Fahrenheit, we can center each data set
separately and then compare them.</p>
<p>The following code snippet shows how we can center data in Python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np

X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>, (<span style="color:#ae81ff">100</span>,))
X_bar <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>mean()
print(X_bar)		<span style="color:#75715e"># 4.9911754</span>

Z <span style="color:#f92672">=</span> X <span style="color:#f92672">-</span> X_bar
print(Z<span style="color:#f92672">.</span>mean()) 	<span style="color:#75715e"># -2.930988e-16 </span>
</code></pre></div><h2 id="normalize">Normalize<a href="#normalize" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Originally, data normalization meant rescaling and shifting a data set&rsquo;s values
so that they range in $ [0, 1] $. The mathematical formula to do that is:</p>
<p>$$ {\bf z} = \frac{ {\bf x} - x_{\text{min}} }{ x_{\text{max}} - x_{\text{min}} }, \quad (2) $$</p>
<p>where $ {\bf x} $ is the input data, $ x_{\text{min}} $ is the minimum element
in the vector $ {\bf x} $, and $ x_{\text{max}} $ is the maximum element.</p>
<p>However, if we would like to normalize our data into a different interval
$ [a, b] $ we can use the following formula:</p>
<p>$$ {\bf z} = \frac{ {\bf x} - x_{\text{min}} }{ x_{\text{max}} - x_{\text{min}} } (b - a) + a. \quad (3) $$</p>
<p>Normalization is used when we know our data do not follow a Gaussian distribution.</p>
<p>In Python, we can normalize any data set using the <em>MinMaxScaler</em> function
of <em>sklearn</em> [2].</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> MinMaxScaler

X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random((<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1</span>))

<span style="color:#75715e"># Normalize into [0, 1]</span>
scaler <span style="color:#f92672">=</span> MinMaxScaler()
Z <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(X)

<span style="color:#75715e"># Normalize into [2, 4]</span>
scaler <span style="color:#f92672">=</span> MinMaxScaler(feature_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>))
Z <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(X)

</code></pre></div><h2 id="standarize">Standarize<a href="#standarize" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Let&rsquo;s assume that we have some data on people&rsquo;s height and weight, and we would
like to use machine learning models on them. Naturally, weight and height
measure different physical quantities and thus are in various scales and units
(height is usually between $ 10 $ and $ 200 $ kilograms, and height is between
$ 0 $ to $ 2 $ m). So, <em>how do we use those data</em>? One solution is to
standardize the data using the z-score</p>
<p>$$  {\bf z} = \frac{{\bf x} - \bar{x}}{\sigma}, \quad (4)  $$</p>
<p>where $ {\bf x} $ is the vector that holds the data, $ \bar{x} $ is the mean
value of $ {\bf x} $, and $ \sigma $ is the standard deviation. When we
standardize, our data ti means they will have a zero mean and a unit standard
deviation. We usually apply a standardization transformation when we know
that our data follow a Gaussian-like distribution.</p>
<p>In Python, we can standardize our data using the preprocessing functions
provided by the <em>sklearn</em> package [3]. Here is an example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler

X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>empty((<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">2</span>))
<span style="color:#75715e"># Both weight and height follow a Gaussian distribution</span>
X[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, (<span style="color:#ae81ff">100</span>, ))		<span style="color:#75715e"># height in meters</span>
X[:, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">70</span>, <span style="color:#ae81ff">50</span>, (<span style="color:#ae81ff">100</span>,))		<span style="color:#75715e"># weight in kilograms</span>

scaler <span style="color:#f92672">=</span> StandardScaler()
Z <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(X)
</code></pre></div><p>When we should use a standardization of our data:</p>
<ul>
<li>Before <em>PCA</em>. Sometimes data points with high variance get weighted
more and dominate the principal components.</li>
<li>Before clustering algorithms such as <em>kNN</em>. Clustering algorithms are based
on the notion of distance as a similarity measure; thus, data with a wide
range of features will affect the distances of the clustering algorithm more.</li>
<li>Before <em>SVM</em>. Classic SVM tries to maximize the distance
between two separable hyperplanes and their support vectors. Thus, data with
a wide range of features will affect the distances in a non-desirable way.</li>
<li>Before <em>LASSO</em> or <em>Ridge</em> regression. These algorithms penalize the
magnitude of their coefficients related to each variable, and thus the scale
of each variable will determine the penalty values. Coefficients with high
variance take small values, and thus they will be penalized less.</li>
</ul>
<p>In the following cases, we can skip standardization since these models are
immune to a wide range of features:</p>
<ul>
<li>Logistic regression</li>
<li>Random forests</li>
<li>Decision trees</li>
<li>Gradient boosting</li>
</ul>
<h2 id="difference-transform">Difference transform<a href="#difference-transform" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>A difference transform is most useful when we are dealing with time series.
In case there is a trend in a time series, and we would like to get rid of it,
we can apply a difference transform by subtracting the value at time $ t-1 $
from the current time $ t $ value. More precisely,</p>
<p>$$ x[t] = x[t] - x[t-1], \quad (5) $$</p>
<p>and if we would like to get rid of a seasonal structure, then we only need
to take into account the period (or frequency) of that seasonality,</p>
<p>$$ x[t] = x[t] - x[t - d], \quad (6) $$</p>
<p>where $ d $ is the delay or the period of seasonality (<em>e.g.</em>, how many
data points in the past we should subtract).</p>
<p>The difference transform is easy to implement in Python. The following code
snippet provides two functions to apply and reverse the difference transform.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">difference</span>(X, delay<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
	n <span style="color:#f92672">=</span> len(X)
    diff <span style="color:#f92672">=</span> [X[i] <span style="color:#f92672">-</span> X[i <span style="color:#f92672">-</span> delay] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(delay, n)]


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">invDifference</span>(X, dX, delay<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
	n <span style="color:#f92672">=</span> len(X)
    inv <span style="color:#f92672">=</span> [dX[i <span style="color:#f92672">-</span> delay] <span style="color:#f92672">+</span> X[i <span style="color:#f92672">-</span> delay] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(delay, n)]


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">10</span>)])
    print(x)					<span style="color:#75715e"># 1, 2, 3, 4, 5, 6, 7, 8, 9</span>

    x_diff <span style="color:#f92672">=</span> difference(x, <span style="color:#ae81ff">1</span>)
    print(x_diff)				<span style="color:#75715e"># [1, 1, 1, 1, 1, 1, 1, 1]</span>

	<span style="color:#75715e"># We can obtain similar results when delay=1 using Numpy&#39;s diff function</span>
    x_diff_prime <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>diff(x)
    print(x_diff_prime)			<span style="color:#75715e"># [1 1 1 1 1 1 1 1]</span>

	x_diff_inv <span style="color:#f92672">=</span> invDifference(x, x_diff, <span style="color:#ae81ff">1</span>)
    print(x_diff_inv)			<span style="color:#75715e"># [2, 3, 4, 5, 6, 7, 8, 9]</span>

	<span style="color:#75715e"># Another way to inverse the difference when delay=1 is the following</span>
    x_diff_prime_inv <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>r_[x[<span style="color:#ae81ff">0</span>], x_diff_prime]<span style="color:#f92672">.</span>cumsum()
    print(x_diff_prime_inv)		<span style="color:#75715e"># [1 2 3 4 5 6 7 8 9]</span>
</code></pre></div><h2 id="power-transform">Power transform<a href="#power-transform" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>We can apply a power transform when we want to make our data distribution look
more like Gaussian. At the same time, a power transformation will also stabilize
the variance of our data. There are two major power transforms, the Cox-Box [4]
and the Yeo-Johnson [5]. The <em>sklearn</em> Python package supports Cox-Box and
Yeo-Johnson transforms, and the Box-Cox transform requires strictly positive
data. On the other hand, Yeo-Johnson supports positive and negative data [6].
The following code snippet demonstrates how we can apply them to our data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> PowerTransformer

X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">10</span>)
print(X)	

<span style="color:#75715e"># array([0.92222554, 0.92306673, 0.82856923, 0.8713333 , 0.08001814,</span>
<span style="color:#75715e">#        0.12258023, 0.5008433 , 0.60396389, 0.24539718, 0.55259061])</span>

pt <span style="color:#f92672">=</span> PowerTransformer(method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;box-cox&#34;</span>)		<span style="color:#75715e"># or method=&#39;yeo-johnson&#39; (default)</span>

<span style="color:#75715e"># fit_transform receives ndarray of shape (n_samples, n_features)</span>
Z <span style="color:#f92672">=</span> pt<span style="color:#f92672">.</span>fit_transform(X<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)	
Z <span style="color:#f92672">=</span> Z[:, <span style="color:#ae81ff">0</span>]								<span style="color:#75715e"># we have only one feature</span>
print(Z)

<span style="color:#75715e"># [-1.71002627  1.17246013 -1.55389497  0.91312675 -0.02584321  0.19532181</span>
<span style="color:#75715e">#  -0.08403872  1.48983064  0.03445026 -0.43138641]</span>
</code></pre></div><h2 id="summary">Summary<a href="#summary" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>We went through five essential data transforms, centering data,
normalization, standardization, difference, and power transform. We briefly
described the math behind those transformations and provided some Python
functions based on <em>sklearn</em> that implement those transformations.</p>
<h3 id="cited-as">Cited as<a href="#cited-as" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-latex" data-lang="latex">@article{detorakis2022acfpacf,
  title   = &#34;Useful data transformations&#34;,
  author  = &#34;Georgios Is. Detorakis&#34;,
  journal = &#34;gdetor.github.io&#34;,
  year    = &#34;2022&#34;,
  url     = &#34;https://gdetor.github.io/posts/normalization&#34;
}
</code></pre></div><h3 id="references">References<a href="#references" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<ol>
<li>Pedregosa et al., <em>Scikit-learn: Machine Learning in Python,</em>, JMLR 12, pp. 2825-2830, 2011.</li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler">Sklearn MinMaxScaler</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">Sklearn StandardScaler</a></li>
<li>G. E. Box and D. R. Cox, <em>An analysis of transformations</em>, Journal of the Royal Statistical Society, Series B. 26 (2): 211–252, 1964.</li>
<li>In-Kwon Yeo and R.A. Johnson, <em>A New Family of Power Transformations to Improve Normality or Symmetry</em>, Biometrica, 87(4), 2000.</li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer">Sklearn PowerTransformer</a></li>
</ol>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://gdetor.github.io/posts/acf_pacf/">
                <span class="button__icon">←</span>
                <span class="button__text">Autocorrelation Functions for Time Series</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://gdetor.github.io/posts/errors/">
                <span class="button__text">Time series forecasting error metrics</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright copyright--user">
        <span>ⓒ  GID, 2021-2022</span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://gdetor.github.io/assets/main.js"></script>
<script src="https://gdetor.github.io/assets/prism.js"></script>




<style>
    #body a, #body span {
        display: inline-block;
        width: 100%;
        text-align: center;
        line-height: 1.7em;
    }
    h2 {
        display: inline-block;
    }
    img.github, img.gitlab, img.rgate, img.orcid{
        -webkit-filter: invert(1);
        filter: invert(1);
    }
</style>

<center>
    <div>
        <a class="inline-block px-2" href="https://github.com/gdetor"><h2><img class="image github" src="/github-square.svg" alt="Github" width="36"></h2></a>
        <a class="inline-block" href="https://gitlab.com/gdetor"><h2><img class="image gitlab" src="/gitlab.svg" alt="Gitlab" width="36"></h2></a>
        <a class="inline-block px-2" href="https://www.researchgate.net/profile/Georgios-Detorakis"><h2>
         <img class="image rgate" src="/researchgate-square.svg" alt="RG" width="36"></h2></a>
        <a class="inline-block px-2" href="https://orcid.org/0000-0001-5891-1702"><h2>
         <img class="image orcid" src="/orcid-square.svg" alt="RG" width="36"></h2></a>
    </div>
</center>


  
</div>

</body>
</html>
