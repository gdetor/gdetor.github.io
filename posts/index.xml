<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on GID Webpage</title>
    <link>https://gdetor.github.io/posts/</link>
    <description>Recent content in Posts on GID Webpage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ⓒ  GID, 2021-2022</copyright>
    <lastBuildDate>Thu, 05 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gdetor.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://gdetor.github.io/posts/welcome/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/welcome/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>Time series and forecasting error measures</title>
      <link>https://gdetor.github.io/posts/errors/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/errors/</guid>
      <description>In this post, we are going to explore the basic error measures used in time-series forecasting. Error measures provide a way to quantify the quality of a forecasting algorithm (e.g., the performance). First, we briefly introduce time series and the fundamental terms of forecasting. Second, we will introduce the most commonly used error measures and give some examples. Finally, we provide a complete example of using errors in a real-life forecasting scenario.</description>
      <content>&lt;p&gt;In this post, we are going to explore the basic error measures
used in time-series forecasting. Error measures provide a way
to quantify the quality of a forecasting algorithm (&lt;em&gt;e.g.&lt;/em&gt;, the
performance). First, we briefly introduce time series and the
fundamental terms of forecasting. Second, we will introduce the
most commonly used error measures and give some examples. Finally,
we provide a complete example of using errors in a real-life
forecasting scenario.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-time-series&#34;&gt;What is a time series&lt;/h2&gt;
&lt;p&gt;A time series is a series of data points indexed in time order in
layman&amp;rsquo;s terms [1, 2].
A few examples of time series are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The daily closing price of a stock in the stock market&lt;/li&gt;
&lt;li&gt;The number of air passengers per month&lt;/li&gt;
&lt;li&gt;The biosignals recorded from electroencephalogram or electrocardiogram&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 1 shows the number of air passengers per month from January 1949
to September 1960.  In all the examples in this post, we are going to
use this dataset, so if you would like to try the examples by yourself;
you can download the data from Kaggle
&lt;a href=&#34;https://www.kaggle.com/datasets/rakannimer/air-passengers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/passengers.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. An example of a time series showing the number of air passengers per month.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;A more rigorous definition of a time series found in [1] (Chapter 1, pg 1)
is given below:&lt;/p&gt;
&lt;p&gt;Let $ k \in \mathbf{N}, T \subseteq \mathbf{R} $. A function
$$ x: T \rightarrow \mathbf{R}^k, \hspace{2mm} t \rightarrow x_t $$
or equivalently, a set of indexed elements of $ \mathbf{R}^k $,
$$  { x_t | x_t \in \mathbf{R}^k, \hspace{2mm} t \in T }  $$
is called an observed time series (or time series).
Sometimes, we write $ x_t(t \in T) $ or $ (x_t)_{t\in T} $.&lt;/p&gt;
&lt;p&gt;When $ k = 1 $ the time series is called &lt;em&gt;univariate&lt;/em&gt;, otherwise is
called &lt;em&gt;multivariate&lt;/em&gt;.
$ T $ determines if the time series is [1]:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;discrete&lt;/strong&gt; $ T $ is countable, and $ \forall a &amp;lt; b \in \mathbb{R}: T \cap[a, b] $
is finite,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;continuous&lt;/strong&gt; $ T = [a, b], a &amp;lt; b \in \mathbb{R}, T = \mathbb{R}_{+}
\hspace{2mm} \text{or} \hspace{2mm} T = \mathbb{R} $,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;equidistant&lt;/strong&gt; $ T $ is discrete, and $ \exists u \hspace{2mm} s.t. \hspace{2mm} t_{j+1} - t_j = u $.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;From now on and for simplicity&amp;rsquo;s sake we will use the following notation for
a time series: $ y[1], y[2], \ldots , y[N] $, where
$ N \in \mathbb{N} $ or $ y[t] $, where $t=1, \ldots , N $.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;more-terminology&#34;&gt;More terminology&lt;/h2&gt;
&lt;p&gt;Before we dive into the post, let&amp;rsquo;s give some valuable terminology for the
unfamiliar reader.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Observed data&lt;/strong&gt; ($ (y_t)_{t\in T} $ or $ y[t] $) This is the
data we obtain by observing a system or a process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictive model&lt;/strong&gt; Is a mathematical representation of observed data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt; ($ y[t] $) This is the gound truth signal we use to train
a predictor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Horizon&lt;/strong&gt; ($ h $) Is the number of points or steps we predict in
the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt; ($ \hat{y}[t] = y[t+h] $) This is a value that predictor
returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forecasting&lt;/strong&gt; Is the process of prediction future values from historical
and present data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlier&lt;/strong&gt; It is a significantly different value from other values
in a time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error&lt;/strong&gt; ($ \epsilon[t] $) is the difference between the target signal
and the prediction of our model. The error is given by
$ \epsilon[t] = y[t] - \hat{y}[t] $.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seasonality&lt;/strong&gt; ($ S $) Seasonality is the periodic appearance of specific
patterns over the same period—for instance, increasing prices before and
over the Christmas holidays.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;four-basic-predictors&#34;&gt;Four basic predictors&lt;/h2&gt;
&lt;p&gt;So far, we have seen what a time series and the basic terminology is.
Now, we will explore some essential predictors or models and see how
we can use them to perform forecasting.&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;ve already seen, a predictor is a statistical (or mathematical)
model that receives as input historical and present data and returns one
(one-step ahead forecasting, $ h = 1$) or multiple future values
(multi-step ahead prediction, $ h &amp;gt; 1 $).
The development of predictors is out of the scope of this post, so we
will not see how to build, train, test/validate, and use a predictor (here
are a few references where the reader can find more details on that matter
[3, 4, 5, 6]). However, we will introduce the four elementary
predictors since some error measures use some of them to estimate the
prediction errors.&lt;/p&gt;
&lt;h4 id=&#34;naive-predictor&#34;&gt;Naive predictor&lt;/h4&gt;
&lt;p&gt;The most straightforward predictor we can imagine is the &lt;em&gt;naive&lt;/em&gt; one,
and it gets the last observed value and returns it as the predicted
value.&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t + h | t] = y[t]. $$&lt;/p&gt;
&lt;h4 id=&#34;seasonal-predictor&#34;&gt;Seasonal predictor&lt;/h4&gt;
&lt;p&gt;We can use the seasonal predictor when we know that our time series
has a seasonal component (seasonality). It is a natural extension of
the naive one, and we can describe it as:&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = y[t+h-S(P+1)]. $$&lt;/p&gt;
&lt;p&gt;$ P $ is $ \Big[\frac{h-1}{S}\Big] $, where $ \Big[ x \Big] $ is the
integer part of $ x $. $ P $ reflects the number of years&amp;ndash;365 days&amp;ndash;
have passed prior to time $ t + h $.&lt;/p&gt;
&lt;h4 id=&#34;average-predictor&#34;&gt;Average predictor&lt;/h4&gt;
&lt;p&gt;This predictor receives historical and present values as input,
computes their average (or mean), and returns it as a prediction.&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = \bar{y} = \frac{1}{N} \sum_{t=1}^{N} y[t] .$$&lt;/p&gt;
&lt;h4 id=&#34;drift-predictor&#34;&gt;Drift predictor&lt;/h4&gt;
&lt;p&gt;Another variation of the naive predictor, only this time
we allow to the predicted value to &lt;em&gt;drift&lt;/em&gt; (fluctuate) over time,&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = y[t] + \frac{h}{t-1} \sum_{j=2}^{t} (y[j]-y[j-1]) = y[t] + h\Big( \frac{y[t] - y[1]}{t - 1} \Big). $$&lt;/p&gt;
&lt;p&gt;You can picture this predictor as a line drawn from the first
observation to the last one and beyond, where beyond is the prediction.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the predictions in each of the cases mentioned above for
the air passengers data (brown line).
The blue line represents the naive predictor, the green line the
seasonal, the orange line shows the average predictor, and finally,
the pink line indicates the drift predictor.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/naive_predictors.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. Forecasts of montly air passengers. Naive predictor (blue line), naive seasonal (green), average (orange), drift (pink).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;h2 id=&#34;forecasting-error-measures&#34;&gt;Forecasting error measures&lt;/h2&gt;
&lt;p&gt;So why do we need error measures? The general idea is to quantify
the distance between an actual observation (target) and a predicted
one. Particularly when we train a model to learn how to predict
future values, we have to measure the error between the actual
observations and the predicted ones, so the minimization of the
error leads to a better model.&lt;/p&gt;
&lt;p&gt;When we teach a model, we need to use some penalties to help the
model improve the predictions. The error measures listed below do
precisely that. They measure how far the model&amp;rsquo;s predictions are
from the ground truth and penalize the model accordingly. Usually,
the smaller the error, the better the predictor.&lt;/p&gt;
&lt;p&gt;Another reason we need error measures is to evaluate the performance
of our model in real-life scenarios. We might have a trained model
that performs some forecasting, and we would like to investigate the
quality of its predictions. In this case, we can measure the error
between the historical data we will collect in the future and the
model&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;We already said that developing and training a predictor is out of
the scope of the present post. Therefore, we will use historical
data and add some Gaussian noise to them to fake the predictions.
Furthermore, we adopt the discrete-time signals time indexing, meaning
that $ y[t] $ is the value of the time series at time index $ t $.
A similar way would be $ y_t $, where $ t $ is the time index.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt; $ y[t] $ is the target signal, $ \hat{y}[t] $ the prediction
signal and $ \epsilon[t] $ the error signal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And now, we are ready to introduce the error measures and some examples
demonstrating their behavior.&lt;/p&gt;
&lt;h3 id=&#34;mean-absolute-error-mae&#34;&gt;Mean Absolute Error (MAE)&lt;/h3&gt;
&lt;p&gt;The MAE is the most straightforward error measure, and as it&amp;rsquo;s
name signifies it is just the difference between a target (or desired)
value and model&amp;rsquo;s prediction. MAE is defined as:&lt;/p&gt;
&lt;p&gt;$$ \frac{1}{N} \sum_{t=1}^{N} |y[t] - \hat{y}[t]| = \frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] |. $$&lt;/p&gt;
&lt;p&gt;By observing the definition of MAE, we can see that MAE is
scale-dependent, meaning that both signals, target, and prediction,
must be of the same scale. Another drawback of MAE that we can identify
by looking at its definition is its sensitivity to outliers (&lt;em&gt;e.g.&lt;/em&gt;,
values in the time series that stick further away from any other value).
Outliers can drag the MAE to higher values, thus affecting the error.
However, there are ways to handle outliers and fix that issue (see here [REF]).&lt;/p&gt;
&lt;h3 id=&#34;mean-absolute-percentage-error-mape&#34;&gt;Mean Absolute Percentage Error (MAPE)&lt;/h3&gt;
&lt;p&gt;MAPE computes the error between a target and a prediction signal
as a ratio of the error $ \epsilon[t] $ and the target signal. More
precisely,&lt;/p&gt;
&lt;p&gt;$$ MAPE = \frac{100\%}{N} \sum_{t=1}^{N} \frac{|y[t] - \hat{y}[t]|}{y[t]} = \frac{100}{N} \sum_{t=1}^{N} \frac{| \epsilon[t] |}{y[t]}. $$&lt;/p&gt;
&lt;p&gt;MAPE is a helpful error measure when it serves as a loss function
in training and validating a regression model [REF].&lt;/p&gt;
&lt;p&gt;Again by observing the definition of MAPE above, we can draw some
conclusions about this measure. MAPE can be problematic when the actual
values are zero or close to zero. We can see from the definition above
that when the denominator is close to zero or zero, the MAPE is too
large or cannot be defined. Moreover, MAPE is susceptible to skewness,
since the term $ \frac{1}{y[t]} $ depends only on the observed data
(not on the model&amp;rsquo;s predictions).&lt;/p&gt;
&lt;h3 id=&#34;mean-squared-error-mse&#34;&gt;Mean Squared Error (MSE)&lt;/h3&gt;
&lt;p&gt;The MSE is one of the most used error measures in Machine and
Deep learning. It computes the average of the square of the errors
between target and prediction signals. We define the MSE as:&lt;/p&gt;
&lt;p&gt;$$ MSE = \frac{1}{N} \sum_{t=1}^{N} (y[t] - \hat{y}[t])^2 = \frac{1}{N} \sum_{t=1}^{N} \epsilon[t]^2 . $$&lt;/p&gt;
&lt;p&gt;If we take the square root of $ MSE $, we get the Root MSE or RMSE.
When the MSE is zero, we call the predictor (model) a perfect predictor.
MSE falls into the category of quadratic errors. Quadratic errors
tend to exaggerate the difference between the target and the model&amp;rsquo;s
prediction, rendering them suitable for training models since the
penalty applied to the model will be more prominent when the error
signal is significant [7].&lt;/p&gt;
&lt;p&gt;MSE combines the &lt;em&gt;bias&lt;/em&gt; and the &lt;em&gt;variance&lt;/em&gt; of a prediction. More
precisely, $ MSE = b^2 + Var $, where $b$ is the bias term and
$ Var $ is the variance. The bias reflects the assumptions the
model makes to simplify the process of finding answers. The more
assumptions a model makes, the larger the bias. On the other hand,
variance refers to how the answers given by the model are subject
to change when we present different training/testing data to the
model. Usually, linear models such as &lt;em&gt;Linear Regression&lt;/em&gt; and
&lt;em&gt;Logistic Regression&lt;/em&gt; have high bias and low variance. Nonlinear
models such as &lt;em&gt;Decision Trees&lt;/em&gt;,  &lt;em&gt;SVM&lt;/em&gt;, and &lt;em&gt;kNN&lt;/em&gt; have low
bias and high variance [8]. Ideally, we would like to find a balance
between bias and variance. That&amp;rsquo;s why sometimes we have to penalize
our model during training using regularization techniques (this is
out of the scope of the present post).&lt;/p&gt;
&lt;h3 id=&#34;symmetric-mean-absolute-percentage-error-smape&#34;&gt;Symmetric Mean Absolute Percentage Error (SMAPE)&lt;/h3&gt;
&lt;p&gt;SMAPE computes the error between the target and the prediction signals
as a ratio of the error with the sum of the absolute values of actual
and prediction values.
The mathematical definition for SMAPE is:&lt;/p&gt;
&lt;p&gt;$$ SMAPE = \frac{100\%}{N} \sum_{t=1}^{N} \frac{| \epsilon [t] |}{(| y[t]| + | \hat{y}[t] |)} $$&lt;/p&gt;
&lt;p&gt;And as we can see from that definition, SMAPE is bounded from
above and below, $ 0 \leq SMAPE \leq 100 $. Another remark we
can make based on  SMAPE&amp;rsquo;s definition: when both a target and
a prediction value are zero, the SMAPE is not defined. If only
the actual or target value is zero, $ SMAPE = 100 $. Finally,
SMAPE can cause troubles when  let&amp;rsquo;s say a prediction is
$ \hat{y}[t] = 10 $ the first time and $ \hat{y}[t] = 12 $ the
second time, while in both cases the target (actual) value is
$ y[t] = 11 $.  In the former case, $ SMAPE  = 4.7 % $ and in
the latter case $ SMAPE = 4.3 % $. We see that we get two different
error values for the same target when our predictor returns
different predictions.&lt;/p&gt;
&lt;h3 id=&#34;mean-absolute-scaled-error-mase&#34;&gt;Mean Absolute Scaled Error (MASE)&lt;/h3&gt;
&lt;p&gt;MASE is a metric that computes the error ratio between the
target and the model&amp;rsquo;s prediction to a naive predictor&amp;rsquo;s error (forecaster).&lt;/p&gt;
&lt;p&gt;The following equation gives the MASE,&lt;/p&gt;
&lt;p&gt;$$ MASE = \frac{\frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] | }{\frac{1}{N-1} \sum_{t=2}^{N} | y[t] - y[t-1] | } $$&lt;/p&gt;
&lt;p&gt;When we are dealing with time series with seasonality with period
$ S $ we can use the following MASE formula instead:&lt;/p&gt;
&lt;p&gt;$$ MASE = \frac{\frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] | }{\frac{1}{N-S} \sum_{t=S+1}^{N} | y[t] - y[t-S] | }. $$&lt;/p&gt;
&lt;p&gt;MASE is scale-invariant, meaning that it&amp;rsquo;s immune to any scaling
we perform on the observed data. MASE is symmetric, which implies
that it penalizes equally the positive and the negative (as well
as big and small) forecast errors. When MASE error is greater than
one, the naive forecaster performs better than our model. MASE can
be problematic only when the actual (target) signal is only zero values.
In that case the naive predictor will be zero ad infinitum and thus
the MASE will be undefined.&lt;/p&gt;
&lt;h3 id=&#34;coefficient-of-determination-cod-or-r&#34;&gt;Coefficient of Determination (CoD) or R²&lt;/h3&gt;
&lt;p&gt;The $ R^2 $ or Coefficient of Determination is an error measure
frequently used in evaluating regression models (&lt;em&gt;goodness of fit&lt;/em&gt;
or &lt;em&gt;best-fit line&lt;/em&gt;). $ R^2 $ counts how many of the target data
points approach the line formed by the regression [7].&lt;/p&gt;
&lt;p&gt;We define $ R^2 $ as&lt;/p&gt;
&lt;p&gt;$$ R^2 = 1 - \frac{\sum_{t=1}^{N}(y[t] - \hat{y}[t])^2 }{\sum_{t=1}^{N}(y[t] - \bar{y})^2} = 1 - \frac{MSE}{Var[y[t]]}, $$&lt;/p&gt;
&lt;p&gt;or alternatively&lt;/p&gt;
&lt;p&gt;$$ R^2 = \frac{SSR}{SST} = \frac{\sum_{t=1}^{N}(y[t] - \hat{y}[t])^2 }{\sum_{t=1}^{N}(y[t] - \bar{y})^2}. $$&lt;/p&gt;
&lt;p&gt;SSR is the sum of squares regression, and SST is the sum of squares
total. SSR represents the total variation of all the predicted values
found on the regression plane from the mean value of all the values
of response variables. SST reflects the total variation of actual
values (targets) from the mean value of all the values of response
variables.&lt;/p&gt;
&lt;p&gt;$R^2$ is bounded from above, $R^2 \leq 1$, since the fraction term
lives always in the interval $ [0, 1] $. In the case of training a
regression model $ R^2 $ is bounded from bellow $ 0 \leq R^2 \leq 1 $.
For the test/validation data, $ R^2 $ can be negative when MSE is
large or the total variance of the target (actual) signal is too
small. A negative $ R^2 $ implies that the term $ \bar{y} $ is a
better predictor than our model. Moreover, from the first definition
of $ R^2 $, we see a direct relation between $ R^2 $ and MSE. While
the $ R^2 $ increases, the MSE tends to approach zero. When we have
an ideal predictor, $ MSE = 0 $ and $ r^2 = 1 $.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022errors-timeseries,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Time series and forecasting error measures&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/errors&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;J. Beran, &lt;em&gt;Mathematical Foundations of Time Series Analysis A Concise
Introduction&lt;/em&gt;, Springer, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Time_series&#34;&gt;&amp;ldquo;Time series&amp;rdquo;&lt;/a&gt;, Wikipedia,
Wikimedia Foundation, May 2 2022.&lt;/li&gt;
&lt;li&gt;A. Nielsen, &lt;em&gt;Practical time series analysis: Prediction with statistics
and machine learning&lt;/em&gt;, O&amp;rsquo;Reilly Media, 2019.&lt;/li&gt;
&lt;li&gt;R. J. Hyndman, and G. Athanasopoulos, &lt;em&gt;Forecasting: principles and practice&lt;/em&gt;,
OTexts, 2018.&lt;/li&gt;
&lt;li&gt;D. Oliveira, &lt;em&gt;Deep learning for time series forecasting&lt;/em&gt;,
&lt;a href=&#34;https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting&#34;&gt;https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting&lt;/a&gt;,
Kaggle, 2019.&lt;/li&gt;
&lt;li&gt;R. Mulla, &lt;em&gt;[Tutorial] TIme series forecasting with XGBoost&lt;/em&gt;,
&lt;a href=&#34;https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost&#34;&gt;https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost&lt;/a&gt;,
Kaggle, 2019.&lt;/li&gt;
&lt;li&gt;A. Kumar, &lt;em&gt;Mean squared error or R-squared - Which one to use?&lt;/em&gt;
&lt;a href=&#34;https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/&#34;&gt;https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/&lt;/a&gt;, 2022.&lt;/li&gt;
&lt;li&gt;C. M. Bishop, and N. M. Nasrabadi, &lt;em&gt;Pattern recognition and machine learning&lt;/em&gt;,
New York: Springer, 2006.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Genetic Algorithms &amp; Island Models</title>
      <link>https://gdetor.github.io/posts/genetic_algorithms/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/genetic_algorithms/</guid>
      <description>In this post, we explore genetic algorithms (GAs) and the so-called island model (IM). GAs and the IM are optimization methods used to maximize or minimize a cost function.
What is Optimization? Let&amp;rsquo;s see an example of an optimization problem we all face every day. Let&amp;rsquo;s assume you&amp;rsquo;d like to go and grab a couple of coffee from your favorite coffee shop. Typically, you ask Google to find the fastest way to the store from your current location.</description>
      <content>&lt;p&gt;In this post, we explore genetic algorithms (GAs) and the so-called
island model (IM). GAs and the IM are optimization methods used to
maximize or minimize a cost function.&lt;/p&gt;
&lt;h2 id=&#34;what-is-optimization&#34;&gt;What is Optimization?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see an example of an optimization problem we all face every day. Let&amp;rsquo;s
assume you&amp;rsquo;d like to go and grab a couple of coffee from your favorite coffee
shop. Typically, you ask Google to find the fastest way to the store from your
current location. But let&amp;rsquo;s forget about technology for now.&lt;/p&gt;
&lt;p&gt;You only have a map. Yes, a paper map! They are still around! You first try to
find the shortest paths from your current location to the coffee shop. If
you&amp;rsquo;re a scrooge, you will define the &amp;ldquo;optimal path&amp;rdquo; as &amp;ldquo;the path I consume the
least fuel.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;An optimization problem is a search for the &amp;ldquo;best&amp;rdquo; set of parameters, where
&amp;ldquo;best&amp;rdquo; is defined as the minimum or maximum of some cost function of interest.
Here, that function is our fuel consumption. However, we could have also
designated it to be the distance from start to finish.&lt;/p&gt;
&lt;p&gt;Mathematically speaking the problem of a minimization can be formulated as
follows [1]:
Given a function $f:A\subseteq \mathbb{R} \rightarrow \mathbb{R}$ we are
searching for an element $ {\bf x}^* $ such that
$$ f({\bf x}^*) \leq f({\bf x}) $$&lt;/p&gt;
&lt;p&gt;for all $ {\bf x} \in A $. Similarly, a maximization would be the search for
a ${\bf x}^* $ such that
$$ f({\bf x}^*) \geq f({\bf x}) $$
for all $ {\bf x} \in A $.&lt;/p&gt;
&lt;p&gt;In both cases, we are searching for a global optimum (either a global minimum
or a global maximum). However, it is not always possible to find a global
optimum point in real-life cases. Instead, we can settle for a local minimum or
maximum. For example, that&amp;rsquo;s the compromise we often make when training neural
networks with backpropagation [2].&lt;/p&gt;
&lt;p&gt;Figure 1A shows the global minimum of the cost function, $f(x)=x^2$, with a
magenta color. Figure 1B displays what is known as the Rastrigin function in
one dimension. It is evident that this function has multiple local minima
(&lt;em&gt;e.g.,&lt;/em&gt;, magenta disc)  and maxima (green disc) as well as one global minimum
at $(0, 0)$ (black disc).&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/fun_extremes.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. Global and local extremes for (A) $f(x)=x^2$, where the magenta disc indicates the global minimum at $(0, 0)$. (B) For Rastrigin $f(x) = 10 + x^2-10\cos(2\pi x)$, there is a global mimimum at $(0, 0)$ and many local mimima and maxima (for instance see the magenta and green discs, respectively).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Sometimes, it&amp;rsquo;s easier to solve a minimization problem from a computational
standpoint. In that case, we would minimize the function $ -f $.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-genetic-algorithm&#34;&gt;What is a Genetic Algorithm?&lt;/h2&gt;
&lt;p&gt;A genetic algorithm is an optimization method that mimics evolution to optimize
a cost function. The entire set of the cost function parameters is called the
genome. Each parameter consists of a gene. Because GA mimics how evolution
works, they require a population of individuals. Each individual is nothing
more than a randomly initialized genome. Any GA starts optimizing a cost
function after initializing a population of genomes (individuals).&lt;/p&gt;
&lt;p&gt;In most GA implementations, an individual is a a data structure that holds a
genome (vector of bits, integers, floats, etc.), the corresponding cost to its
genome, a unique ID, a flag indicating whether the current individual is about
to mate (after a selection process), and other relevant information that the
developer deems necessary.&lt;/p&gt;
&lt;p&gt;When the GA optimizes a cost function, it usually applies three basic operators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt; This operator selects two individuals from a population
(&lt;em&gt;i.e.,&lt;/em&gt; a set of many individuals) to mate and eventually procreate. The
selected individuals are called parents. Some selection operators
are k-tournament, roulette-wheel, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Crossover&lt;/strong&gt; This operator mises the genomes of the selected parents. A
crossover operator will combine a part of the first parent&amp;rsquo;s genome with a
part of the second parent&amp;rsquo;s genome. Some crossover operators are one-point
crossover, two-points crossover, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mutation&lt;/strong&gt; Finally, the potential offspring&amp;rsquo;s (or child&amp;rsquo;s) genome is
subject to a mutation, which will further change the offspring&amp;rsquo;s genome.
Some of the most used mutation operators are delta, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically, a GA will repeat every generation&amp;rsquo;s operations mentioned above
(another fancy term for iteration). At every iteration, potential offspring
will replace their parents. Usually, the best-performing offspring will replace
the most poor-performing parents in terms of fitness. We call that kind of
replacement &amp;ldquo;elite&amp;rdquo; replacement. Another idea of replacing the parents is
randomly choosing some of the parents and replacing them.&lt;/p&gt;
&lt;p&gt;When the GA exhausts the predefined number of generations, the algorithm
terminates. We can evaluate its performance by inspecting the average fitness
(the average cost function value over all the individuals) or the best-so-far
fitness (BSF). Figure 2A shows a typical example of average fitness and 2B a
BSF. Upon the algorithm&amp;rsquo;s termination, the individual with the best fitness
function provides the genome (set of parameters) optimizes the cost function
[3].&lt;/p&gt;
&lt;h2 id=&#34;how-a-genetic-algorithm-works&#34;&gt;How a Genetic Algorithm works?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use an example to try to understand how GAs work.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s try to solve the XOR problem using a feed-forward neural network.  The
XOR (exclusive OR) is a binary operator that returns true if and only if the
operands are different. This means that if $ X = 1 $ (or $ X = 0 $) and $ Y = 0
$ (or $ Y = 1 $) then $ X \oplus Y = 1 $ (same when ). If both $ X $ and $ Y $
are zeros or ones, then $ X \oplus Y = 0 $.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/xor_net.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. XOR neural networ.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Our neural network will consist of two input units (since the XOR operator is a
binary one), two hidden units, and one output unit. Please see [CITE HERE] for
more details on why we choose such an architecture. Figure 2 shows the neural
network we are about to use.  Let&amp;rsquo;s optimize this neural network that will
serve as an XOR operator. To facilitate the demonstration, let&amp;rsquo;s split the
process into four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Define the cost function and the size of our genome.
Since the input to the XOR function is two-dimensional, we have a
genome of size two. Regarding the cost function, we first define an
error term as
$$ \epsilon = | y_{\text{target} - y_{\text{pred}}} | $$
where $ y_{\text{target} $ is the true value that the XOR function returns
and $x_{\text{pred}$ is the value of our network&amp;rsquo;s output. Since we have four
pairs of inputs $ (0, 1), (1, 0), (1, 1), (0, 0) $, and four real outputs
$1, 1, 0, 0$, respectively, we define the cost function as:
$$ f({\bf x}) = \sum_{i=1}^{4} \epsilon_i $$
Ideally, we expect the cost function to be zero to get our
optimal solution. That&amp;rsquo;s the case in Figure 2, where we see the best-so-far
fitness and the average fitness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once we have defined the cost function (that&amp;rsquo;s always the most challenging
part), we determine the number of genes per individual, which is $9$ in
this case. The neural network has the two input units connected via a $ 2
\times 2 $ matrix with the two hidden units. Next, the hidden units
connect via a $ 2\times 1 $ matrix to the output unit. Moreover, the
hidden and output units have a bias term (3 bias terms in total).
Therefore, the entire network has nine parameters (six weights and three
bias terms) we have to optimize. So, we initialize the neural network with
small random weights, define the number of individuals (population size)
to be $ 20 $, and define the size of the genome to be $ 9 $. Remember,
this is the number of cost function parameters to optimize. We set the
number of generations to $ 5000 $,  the number of offspring to $ 10 $, and
the replacement size to $ 5 $, which means $ 5 $ parents will be replaced
by $ 5 $ offsprings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We need to decide what operators we will use for our GA.
In this particular example, we use a k-tournament selection,
a one-point crossover, and a delta mutation operator.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;k-tournament selection&lt;/strong&gt; This operator will randomly choose
$ k $ individuals from the population. It will, then, choose the best
individual, based on the fitness from the tournament with probability
$ p $, choose the second-best individual with probability $ p(1-p) $,
the third-best with probability $ p(1-p)^2 $, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-point crossover&lt;/strong&gt; will take the genes of two parents as input,
it will randomly pick a number from zero to the size of genes, and it
will cut the parents&amp;rsquo; genes at that point. Then it will swap the
sliced genes between the two parents, and thus the offspring will carry
genetic information from both parents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delta mutation&lt;/strong&gt; operator will draw uniformly a number from the
interval $ [0, 1] $ for each gene in an individual&amp;rsquo;s genome. If that
number is greater than a probability $ p $ (usually $ p = \frac{1}{2} $).
A predefined increment will increase the value of the gene.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we set our Optimization in motion. The GA will first evaluate
the cost function of each individual. It will sort the individuals based
on their fitness values. To this end, we feed the XOR input to the neural
network, and we collect the output $ y_{\text{pred}} $. Then we use the
output to compute the cost function value for that individual. The next step
for our GA is to choose two parents based on the k-tournament selection. The
genome of the two selected parents will be crossed over using the
one-point operator. That will give rise to a new genome, a child or
offspring. The offspring&amp;rsquo;s genome will undergo a mutation based on the
delta operator. Finally, the GA will add the offspring to a list.  The
selection, crossover, and mutation processes continue until the number
of offspring have been exhausted. Then, the best performing offspring
will replace the poorest-performing (maximum cost function value)
parents (elite replacement). And the entire process repeats for $ 4999$
more generations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the convergence of the process described in step $ 4 $, the Optimization
of the XOR fitness terminates, and we can inspect the results. Figure 3 shows
the best-so-far fitness (BSF) and the average fitness, respectively. The first
observation is that the cost function value indeed converges to zero. Thus, our
GA&amp;rsquo;s best genome is optimal, and our neural network solves the XOR problem. The
second observation is that we could have used only $500$ generations for this
particular instance. The difficulty of the problem at hand usually determines
the number of generations and the initial values of the genome.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/bsf_avg_fit.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 3. BSF and average fitness for the optimization of the XOR fitness function.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Bellow, you can find the source code of the Python script we used to optimize
the XOR problem. As you can see, we combine Pytorch and PyGAIM to build
a neural network and optimize the weights and the biases.&lt;/p&gt;
&lt;p&gt;The first snippet shows what packages we need to import, the class of the
neural network, and a function that we will use to measure the accuracy
of our optimization process.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pylab &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; random &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; shuffle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; nn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home/gdetorak/packages/gaim/pygaim&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pygaim &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GAOptimize, c2numpy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;XOR_NET&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        super(XOR_NET, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sigmoid()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1(x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2(out)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate the XOR_NET class&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; XOR_NET()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# src: Input XOR&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# tgt: Output XOR&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;src &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;f&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;f&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;accuracy&lt;/span&gt;(genome):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Measures the accuracy of the XOR_NET. Runs over 100 times
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    and compares the network output against the target pattern
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    each time.      
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Assign genomes to network weights&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w1)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w2)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(src[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tgt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(dst[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net(inp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(tgt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            count &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# print(y.item(), tgt.item())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; / 100&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; count)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The fitness function takes the genome (C array) as input and
returns the negative of fitness value (or loss) since we perform
a minimization. It presents each time all four XOR patterns to
the neural network and computes the loss for each pattern.
Finally, it sums up all four individual losses and returns the
total loss.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fitness&lt;/span&gt;(x, length):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Fitness function. Receives the genome (x) from GAIM, passes it through
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    net.forward (Pytorch) and computes the absolute loss. 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c2numpy(x, length)				&lt;span style=&#34;color:#75715e&#34;&gt;# Convert C array to Numpy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]						&lt;span style=&#34;color:#75715e&#34;&gt;# First layer weights (2x2)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]					&lt;span style=&#34;color:#75715e&#34;&gt;# First layer bias (2X1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]					&lt;span style=&#34;color:#75715e&#34;&gt;# Second layer weights (2x1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;:]						&lt;span style=&#34;color:#75715e&#34;&gt;# Second layer bias (1x1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Assign the new values to network weights	&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w1)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w2)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;						
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shuffle(index)					&lt;span style=&#34;color:#75715e&#34;&gt;# shuffle the index&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# loop over all four patterns in a random order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; index:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_numpy(src[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tgt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_numpy(dst[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net(inp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; tgt[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; float(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item())			&lt;span style=&#34;color:#75715e&#34;&gt;# return -loss (minimize)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The final snippet provides the code to call the GAOptimize function
of PyGAIM, to plot the results and measure the performance (accuracy)
of the neural network on solving XOR.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    genome_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ga &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GAOptimize(fitness,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_generations&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    population_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    genome_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;genome_size,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_offsprings&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_replacements&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[float(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(genome_size)],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[float(&lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(genome_size)],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    mutation_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    mutation_var&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;.1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    genome, _, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ga&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ga&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    test_weights(genome)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;GAIM [4] is a C++ library for genetic algorithms and the island model.
It implements the most fundamental selection, crossover, and mutation
operators. It also provides an MPI and POSIX threads implementation
of the island model. Finally, it comes with a Python interface
called PyGAIM that simplifies GA-based optimization problem setup,
and PyGAIM provides a scikit-learn-like interface.&lt;/p&gt;
&lt;p&gt;For more information about GAIM, its source code, and examples,
you can visit its &lt;a href=&#34;https://github.com/gdetor/gaim&#34;&gt;Github&lt;/a&gt; repository.&lt;/p&gt;
&lt;h2 id=&#34;what-is-an-island-model&#34;&gt;What is an Island Model?&lt;/h2&gt;
&lt;p&gt;An island model is a computational method that runs multiple instances of GAs
on the same optimization problem in a distributed and parallel fashion. In some
cases, each island (another fancy word for process, thread, or computational
node in a cluster) can run a part of the optimization problem [5].&lt;/p&gt;
&lt;p&gt;What is essential in an island model is the periodic exchange of individuals
between islands. According to a predetermined time interval, a migration of a
subpopulation takes place. This circulation of individuals between islands
relies on specific communication protocols and predetermined topologies
(&lt;em&gt;e.g.,&lt;/em&gt;  ring topology). In this case, the islands are connected, forming a
ring, meaning the current island (node) connects to the one on its right (or
left).&lt;/p&gt;
&lt;p&gt;Figure 4 shows three basic topologies, (A) all-to-all, where every island
connects to all other ones, (B) ring, and (C) star topology, where one island
serves as a communication hub [5, 6, 7].&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/island_model.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 4. Island model topologies. (A) all-to-all, (B) ring, (C) star.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Each island begins with a population of $N$ individuals where $$ N =
\frac{K}{M} $$ where $ K $ is the number of all individuals, and $M$ is the
number of islands.
Every island will initialize a GA based on all the parameters and procedures
described earlier. When a time counter exhausts, migration takes place. The IM
algorithm selects a subpopulation on each island via some selection method
(random, elite - the best performing individuals, etc.). The selected genomes
move to the neighboring, connected island or islands. The newly arrived ones
replace local individuals via a replacement method (random, poor - the
worst-performing individuals, etc.). IM fills the vacant spots on the source
island with new offspring or randomly generated individuals [6]. The number of
individuals moved at every migration interval is called the &amp;ldquo;migration size.&amp;rdquo;
The reader can refer to [8, 9] for more information on how the migration
interval and the migration size affect the performance of an island model.&lt;/p&gt;
&lt;p&gt;The island model offers a means of faster convergence since each island can
potentially follow a different evolutionary trajectory covering different parts
of the search space. Furthermore, exchanging individuals between islands can
help the overall optimization process avoid being stuck in some local
minimum/maximum. That doesn&amp;rsquo;t mean that the island model is impervious to local
extrema.&lt;/p&gt;
&lt;h2 id=&#34;when-should-we-use-gas&#34;&gt;When should we use GAs?&lt;/h2&gt;
&lt;p&gt;As we have seen, GAs can optimize virtually any function given
a well-defined cost function (and that&amp;rsquo;s the most challenging part of using
GAs). However, there are some cases where we should try to use a GA
instead of any other conventional optimization method. These instances are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the search space is massive, then GAs are suitable for optimizing a
function in that space (for instance, non-linear functions).&lt;/li&gt;
&lt;li&gt;Another issue with Optimization is the cost function. The cost function may
be discontinuous (having gaps), or it may be non-differentiable. GAs do not
use derivatives and are therefore immune to such issues.&lt;/li&gt;
&lt;li&gt;When a cost function is too complex, GAs have more chances than the vanilla
optimization methods to avoid local minima and correctly find the global
optimum. A genetic algorithm can simultaneously explore the search space in
multiple directions, even if some offspring will never discover an optimal
solution to the problem.&lt;/li&gt;
&lt;li&gt;Finally, GAs are agnostic to the problem at hand. They do not require any
information about the system or the function they optimize to solve the
optimization problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-are-some-of-the-gas-applications&#34;&gt;What are some of the GAs applications?&lt;/h2&gt;
&lt;p&gt;Here, you can find some of the GA&amp;rsquo;s applications in real life. Although the
the following list is not complete; you will glimpse where and how GAs are used.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; As discussed in this post, GAs can optimize almost any function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; GAs can tune ML/DL models to discover optimal neural
network parameters. Moreover, they can design neural networks (searching
for the optimal neural network topology [10]).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Path and trajectory planning&lt;/strong&gt; GAs can aid in the designing and planning
of paths and trajectories for autonomous robotic platforms, vehicles, or
manipulators, such as robotic arms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNA Analysis&lt;/strong&gt; GAs can analyze the structure of DNA samples.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finance&lt;/strong&gt; GAs are an excellent tool for analyzing and forecasting stock prices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aerospace engineering&lt;/strong&gt; GAs can aid in the process of designing
aircraft.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Traveling salesman problem (TSP)&lt;/strong&gt; The
&lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;TSP&lt;/a&gt; is
well-defined in combinatorial Optimization and has many applications in
real-life issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022geneticalg,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Genetic algorithms and island models&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/genetics_algorithms&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;D. A. Pierre, &lt;em&gt;Optimization theory with applications&lt;/em&gt;,
Courier Corporation, 1986.&lt;/li&gt;
&lt;li&gt;I. Goodfellow, B. Yoshua, and A. Courville, &lt;em&gt;Deep learning&lt;/em&gt;,
MIT press, 2016.&lt;/li&gt;
&lt;li&gt;K. De Jong, &lt;em&gt;Evolutionary computation&lt;/em&gt;, Wiley Interdisciplinary Reviews:
Computational Statistics, 2009, 1.1: 52-56.&lt;/li&gt;
&lt;li&gt;G. Is. Detorakis and A. Burton, &lt;em&gt;GAIM: A C++ library for Genetic Algorithms
and Island Models&lt;/em&gt;, Journal of Open Source Software, 2019, 4.44: 1839.&lt;/li&gt;
&lt;li&gt;D. Whitley, S. Rana, and R.B. Heckendorn, &lt;em&gt;The island model genetic
algorithm: On separability, population size and convergence&lt;/em&gt;,
Journal of computing and information technology, 7:1, 33&amp;ndash;47, 1999.&lt;/li&gt;
&lt;li&gt;D. Sudholt, &lt;em&gt;Parallel evolutionary algorithms&lt;/em&gt;, Springer Handbook of
Computational Intelligence, 929&amp;ndash;959, 2015.&lt;/li&gt;
&lt;li&gt;W. N. Martin, J. Lienig, and J. P. Cohoon, &lt;em&gt;Parallel Genetic Algorithms Based
on Punctuated Equilibria&lt;/em&gt;, Handbook of Evolutionary Computation, IOP Publishing group,&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Z. Skolicki, &lt;em&gt;An analysis of island models in evolutionary computation&lt;/em&gt;,
Proceedings of the 7th annual workshop on Genetic and evolutionary computation,
386&amp;ndash;389, 2005.&lt;/li&gt;
&lt;li&gt;Z. Skolicki, and K. De Jong, &lt;em&gt;The influence of migration sizes and intervals
on island models&lt;/em&gt;, Proceedings of the 7th annual conference on Genetic and
evolutionary computation, 1295&amp;ndash;1302, 2005.&lt;/li&gt;
&lt;li&gt;K. O. Stanley, and R. Miikkulainen, &lt;em&gt;Efficient evolution of neural network
topologies&lt;/em&gt;, Proceedings of the 2002 Congress on Evolutionary Computation.
2, 1757&amp;ndash;1762, 2002.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Tips &amp; Tricks (Linux/Vim/Git/Programming)</title>
      <link>https://gdetor.github.io/posts/tipsntricks/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/tipsntricks/</guid>
      <description>In this post you can find various simple tricks and tips for Linux, Vim, Git, Python and many other stuff. All the material provided in this page are free and can be redistributed and/or modified. There is no any warranty that they work for you or are suitable to your need. The author of this page is not responsible for any damage this material may cause.
Joining dictinary keys and values to a list in Python (Programming) &amp;#39;_&amp;#39;.</description>
      <content>&lt;p&gt;In this post you can find various simple tricks and tips for Linux, Vim,
Git, Python and many other stuff. All the material provided in this page are
free and can be redistributed and/or modified. There is no any warranty that
they work for you or are suitable to your need. The author of this page is
not responsible for any damage this material may cause.&lt;/p&gt;
&lt;h3 id=&#34;joining-dictinary-keys-and-values-to-a-list-in-python-programming&#34;&gt;Joining dictinary keys and values to a list in Python (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{!s}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{!r}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(key, val) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (key, val) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;check-the-health-status-of-your-hard-drive-linux&#34;&gt;Check the health status of your hard drive (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ badblocks -w -s -o error.log /dev/sdX     &lt;span style=&#34;color:#75715e&#34;&gt;# get X using lsblk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-load-reload-modules-ipython&#34;&gt;How to load (reload) modules (iPython)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;load_ext autoreload
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;autoreload &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;power-off-an-external-hard-drive-linux&#34;&gt;Power-off an external hard drive (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ udiskctl power-off -b /dev/sdX    &lt;span style=&#34;color:#75715e&#34;&gt;# obtain X using lsblk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rsync-your-data-linux&#34;&gt;rsync your data (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ rsync -avzhP src dst --exclude-from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;file.txt --inlude&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.git/config&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;recover-data-linux&#34;&gt;Recover data (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ddrescue -dr3 /dev/sdX imagename.image logfile
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;random-execution-of-a-linux-command-linux&#34;&gt;Random execution of a Linux command (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;$((&lt;/span&gt;RANDOM &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;))&lt;/span&gt; -eq &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; command
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;open-a-file-in-vim-with-no-compatibility-vim&#34;&gt;Open a file in VIM with no compatibility (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ vim -u NONE -u NORC fname
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;io-devices-latency-linux&#34;&gt;IO devices latency (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo ioping -R /dev/sdX         &lt;span style=&#34;color:#75715e&#34;&gt;# (seek rate)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo ioping -RL /dev/sdX        &lt;span style=&#34;color:#75715e&#34;&gt;# (sequential speed)    &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;copy-a-file-to-the-clipboard-linux&#34;&gt;Copy a file to the clipboard (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ xclip -sel clip &amp;lt; file
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;import-date-in-vim-vim&#34;&gt;Import date in VIM (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-vim&#34; data-lang=&#34;vim&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;:&lt;span style=&#34;color:#a6e22e&#34;&gt;read&lt;/span&gt; !&lt;span style=&#34;color:#a6e22e&#34;&gt;date&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;import-the-output-of-a-linux-command-in-vim-vim&#34;&gt;Import the output of a Linux command in Vim (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-vim&#34; data-lang=&#34;vim&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; @&lt;span style=&#34;color:#a6e22e&#34;&gt;a&lt;/span&gt;=&lt;span style=&#34;color:#a6e22e&#34;&gt;system&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;In&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;press&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Ctrl&lt;/span&gt;+&lt;span style=&#34;color:#a6e22e&#34;&gt;R&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;then&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;convert-a-bunch-of-png-files-to-a-video-linux&#34;&gt;Convert a bunch of png files to a video (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ffmpeg -framerate 1/5 -patter_type glob -i &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.png&amp;#34;&lt;/span&gt; -vf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fps=95,format=yuv420p&amp;#34;&lt;/span&gt; output.mp4
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;convert-a-photo-into-ascii-art-linux&#34;&gt;Convert a photo into ASCII art (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ jp2a photo.jpg | tee photo.ascii
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;read-gz-files-without-extracting-them-linux&#34;&gt;Read .gz files without extracting them (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ zmore file.gz
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ zless file.gz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-load-huge-files-in-python-programming&#34;&gt;How to load huge files in Python (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mmap
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HUGE.txt&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mmap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mmap(f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filend, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, access&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mmap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ACCESS_READ, offset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mm[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;:])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;git-credential-storage-git&#34;&gt;Git credential storage (Git)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git config --global credential.helper cache &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--timeout S&amp;#39;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# S -&amp;gt; seconds&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;include-a-help-flag-in-makefiles-programming&#34;&gt;Include a help flag in Makefiles (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-make&#34; data-lang=&#34;make&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;.PHONY&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    help
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;help&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cat makefile | grep -oP &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;^#\K(.*)&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;copy-a-file-into-multiple-directories-linux&#34;&gt;Copy a file into multiple directories (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo dir1 dir2 | xargs -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; cp file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;parallel cp file ::: dir1 dir2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-run-a-elf-programming&#34;&gt;How to run a ELF (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ touch file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ cp /bin/ls .
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ chmod -x ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;-bash: ./ls: Permission denied
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ strings ./ls | head -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/lib64/ld-linux-x86-64.so.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ /lib64/ld-linux-x86-64.so.2 ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file ls
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;set-an-alert-message-when-a-job-finishes-linux&#34;&gt;Set an alert message when a job finishes (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt; execute_something_that_takes_time; xmessage DONE; &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&amp;amp;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;dennis-ritchie-linuxprogrammin&#34;&gt;Dennis Ritchie (Linux/Programmin)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ curl -L git.io/unix
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;play-tetris-linux&#34;&gt;Play tetris (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ssh netris&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;rocketnine&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;space
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;python-scope-programming&#34;&gt;Python scope (Programming)&lt;/h3&gt;
&lt;p&gt;LEGB:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;ocal (within a function)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;nclosing functions locals&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G&lt;/strong&gt;lobal (module)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;uilt-in (python) preassigned names&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c-pointers-programming&#34;&gt;C Pointers (Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ptr++ Evaluates ptr and then increments ptr by 1xbase_type_size&lt;/li&gt;
&lt;li&gt;*ptr++ Evaluates ptr, increments it, and dereferences the evaluated value&lt;/li&gt;
&lt;li&gt;int *ptr[10] Array of 10 pointers to integer (int *)&lt;/li&gt;
&lt;li&gt;int (*ptr)[10] Pointer to an array of 10 integers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;delete-the-last-column-of-a-text-file-linux&#34;&gt;Delete the last column of a text file (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NF {NF-=1};1&amp;#39;&lt;/span&gt;&amp;lt; input_file &amp;gt; output_file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NF {NF--};1&amp;#39;&lt;/span&gt;&amp;lt; input_file &amp;gt; output_file
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;automatic-variables-in-make-programming&#34;&gt;Automatic variables in Make (Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$@: The target filename.&lt;/li&gt;
&lt;li&gt;$*: The target filename without the file extension.&lt;/li&gt;
&lt;li&gt;$&amp;lt;: The first prerequisite filename.&lt;/li&gt;
&lt;li&gt;$^: The filenames of all the prerequisites, separated by spaces, discard duplicates.&lt;/li&gt;
&lt;li&gt;$+: Similar to $^, but includes duplicates.&lt;/li&gt;
&lt;li&gt;$?: The names of all prerequisites that are newer than the target, separated by spaces.&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Research Demos</title>
      <link>https://gdetor.github.io/posts/demos/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/demos/</guid>
      <description>The following videos demonstrate a computation model of primary somatosensory cortex undergoing self-organization. The model relies on Neural Field [1]. The complete mathematical/computational model as well as all the details and results are given in [2] and [3].
 Self-organization of receptive fields (RFs)    Evolution of a single RF during self-organization    Evolution of multiple RFs during self-organization    References  Dynamics of pattern formation in lateral-inhibition type neural fields A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance and Reorganization of Ordered Topographic Maps Structure of receptive fields in a computational model of area 3b of primary sensory cortex  </description>
      <content>&lt;p&gt;The following videos demonstrate a computation model of primary somatosensory
cortex undergoing self-organization. The model relies on Neural Field [1].
The complete mathematical/computational model as well as all the details
and results are given in [2] and [3].&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;self-organization-of-receptive-fields-rfs&#34;&gt;Self-organization of receptive fields (RFs)&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/JU0PKFpagUo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;evolution-of-a-single-rf-during-self-organization&#34;&gt;Evolution of a single RF during self-organization&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UzosJK8YOU0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;evolution-of-multiple-rfs-during-self-organization&#34;&gt;Evolution of multiple RFs during self-organization&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/F8JOQs2MYN4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF00337259&#34;&gt;Dynamics of pattern formation in lateral-inhibition type neural fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance and Reorganization of Ordered Topographic Maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/full&#34;&gt;Structure of receptive fields in a computational model of area 3b of primary sensory cortex&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Tweets</title>
      <link>https://gdetor.github.io/posts/news/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/news/</guid>
      <description>&amp;quot;I, Pencil&amp;quot; An essay published in 1958 which beautifully describes the amount of human coordination, cooperation and trade that needs to happen for the creation of a seemingly simple object, a lead pencil. ✏️
It&amp;#39;s a great read: https://t.co/P4Ha7HpoGp pic.twitter.com/vPPZuCZ5cB
&amp;mdash; Fermat&amp;#39;s Library (@fermatslibrary) April 6, 2021  </description>
      <content>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;I, Pencil&amp;quot; &lt;br&gt;&lt;br&gt;An essay published in 1958 which beautifully describes the amount of human coordination, cooperation and trade that needs to happen for the creation of a seemingly simple object, a lead pencil.  ✏️&lt;br&gt;&lt;br&gt;It&amp;#39;s a great read: &lt;a href=&#34;https://t.co/P4Ha7HpoGp&#34;&gt;https://t.co/P4Ha7HpoGp&lt;/a&gt; &lt;a href=&#34;https://t.co/vPPZuCZ5cB&#34;&gt;pic.twitter.com/vPPZuCZ5cB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Fermat&amp;#39;s Library (@fermatslibrary) &lt;a href=&#34;https://twitter.com/fermatslibrary/status/1379424914445377536?ref_src=twsrc%5Etfw&#34;&gt;April 6, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</content>
    </item>
    
  </channel>
</rss>
