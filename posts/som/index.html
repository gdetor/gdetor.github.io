<!doctype html>







































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>An introduction to self-organizing maps - GID</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="This post presents the classical self-organizing map algorithm proposed by Grossberg [1] and Teuvo Kohonen [2]. We explain the algorithm&rsquo;s fundamental aspects and applications and offer a basic implementation in Pytorch.
Introduction Let us begin with a prevalent problem in science. We often have to deal with data that live in a high-dimensional space $\mathcal{X} \in \mathbb{R}^k$. When $k &gt; 3$, things get rough for people who need help to visualize what&rsquo;s happening." />
  <meta name="author" content="Georgios Is. Detorakis" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://gdetor.github.io/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://gdetor.github.io/theme.png" />

  
  
  
  
  <link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/7115299?v=4" />
  
  

  
  
  <link rel="preload" as="image" href="https://gdetor.github.io/github.svg" />
  
  <link rel="preload" as="image" href="https://gdetor.github.io/linkedin.svg" />
  
  

  
  
  <script
    defer
    src="https://gdetor.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  
  <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>

<script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>

  
  
  

  
  <link rel="icon" href="https://gdetor.github.io/icons/favicon.ico" />
  <link rel="apple-touch-icon" href="https://gdetor.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.92.2" />

  
  
  
  
  
  <meta itemprop="name" content="An introduction to self-organizing maps">
<meta itemprop="description" content="This post introduces the classical self-organizing map (SOM) algorithm proposed by Grossberg and later by Kohonen. We explain the algorithm&#39;s fundamental aspects and implement it on Pytorch."><meta itemprop="datePublished" content="2023-12-29T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-12-29T00:00:00+00:00" />
<meta itemprop="wordCount" content="2125">
<meta itemprop="keywords" content="SOM,Kohonen,neural networks,self-organizing map,pytorch," />
  
  <meta property="og:title" content="An introduction to self-organizing maps" />
<meta property="og:description" content="This post introduces the classical self-organizing map (SOM) algorithm proposed by Grossberg and later by Kohonen. We explain the algorithm&#39;s fundamental aspects and implement it on Pytorch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gdetor.github.io/posts/som/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-12-29T00:00:00+00:00" />


  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An introduction to self-organizing maps"/>
<meta name="twitter:description" content="This post introduces the classical self-organizing map (SOM) algorithm proposed by Grossberg and later by Kohonen. We explain the algorithm&#39;s fundamental aspects and implement it on Pytorch."/>

  
  
  
  <link rel="canonical" href="https://gdetor.github.io/posts/som/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://gdetor.github.io"
      >GID</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >About</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/research/"
        >Research</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/software/"
        >Software</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/publications/"
        >Publications</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/artwork/"
        >Artwork</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/gdetor"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./linkedin.svg)"
        href="https://linkedin.com/in/georgiosdetorakis"
        target="_blank"
        rel="me"
      >
        linkedin
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">An introduction to self-organizing maps</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Dec 29, 2023</time>
      
      
      
      
      <span class="mx-1">&middot;</span>
      <span>Georgios Is. Detorakis</span>
      <span class="mx-1">&middot;</span>
      <span class="inline-item reading">10 minutes read</span>
      
    </div>
    
  </header>

  <section><div style="text-align: justify;">
<p>This post presents the classical self-organizing map algorithm proposed by Grossberg [1] and Teuvo Kohonen [2]. We explain the algorithm&rsquo;s fundamental aspects and applications and offer a basic implementation in Pytorch.</p>
<h2 id="introduction">Introduction</h2>
<p>Let us begin with a prevalent problem in science. We often have to deal with
data that <em>live</em> in a high-dimensional space $\mathcal{X} \in \mathbb{R}^k$.
When $k &gt; 3$, things get rough for people who need help to visualize
what&rsquo;s happening. Moreover, many algorithms cannot operate fast
enough in high-dimensional spaces [3, 4]. Therefore, we rely on methods
that reduce the dimensionality without compromising or losing much information.</p>
<p>One such method is a self-organizing map or SOM. SOM is an unsupervised
learning algorithm that maps a high-dimensional space into a lower-dimensional
one through an artificial neural network.
In a more mathematical context, the idea behind an unsupervised learning
problem and mainly a self-organizing map, is to learn the input
distribution, meaning we are looking at an approximation for the joint
distribution $p(x, y)$, where $x$ represents input samples and $y$ the output,
without making any assumptions about causality.</p>
<h3 id="applications">Applications</h3>
<p>We have already seen the dimensionality reduction problem, where the data we
handle are usually high-dimensional, rendering their analysis, processing, or
interpretation hard. Therefore, we can use a SOM to map the original input
space to a low-dimensional space and work with the reduced data [2].
Another feature of the SOM is that it retains the topographic properties of the
input distribution (space). The code words are topographically organized,
meaning that the neighboring units of the map represent data that are nearby in
the input space.
For instance, classifying world poverty is a problem where clusters (or groups
of units/neurons) represent poor, rich, or in-between countries [http://www.cis.hut.fi/research/som-research/worldmap.html].
There are many other applications, but providing an exhaustive list is out of
the scope of this post.</p>
<h3 id="notation-and-terminology">Notation and Terminology</h3>
<p>Before diving into the details and implementing the SOM algorithms,
we must define some mathematical quantities and determine out terminology.
The SOM is implemented as a neural network $\mathcal{F}({\bf \theta})$
that maps a high-dimensional manifold $ \mathcal{X} \in \mathbb{R}^k $
to a low-dimensional predefined lattice $\mathcal{Y} \in \mathbb{R}^m$, where
$ m \ll k $. The input to the SOM is a vector $ {\bf x} \in \mathcal{X} $, and
the weights, $ {\bf w} $, of the neural network that learns the representations
are called <em>code words</em> and the set of the code words,
$ \mathcal{W} = {\bf w_1, \ldots, w_n} $, is called a <em>codebook</em>.
The cardinal number of $\mathcal{W}$ is the number of units we use in the
low-level lattice or the number of neural network units.
We let $ d(\cdot, \cdot) $ be the Minkowski distance
defined as
$$ d({\bf x}, {\bf y}) = \Big( \sum_{i=1}^{k} |x_i - y_i|^p \Big)^{\frac{1}{p}}, \qquad (1) $$
where here ${\bf x}, {\bf y} \in \mathbb{R}^n $ and $ p \geq 1$. For different values of $ p $, we recover different known
distance functions such as the Euclidean distance ($p=2$) or the Manhattan ($p=1$).</p>
<h2 id="the-som-algorithm">The SOM Algorithm</h2>
<p>We are now ready to introduce the SOM algorithm. First, we will describe the
main idea behind the SOM algorithm in plain words, and then we will give the
pseudocode and its implementation along with some examples.</p>
<figure><img src="/images/somtraining.png"
         alt="Figure 1. Illustration of SOM algorithm steps. At the beginning (t=0), we see that the winner unit (BMU, center of yellow disc) that is closer to the current sample (white disc) has been identified. The SOM algorithm adjusts the codebook based on a neighborhood function (yellow disc) so the map will come closer to the sample. After all the iterations had been exhausted, the SOM learned the representation (black grid) of the input distribution (blue cloud). This figure is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported (Wikipedia)."/><figcaption>
            <p>Figure 1. Illustration of SOM algorithm steps. At the beginning (t=0), we see that the winner unit (BMU, center of yellow disc) that is closer to the current sample (white disc) has been identified. The SOM algorithm adjusts the codebook based on a neighborhood function (yellow disc) so the map will come closer to the sample. After all the iterations had been exhausted, the SOM learned the representation (black grid) of the input distribution (blue cloud). This figure is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported (Wikipedia).</p>
        </figcaption>
</figure>
<p>The first step is to initialize the codebook; we usually do that by assigning
random values to the code words. Once we initialize the codebook, we start the
learning iteration. At each iteration or epoch, we draw a sample ${\bf x}$ from
the input space $ \mathcal{X} $ and compute the distance of that sample
from all the code words. We select the code word with the smallest distance
from ${\bf x}$ such that
$$ i_{\text{BMU}} = \text{BMU} = \text{arg}\min_{i\in \mathbb{N}^+} \left\{ d({\bf x}, {\bf w}^t_i) \right\}, \qquad (2) $$</p>
<p>and we call that unit (neuron) the best matching unit (BMU) or the winner unit.</p>
<p>Figure 1 shows a random initialization of the codebook (black grid) that contains
two-dimensional code words ($ \mathcal{X} \in \mathbb{R}^2$). The white disc
represents the sample ${\bf x}$ we drew. After we compute all the distances
between that sample and all the grid points, we observe that the closest point
of the grid to that sample is the one at the center of the yellow disc.</p>
<p>So now the next step is to update the code words ${\bf w}_i$ ($i = 1, \ldots, n$)
by moving them towards the sample ${\bf x}$ according to the following equation:</p>
<p>$$ {\bf w}_i^{t+1} = {\bf w}_i^t + \epsilon(t) h(t, \text{BMU}, \sigma(t))({\bf x} - {\bf w}_i^t), \qquad(3) $$</p>
<p>and updating the code words that fall within a neighborhood defined by the function:</p>
<p>$$ h(t, \text{BMU}; \sigma(t)) = \exp\Big( \Big( \frac{d({\bf w}^t_{\text{BMU}}, {\bf w}^t_i)}{\sigma(t)} \Big)^2 \Big), \qquad(4) $$</p>
<p>where $t$ is the current iteration (or time step or epoch). $\epsilon(t)$ and
$\sigma(t)$ are the time-dependent learning rate and
neighborhood function&rsquo;s variance, respectively. They both decay exponentially
based on $ z_i \Big( \frac{z_f}{z_i} \Big)^{\frac{t}{t_f}} $, where $z_i$ and
$z_f$ are the initial and final values, respectively ($z = {\epsilon, \sigma}$).
We assume $z_i \gg z_f$ for the initial and final values. The idea behind
a time-dependent decaying learning rate is to freeze learning after some time,
and a decaying variance of the neighborhood function guarantees that as the
learning progresses, the number of units that get to update their weights
decreases. Moreover, at the beginning of learning, the radius of the
neighborhood function should be large enough. At the end of learning,
we should keep the radius small such that only one or zero neighbors are within
the immediate vicinity of the BMU.</p>
<p>Figure 2 shows the pseudocode of the SOM algorithm. The algorithm presented
here is an online learning algorithm (or sequential), meaning that one sample
of data is processed at each iteration. A different version of the SOM
algorithm runs offline and uses the entire data set at each iteration, called
batch SOM. In this post, we will implement the online algorithm used in our
examples.</p>
<figure><img src="/images/som_algo.png"
         alt="Figure 2. Self-organizing map basic algorithm."/><figcaption>
            <p>Figure 2. Self-organizing map basic algorithm.</p>
        </figcaption>
</figure>
<h2 id="pytorch-implementation">Pytorch Implementation</h2>
<p>The source code below implements the SOM algorithm given in
Figure 2. It is a simple <em>translation</em> of the pseudocode to
Python. In this implementation, we contain all the parameters and the codebooks
in $[0, 1]^k$, where $k \in \mathbb{N}^+$. We also consider the neighborhood
function a Gaussian, and the final time $t_f = 1$. The metric $d(\cdot, \cdot)$
is the Euclidean distance ($p=2$ in Equation 1).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">import</span> matplotlib.pylab <span style="color:#66d9ef">as</span> plt

<span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SOM</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self,
                 n_units<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
                 dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
                 iters<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
                 lrate<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.01</span>),
                 sigma<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.01</span>)):
        super()<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>n_units <span style="color:#f92672">=</span> n_units				<span style="color:#75715e"># Number of units (n in main text)</span>
        self<span style="color:#f92672">.</span>dim <span style="color:#f92672">=</span> dim						<span style="color:#75715e"># Code words dim (k in main text)</span>
        self<span style="color:#f92672">.</span>iters <span style="color:#f92672">=</span> iters
        self<span style="color:#f92672">.</span>lrate_i <span style="color:#f92672">=</span> lrate[<span style="color:#ae81ff">0</span>]
        self<span style="color:#f92672">.</span>lrate_f <span style="color:#f92672">=</span> lrate[<span style="color:#ae81ff">1</span>]
        self<span style="color:#f92672">.</span>sigma_i <span style="color:#f92672">=</span> sigma[<span style="color:#ae81ff">0</span>]
        self<span style="color:#f92672">.</span>sigma_f <span style="color:#f92672">=</span> sigma[<span style="color:#ae81ff">1</span>]

		<span style="color:#75715e"># Initialize the code words</span>
        self<span style="color:#f92672">.</span>initCodebook()
        
		<span style="color:#75715e"># Initialize a two-dimensional grid</span>
        self<span style="color:#f92672">.</span>initLatice()

		<span style="color:#75715e"># Pre-compute the time array, learning rate, and variance</span>
        self<span style="color:#f92672">.</span>t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, steps<span style="color:#f92672">=</span>iters)
        self<span style="color:#f92672">.</span>lrate <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrate_i <span style="color:#f92672">*</span> (self<span style="color:#f92672">.</span>lrate_f <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>lrate_i)<span style="color:#f92672">**</span>self<span style="color:#f92672">.</span>t
        self<span style="color:#f92672">.</span>sigma <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sigma_i <span style="color:#f92672">*</span> (self<span style="color:#f92672">.</span>sigma_f <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>sigma_i)<span style="color:#f92672">**</span>self<span style="color:#f92672">.</span>t

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initCodebook</span>(self):
    	<span style="color:#e6db74">&#34;&#34;&#34; Initialize the code words using random values drawn from a 
</span><span style="color:#e6db74">    	uniform distribution in [0, 0.01).
</span><span style="color:#e6db74">    	&#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>codebook <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones([self<span style="color:#f92672">.</span>n_units<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>n_units, self<span style="color:#f92672">.</span>dim])
        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>uniform_(self<span style="color:#f92672">.</span>codebook, a<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, b<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initLatice</span>(self):
    	<span style="color:#e6db74">&#34;&#34;&#34; Initialize a two-dimensional grid (latice) on which units
</span><span style="color:#e6db74">    	(or neurons) are placed. Moreover, computes the distances between all
</span><span style="color:#e6db74">    	the pairs within the grid.
</span><span style="color:#e6db74">    	
</span><span style="color:#e6db74">    	&#34;&#34;&#34;</span>
        line <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, steps<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>n_units)
        grid_x, grid_y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>meshgrid(line, line, indexing<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ij&#34;</span>)
        p <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((grid_x<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>),
                       grid_y<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        self<span style="color:#f92672">.</span>dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cdist(p, p)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X):
    	<span style="color:#e6db74">&#34;&#34;&#34; Fit the input data X to a two-dimensional SOM.
</span><span style="color:#e6db74">    	&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>iters):
        	
        	<span style="color:#75715e"># Sample the input space</span>
        	x <span style="color:#f92672">=</span> X[np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])]			

			<span style="color:#75715e"># Find the BMU</span>
            BMU <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmin((((self<span style="color:#f92672">.</span>codebook <span style="color:#f92672">-</span> x))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))

			<span style="color:#75715e"># Compute the neighborhood function</span>
            h <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>(self<span style="color:#f92672">.</span>dist[BMU]<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>sigma[t])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)

			<span style="color:#75715e"># Update the codebook</span>
            self<span style="color:#f92672">.</span>codebook <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>lrate[t] <span style="color:#f92672">*</span> h <span style="color:#f92672">*</span> (self<span style="color:#f92672">.</span>codebook <span style="color:#f92672">-</span> x)

</code></pre></div><h2 id="examples">Examples</h2>
<figure><img src="/images/som_maps.png"
         alt="Figure 3. Self-organizing maps. (A) Shows a self-organizing map  of the unit square $[0, 1]^2$, and (B) shows a a map for a ring in $[-1, 1]^2$."/><figcaption>
            <p>Figure 3. Self-organizing maps. (A) Shows a self-organizing map  of the unit square $[0, 1]^2$, and (B) shows a a map for a ring in $[-1, 1]^2$.</p>
        </figcaption>
</figure>
<h3 id="square-in--mathbbr2-">Square in $ \mathbb{R}^2 $</h3>
<p>Our first example is to map a two-dimensional rectangular distribution in the
$[0, 1] \times [0, 1]$. To this end, we randomly generate points
$(x_1, x_2) \in [0, 1] \times [0, 1]$ from a uniform distribution $\mathcal{U}$.
After running the code presented earlier for $3000$ iterations and with
$\epsilon_i = \sigma_i = 0.5$ and $\epsilon_f = \sigma_f = 0.01$, we obtain the
self-organized map illustrated in Figure 3A, where the red dots represent the
input space (and thus the samples we used to train our neural network). The
white discs are the units of our SOM.</p>
<h3 id="ring-in--mathbbr2-">Ring in $ \mathbb{R}^2 $</h3>
<p>The second example is again related to space $[0, 1] \times [0, 1] $, only this
time we draw uniformly points from a ring, rendering this problem a little
bit harder for the SOM algorithm since there is a hole in the center of our
distribution. Figure 3B shows the input space distribution in red dots, and
again, we see that the SOM algorithm has captured the input space. The code
words (white discs) cover the space, and the lattice (black solid lines) are
well organized. However, we see some units representing the hole (void) in the
middle of the ring. That is a known problem with Kohonen&rsquo;s SOM algorithm.
The parameters for this experiment are the same as before.</p>
<h2 id="evaluate-a-som">Evaluate a SOM</h2>
<p>When measuring the quality of a self-organized map, there is no one measure to
rule them all. People use many different measures in the literature to measure
how good or bad a self-organized map is. Three well-known measures are:</p>
<ul>
<li>
<p>The quantization error (or distortion), which is the average distance of the
minimum of all the pairwise distances between the input space samples
$\mathcal{X}$ and the codebook $\mathcal{W}$,
$ \mathcal{E_{Q}} = \mathbb{E}[\min\left\{ \mathcal{D}(\mathcal{X}, \mathcal{W}) \right\} $.
The quantization error does not measure how well the map preserves the data&rsquo;s
topology but instead measures the goodness-of-fit of the input distribution
by the map. The lower the $\mathcal{E_Q}$, the better.</p>
</li>
<li>
<p>The topographic error quantifies how well the map captures the shape of the
input data and how well the topology of the map preserves the input space.
The topographic error is computed as
$\mathcal{E_{T}} = \frac{1}{|\mathcal{X}|} \sum_{{\bf x} \in \mathcal{X}}^{}I({\bf x}) $,
where $I({\bf x}) = 1$ when the BMU and the second best matching unit are
neighbors, and zero otherwise. The lower the value of the $\mathcal{E_{T}}$,
the better.</p>
</li>
</ul>
<p>In our examples above, we measure the quantization and topographic errors.
As Table 1 shows, the errors for the square distribution are indeed minor,
meaning that the maps capture and preserve the topology of the input distribution.
However, the quantization error is not that small for the ring distribution,
meaning that the map preserves the topology since the topographic error is small.
Still, the map needs to fit the data distribution better. We can see that the
map also tries to find non-existent data in the ring&rsquo;s hole.</p>
<div class="data-table" role="region" tabindex="0" aria-labelledby="table-caption-table1">
  <table class="table optional CSS class declaration" id="table1" itemscope itemtype="https://schema.org/Table"><caption id="table-caption-table1" itemprop="about"><b>Table</b> ${\bf 1}$ SOM Errors. Quantization $\mathcal{E_Q}$ and topographic $\mathcal{E_T}$ errors for the square and ring distributions.</caption>
<thead>
<tr>
<th style="text-align:center">Example</th>
<th style="text-align:center">$\mathcal{E_Q} $</th>
<th style="text-align:center">$\mathcal{E_{T}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Square</td>
<td style="text-align:center">$0.034$</td>
<td style="text-align:center">$0.028$</td>
</tr>
<tr>
<td style="text-align:center">Ring</td>
<td style="text-align:center">$0.255$</td>
<td style="text-align:center">$0.04$</td>
</tr>
</tbody>
</table>
</div>
<h2 id="summary">Summary</h2>
<p>In this post, we introduced some fundamental ideas of self-organizing maps.
We tried to convey how the SOM algorithm works and provided a
simple Python implementation using Pytorch. Finally, we tested our implementation
on two basic examples: a two-dimensional distribution of a rectangle and one of
a two-dimensional disc. In both cases, we see how the SOM algorithm learns the
representations and correctly maps the input distribution.</p>
<h3 id="source-code">Source Code</h3>
<p>The entire source code for the experiments used in this post is freely available
<a href="https://gist.github.com/gdetor/379c2e3897d474894f42735b5b1ba641">here</a>.</p>
</div>
<h3 id="cite-as">Cite as</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-latex" data-lang="latex">@article{detorakis2023som,
  title   = &#34;An introduction to self-organizing maps&#34;,
  author  = &#34;Georgios Is. Detorakis&#34;,
  journal = &#34;gdetor.github.io&#34;,
  year    = &#34;2023&#34;,
  url     = &#34;https://gdetor.github.io/posts/som&#34;
}
</code></pre></div><h3 id="references">References</h3>
<ol>
<li>Grossberg S., <em>Physiological interpretation of the self-organizing map algorithm</em>, 1994.</li>
<li>Kohonen T., <em>Self-organized formation of topologically correct feature maps</em>, Biol Cybern., 1982;43(1):59–69.</li>
<li>Bellman R. E., <em>Dynamic programming</em>, Princeton University Press, Rand Corporation (1957).</li>
<li>Bellman R. E., <em>Adaptive control processes: a guided tour</em>, Princeton University Press. ISBN 9780691079011, 1961.</li>
<li>Ponmalai, R., and Kamath, C., <em>Self-organizing maps and their applications to data analysis</em>, Lawrence Livermore National Lab.(LLNL), Livermore, CA, 2019.</li>
</ol>
</section>

  
  
  <footer class="mt-12 flex flex-wrap">
     
    <a
      class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]"
      href="https://gdetor.github.io/tags/som"
      >SOM</a
    >
     
    <a
      class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]"
      href="https://gdetor.github.io/tags/kohonen"
      >Kohonen</a
    >
     
    <a
      class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]"
      href="https://gdetor.github.io/tags/neural-networks"
      >neural networks</a
    >
     
    <a
      class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]"
      href="https://gdetor.github.io/tags/self-organizing-map"
      >self-organizing map</a
    >
     
    <a
      class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]"
      href="https://gdetor.github.io/tags/pytorch"
      >pytorch</a
    >
    
  </footer>
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://gdetor.github.io/posts/tipsntricks/"
      ><span class="mr-1.5">←</span><span>Tips &amp; Tricks (Linux/Vim/Git/Programming)</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://gdetor.github.io/posts/cloudlab/"
      ><span>Organs-on-Chip-as-a-Service: A proposal bridging cloud services and scientific experiments</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://gdetor.github.io">GID</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
