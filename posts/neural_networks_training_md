+++
title = "Tips and tricks on neural networks training"
date = "2022-10-13"
author = "Georgios Is. Detorakis"
tags = ["neural networks", "training neural networks", "practical guide of neural network training", "deep neural networks"]
description = "Blog post on training neural networks"
math = "true"
draft = "true"
+++


In this post, we are going through the training process of (deep) neural
networks. We will briefly introduce the notions of neural network and deep 
neural network, backpropagation, and loss function. For the examples presented
in this post we use Python and Pytorch [CITE] and as for the hyperparameters
tuning we rely on Ray [CITE]. 

*Disclosure:* This is not a thoroough introduction to neural networks, machine/
deep learning or artificial inteligence. The sole purpose of the present post
is to summarize several training methods and common mistakes people commit when
they train neural networks. The reader is refer to [CITE BOOKS HERE AND PAPERS]
for detailed introduction to all the aformentioned fields. 


## What is a Neural Network

The story of neural networks goes way back in the [50s?], when 

### Types of Aritificial Neural Networks (ANNs)


### Deep Neural Networks (DNNs)

A Deep Neural Network (DNN from now on), is a type of artificial neural network
with many hidden layers. Most researchers and practicioners agree that a 
NN with XX hidden layers is consindered to be a DNN.

### Backpropagation

### Types of Continuous Neural Networks (CNNs)

In this category lies a vast variety of neural networks that are being used
in modeling biological neural tissue. 


From now on we focus only on the training of ANNs. 

## Training Neural Networks


### Architecture - Activation functions


### Initialization


### Optimal learning rate


### Hyperparameters tuning


### Penalize overestimation


### Optimization


## Regularization Techniques


### Dataset Augmentation


### Early Stopping

### Batch Normalization


### Weight Normalization


### Dropout/Dropconnect


## Common mistakes in training neural networks

Here is a list with the most common mistakes in training neural networks:
  - Not using the `train/eval` methods
  - Not zeroing the grad before the backward phase (`zero_grad()`)
  - Passing softmaxed outputs to a loss function that expects logits
  - Not setting bias to zero when batch normalization is enabled
  - Not using `reshape` instead of `view`


## Summary

ADD TEXT



### Cited as
```latex
@article{detorakis2022acfpacf,
  title   = "Tips and tricks on neural networks training",
  author  = "Georgios Is. Detorakis",
  journal = "gdetor.github.io",
  year    = "2022",
  url     = "https://gdetor.github.io/posts/neural_networks_training"
}
```


### References
