<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GID Webpage</title>
    <link>https://gdetor.github.io/</link>
    <description>Recent content on GID Webpage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ⓒ  GID, 2021-2022</copyright>
    <lastBuildDate>Thu, 05 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gdetor.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://gdetor.github.io/posts/welcome/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/welcome/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>Time series forecasting error metrics</title>
      <link>https://gdetor.github.io/posts/errors/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/errors/</guid>
      <description>In this post, we are going to explore the basic error measures used in time-series forecasting. Error measures provide a way to quantify the quality of a forecasting algorithm (e.g., the performance). First, we briefly introduce time series and the fundamental terms of forecasting. Second, we will introduce the most commonly used error measures and give some examples. Finally, we provide a complete example of using errors in a real-life forecasting scenario.</description>
      <content>&lt;p&gt;In this post, we are going to explore the basic error measures
used in time-series forecasting. Error measures provide a way
to quantify the quality of a forecasting algorithm (&lt;em&gt;e.g.&lt;/em&gt;, the
performance). First, we briefly introduce time series and the
fundamental terms of forecasting. Second, we will introduce the
most commonly used error measures and give some examples. Finally,
we provide a complete example of using errors in a real-life
forecasting scenario.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-time-series&#34;&gt;What is a time series&lt;/h2&gt;
&lt;p&gt;A time series is a series of data points indexed in time order in
layman&amp;rsquo;s terms [1, 2].
A few examples of time series are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The daily closing price of a stock in the stock market&lt;/li&gt;
&lt;li&gt;The number of air passengers per month&lt;/li&gt;
&lt;li&gt;The biosignals recorded from electroencephalogram or electrocardiogram&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 1 shows the number of air passengers per month from January 1949
to September 1960.  In all the examples in this post, we are going to
use this dataset, so if you would like to try the examples by yourself;
you can download the data from Kaggle
&lt;a href=&#34;https://www.kaggle.com/datasets/rakannimer/air-passengers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/passengers.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. An example of a time series showing the number of air passengers per month.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;A more rigorous definition of a time series found in [1] (Chapter 1, pg 1)
is given below:&lt;/p&gt;
&lt;p&gt;Let $ k \in \mathbf{N}, T \subseteq \mathbf{R} $. A function
$$ x: T \rightarrow \mathbf{R}^k, \hspace{2mm} t \rightarrow x_t $$
or equivalently, a set of indexed elements of $ \mathbf{R}^k $,
$$  { x_t | x_t \in \mathbf{R}^k, \hspace{2mm} t \in T }  $$
is called an observed time series (or time series).
Sometimes, we write $ x_t(t \in T) $ or $ (x_t)_{t\in T} $.&lt;/p&gt;
&lt;p&gt;When $ k = 1 $ the time series is called &lt;em&gt;univariate&lt;/em&gt;, otherwise is
called &lt;em&gt;multivariate&lt;/em&gt;.
$ T $ determines if the time series is [1]:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;discrete&lt;/strong&gt; $ T $ is countable, and $ \forall a &amp;lt; b \in \mathbb{R}: T \cap[a, b] $
is finite,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;continuous&lt;/strong&gt; $ T = [a, b], a &amp;lt; b \in \mathbb{R}, T = \mathbb{R}_{+}
\hspace{2mm} \text{or} \hspace{2mm} T = \mathbb{R} $,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;equidistant&lt;/strong&gt; $ T $ is discrete, and $ \exists u \hspace{2mm} s.t. \hspace{2mm} t_{j+1} - t_j = u $.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;From now on and for simplicity&amp;rsquo;s sake we will use the following notation for
a time series: $ y[1], y[2], \ldots , y[N] $, where
$ N \in \mathbb{N} $ or $ y[t] $, where $t=1, \ldots , N $.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;more-terminology&#34;&gt;More terminology&lt;/h2&gt;
&lt;p&gt;Before we dive into the post, let&amp;rsquo;s give some valuable terminology for the
unfamiliar reader.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Observed data&lt;/strong&gt; ($ (y_t)_{t\in T} $ or $ y[t] $) This is the
data we obtain by observing a system or a process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictive model&lt;/strong&gt; Is a mathematical representation of observed data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt; ($ y[t] $) This is the gound truth signal we use to train
a predictor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Horizon&lt;/strong&gt; ($ h $) Is the number of points or steps we predict in
the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt; ($ \hat{y}[t] = y[t+h] $) This is a value that predictor
returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forecasting&lt;/strong&gt; Is the process of prediction future values from historical
and present data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlier&lt;/strong&gt; It is a significantly different value from other values
in a time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error&lt;/strong&gt; ($ \epsilon[t] $) is the difference between the target signal
and the prediction of our model. The error is given by
$ \epsilon[t] = y[t] - \hat{y}[t] $.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seasonality&lt;/strong&gt; ($ S $) Seasonality is the periodic appearance of specific
patterns over the same period—for instance, increasing prices before and
over the Christmas holidays.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;four-basic-predictors&#34;&gt;Four basic predictors&lt;/h2&gt;
&lt;p&gt;So far, we have seen what a time series and the basic terminology is.
Now, we will explore some essential predictors or models and see how
we can use them to perform forecasting.&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;ve already seen, a predictor is a statistical (or mathematical)
model that receives as input historical and present data and returns one
(one-step ahead forecasting, $ h = 1$) or multiple future values
(multi-step ahead prediction, $ h &amp;gt; 1 $).
The development of predictors is out of the scope of this post, so we
will not see how to build, train, test/validate, and use a predictor (here
are a few references where the reader can find more details on that matter
[3, 4, 5, 6]). However, we will introduce the four elementary
predictors since some error measures use some of them to estimate the
prediction errors.&lt;/p&gt;
&lt;h4 id=&#34;naive-predictor&#34;&gt;Naive predictor&lt;/h4&gt;
&lt;p&gt;The most straightforward predictor we can imagine is the &lt;em&gt;naive&lt;/em&gt; one,
and it gets the last observed value and returns it as the predicted
value.&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t + h | t] = y[t]. $$&lt;/p&gt;
&lt;h4 id=&#34;seasonal-predictor&#34;&gt;Seasonal predictor&lt;/h4&gt;
&lt;p&gt;We can use the seasonal predictor when we know that our time series
has a seasonal component (seasonality). It is a natural extension of
the naive one, and we can describe it as:&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = y[t+h-S(P+1)]. $$&lt;/p&gt;
&lt;p&gt;$ P $ is $ \Big[\frac{h-1}{S}\Big] $, where $ \Big[ x \Big] $ is the
integer part of $ x $. $ P $ reflects the number of years&amp;ndash;365 days&amp;ndash;
have passed prior to time $ t + h $.&lt;/p&gt;
&lt;h4 id=&#34;average-predictor&#34;&gt;Average predictor&lt;/h4&gt;
&lt;p&gt;This predictor receives historical and present values as input,
computes their average (or mean), and returns it as a prediction.&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = \bar{y} = \frac{1}{N} \sum_{t=1}^{N} y[t] .$$&lt;/p&gt;
&lt;h4 id=&#34;drift-predictor&#34;&gt;Drift predictor&lt;/h4&gt;
&lt;p&gt;Another variation of the naive predictor, only this time
we allow to the predicted value to &lt;em&gt;drift&lt;/em&gt; (fluctuate) over time,&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = y[t] + \frac{h}{t-1} \sum_{j=2}^{t} (y[j]-y[j-1]) = y[t] + h\Big( \frac{y[t] - y[1]}{t - 1} \Big). $$&lt;/p&gt;
&lt;p&gt;You can picture this predictor as a line drawn from the first
observation to the last one and beyond, where beyond is the prediction.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the predictions in each of the cases mentioned above for
the air passengers data (brown line).
The blue line represents the naive predictor, the green line the
seasonal, the orange line shows the average predictor, and finally,
the pink line indicates the drift predictor.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/naive_predictors.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. Forecasts of montly air passengers. Naive predictor (blue line), naive seasonal (green), average (orange), drift (pink).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;h2 id=&#34;forecasting-error-measures&#34;&gt;Forecasting error measures&lt;/h2&gt;
&lt;p&gt;So why do we need error measures? The general idea is to quantify
the distance between an actual observation (target) and a predicted
one. Particularly when we train a model to learn how to predict
future values, we have to measure the error between the actual
observations and the predicted ones, so the minimization of the
error leads to a better model.&lt;/p&gt;
&lt;p&gt;When we teach a model, we need to use some penalties to help the
model improve the predictions. The error measures listed below do
precisely that. They measure how far the model&amp;rsquo;s predictions are
from the ground truth and penalize the model accordingly. Usually,
the smaller the error, the better the predictor.&lt;/p&gt;
&lt;p&gt;Another reason we need error measures is to evaluate the performance
of our model in real-life scenarios. We might have a trained model
that performs some forecasting, and we would like to investigate the
quality of its predictions. In this case, we can measure the error
between the historical data we will collect in the future and the
model&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;We already said that developing and training a predictor is out of
the scope of the present post. Therefore, we will use historical
data and add some Gaussian noise to them to fake the predictions.
Furthermore, we adopt the discrete-time signals time indexing, meaning
that $ y[t] $ is the value of the time series at time index $ t $.
A similar way would be $ y_t $, where $ t $ is the time index.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt; $ y[t] $ is the target signal, $ \hat{y}[t] $ the prediction
signal and $ \epsilon[t] $ the error signal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And now, we are ready to introduce the error measures and some examples
demonstrating their behavior.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;In the following sections, we use some basic examples to demonstrate how the
reader can implement the error measures in Python.
We provide a custom implementation of the error measure and a Sklearn one in
every case.
We provide a custom implementation of the error measure and a Sklearn one in
every case. The reader should rely more on the Sklearn [7] implementation since
it&amp;rsquo;s generic and optimized. We provide the custom implementation so the reader
can understand better the mathematical formulas.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seed(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_true &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])        &lt;span style=&#34;color:#75715e&#34;&gt;# This is y (target)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2&lt;/span&gt;])      &lt;span style=&#34;color:#75715e&#34;&gt;# This is y_hat (prediction)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mean-absolute-error-mae&#34;&gt;Mean Absolute Error (MAE)&lt;/h3&gt;
&lt;p&gt;The MAE is the most straightforward error measure, and as it&amp;rsquo;s
name signifies it is just the difference between a target (or desired)
value and model&amp;rsquo;s prediction. MAE is defined as:&lt;/p&gt;
&lt;p&gt;$$ \frac{1}{N} \sum_{t=1}^{N} |y[t] - \hat{y}[t]| = \frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] |. $$&lt;/p&gt;
&lt;p&gt;By observing the definition of MAE, we can see that MAE is
scale-dependent, meaning that both signals, target, and prediction,
must be of the same scale. Another drawback of MAE that we can identify
by looking at its definition is its sensitivity to outliers (&lt;em&gt;e.g.&lt;/em&gt;,
values in the time series that stick further away from any other value).
Outliers can drag the MAE to higher values, thus affecting the error.
However, there are ways to handle outliers and fix that issue (see here [8, 9]).&lt;/p&gt;
&lt;h4 id=&#34;example-1&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_absolute_error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MAE&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; error &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; N
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MAE(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mean_absolute_error(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mean-absolute-percentage-error-mape&#34;&gt;Mean Absolute Percentage Error (MAPE)&lt;/h3&gt;
&lt;p&gt;MAPE computes the error between a target and a prediction signal
as a ratio of the error $ \epsilon[t] $ and the target signal. More
precisely,&lt;/p&gt;
&lt;p&gt;$$ MAPE = \frac{100\%}{N} \sum_{t=1}^{N} \frac{|y[t] - \hat{y}[t]|}{|y[t]|} = \frac{100}{N} \sum_{t=1}^{N} \frac{| \epsilon[t] |}{| y[t] |}. $$&lt;/p&gt;
&lt;p&gt;MAPE is a helpful error measure when it serves as a loss function
in training and validating a regression model [10]. This error measure is not
susceptible to global scaling of the target signal.&lt;/p&gt;
&lt;p&gt;Again by observing the definition of MAPE above, we can draw some
conclusions about this measure. MAPE can be problematic when the actual
values are zero or close to zero. We can see from the definition above
that when the denominator is close to zero or zero, the MAPE is too
large or cannot be defined. Moreover, MAPE is susceptible to skewness,
since the term $ \frac{1}{y[t]} $ depends only on the observed data
(not on the model&amp;rsquo;s predictions).&lt;/p&gt;
&lt;h4 id=&#34;example-2&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_absolute_percentage_error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MAPE&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;100.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; N) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MAPE(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;19.5555555555555557&lt;/span&gt;                 &lt;span style=&#34;color:#75715e&#34;&gt;# this is because we multiply by 100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mean_absolute_percentage_error(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.19555555555555554&lt;/span&gt;                 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mean-squared-error-mse&#34;&gt;Mean Squared Error (MSE)&lt;/h3&gt;
&lt;p&gt;The MSE is one of the most used error measures in Machine and
Deep learning. It computes the average of the square of the errors
between target and prediction signals. We define the MSE as:&lt;/p&gt;
&lt;p&gt;$$ MSE = \frac{1}{N} \sum_{t=1}^{N} (y[t] - \hat{y}[t])^2 = \frac{1}{N} \sum_{t=1}^{N} \epsilon[t]^2 . $$&lt;/p&gt;
&lt;p&gt;If we take the square root of $ MSE $, we get the Root MSE or RMSE.
When the MSE is zero, we call the predictor (model) a perfect predictor.
MSE falls into the category of quadratic errors. Quadratic errors
tend to exaggerate the difference between the target and the model&amp;rsquo;s
prediction, rendering them suitable for training models since the
penalty applied to the model will be more prominent when the error
signal is significant [11].&lt;/p&gt;
&lt;p&gt;MSE combines the &lt;em&gt;bias&lt;/em&gt; and the &lt;em&gt;variance&lt;/em&gt; of a prediction. More
precisely, $ MSE = b^2 + Var $, where $b$ is the bias term and
$ Var $ is the variance. The bias reflects the assumptions the
model makes to simplify the process of finding answers. The more
assumptions a model makes, the larger the bias. On the other hand,
variance refers to how the answers given by the model are subject
to change when we present different training/testing data to the
model. Usually, linear models such as &lt;em&gt;Linear Regression&lt;/em&gt; and
&lt;em&gt;Logistic Regression&lt;/em&gt; have high bias and low variance. Nonlinear
models such as &lt;em&gt;Decision Trees&lt;/em&gt;,  &lt;em&gt;SVM&lt;/em&gt;, and &lt;em&gt;kNN&lt;/em&gt; have low
bias and high variance [12]. Ideally, we would like to find a balance
between bias and variance. That&amp;rsquo;s why sometimes we have to penalize
our model during training using regularization techniques (this is
out of the scope of the present post).&lt;/p&gt;
&lt;h4 id=&#34;example-3&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_squared_error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MSE&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; error &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; N
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MSE(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.08333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mean_squared_error(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.08333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;symmetric-mean-absolute-percentage-error-smape&#34;&gt;Symmetric Mean Absolute Percentage Error (SMAPE)&lt;/h3&gt;
&lt;p&gt;SMAPE computes the error between the target and the prediction signals
as a ratio of the error with the sum of the absolute values of actual
and prediction values.
The mathematical definition for SMAPE is:&lt;/p&gt;
&lt;p&gt;$$ SMAPE = \frac{100\%}{N} \sum_{t=1}^{N} \frac{| \epsilon [t] |}{(| y[t]| + | \hat{y}[t] |)} $$&lt;/p&gt;
&lt;p&gt;And as we can see from that definition, SMAPE is bounded from
above and below, $ 0 \leq SMAPE \leq 100 $. Another remark we
can make based on  SMAPE&amp;rsquo;s definition: when both a target and
a prediction value are zero, the SMAPE is not defined. If only
the actual or target value is zero, $ SMAPE = 100 $. Finally,
SMAPE can cause troubles when  let&amp;rsquo;s say a prediction is
$ \hat{y}[t] = 10 $ the first time and $ \hat{y}[t] = 12 $ the
second time, while in both cases the target (actual) value is
$ y[t] = 11 $.  In the former case, $ SMAPE  = 4.7 % $ and in
the latter case $ SMAPE = 4.3 % $. We see that we get two different
error values for the same target when our predictor returns
different predictions.&lt;/p&gt;
&lt;h3 id=&#34;mean-absolute-scaled-error-mase&#34;&gt;Mean Absolute Scaled Error (MASE)&lt;/h3&gt;
&lt;p&gt;MASE is a metric that computes the error ratio between the
target and the model&amp;rsquo;s prediction to a naive predictor&amp;rsquo;s error (forecaster).&lt;/p&gt;
&lt;p&gt;The following equation gives the MASE,&lt;/p&gt;
&lt;p&gt;$$ MASE = \frac{\frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] | }{\frac{1}{N-1} \sum_{t=2}^{N} | y[t] - y[t-1] | } $$&lt;/p&gt;
&lt;p&gt;When we are dealing with time series with seasonality with period
$ S $ we can use the following MASE formula instead:&lt;/p&gt;
&lt;p&gt;$$ MASE = \frac{\frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] | }{\frac{1}{N-S} \sum_{t=S+1}^{N} | y[t] - y[t-S] | }. $$&lt;/p&gt;
&lt;p&gt;MASE is scale-invariant, meaning that it&amp;rsquo;s immune to any scaling
we perform on the observed data. MASE is symmetric, which implies
that it penalizes equally the positive and the negative (as well
as big and small) forecast errors. When MASE error is greater than
one, the naive forecaster performs better than our model. MASE can
be problematic only when the actual (target) signal is only zero values.
In that case the naive predictor will be zero ad infinitum and thus
the MASE will be undefined.&lt;/p&gt;
&lt;h3 id=&#34;coefficient-of-determination-cod-or-r&#34;&gt;Coefficient of Determination (CoD) or R²&lt;/h3&gt;
&lt;p&gt;The $ R^2 $ or Coefficient of Determination is an error measure
frequently used in evaluating regression models (&lt;em&gt;goodness of fit&lt;/em&gt;
or &lt;em&gt;best-fit line&lt;/em&gt;). $ R^2 $ counts how many of the target data
points approach the line formed by the regression [11].&lt;/p&gt;
&lt;p&gt;We define $ R^2 $ as&lt;/p&gt;
&lt;p&gt;$$ R^2 = 1 - \frac{\sum_{t=1}^{N}(y[t] - \hat{y}[t])^2 }{\sum_{t=1}^{N}(y[t] - \bar{y})^2} = 1 - \frac{MSE}{Var[y[t]]}, $$&lt;/p&gt;
&lt;p&gt;or alternatively&lt;/p&gt;
&lt;p&gt;$$ R^2 = \frac{SSR}{SST} = \frac{\sum_{t=1}^{N}(y[t] - \hat{y}[t])^2 }{\sum_{t=1}^{N}(y[t] - \bar{y})^2}. $$&lt;/p&gt;
&lt;p&gt;SSR is the sum of squares regression, and SST is the sum of squares
total. SSR represents the total variation of all the predicted values
found on the regression plane from the mean value of all the values
of response variables. SST reflects the total variation of actual
values (targets) from the mean value of all the values of response
variables.&lt;/p&gt;
&lt;p&gt;$R^2$ is bounded from above, $R^2 \leq 1$, since the fraction term
lives always in the interval $ [0, 1] $. In the case of training a
regression model $ R^2 $ is bounded from bellow $ 0 \leq R^2 \leq 1 $.
For the test/validation data, $ R^2 $ can be negative when MSE is
large or the total variance of the target (actual) signal is too
small. A negative $ R^2 $ implies that the term $ \bar{y} $ is a
better predictor than our model. Moreover, from the first definition
of $ R^2 $, we see a direct relation between $ R^2 $ and MSE. While
the $ R^2 $ increases, the MSE tends to approach zero. When we have
an ideal predictor, $ MSE = 0 $ and $ r^2 = 1 $.&lt;/p&gt;
&lt;h4 id=&#34;example-4&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; r2_score
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; var
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;R2&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MSE(y_true, y_pred)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    variance &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; var(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (mse &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; var)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(R2(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9351351351351351&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(r2_score(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9351351351351351&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we briefly introduced the concept of time series and the most
frequently used error measures in forecasting. we described the pros and cons
of each measure so the reader can decide which one best suits their needs.
If you find any typos or errors, or you have any other comments, please feel
free to report them (you can find contact information &lt;a href=&#34;https://gdetor.github.io/about&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022errors-timeseries,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Time series and forecasting error measures&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/errors&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;J. Beran, &lt;em&gt;Mathematical Foundations of Time Series Analysis A Concise
Introduction&lt;/em&gt;, Springer, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Time_series&#34;&gt;&amp;ldquo;Time series&amp;rdquo;&lt;/a&gt;, Wikipedia,
Wikimedia Foundation, May 2 2022.&lt;/li&gt;
&lt;li&gt;A. Nielsen, &lt;em&gt;Practical time series analysis: Prediction with statistics
and machine learning&lt;/em&gt;, O&amp;rsquo;Reilly Media, 2019.&lt;/li&gt;
&lt;li&gt;R. J. Hyndman, and G. Athanasopoulos, &lt;em&gt;Forecasting: principles and practice&lt;/em&gt;,
OTexts, 2018.&lt;/li&gt;
&lt;li&gt;D. Oliveira, &lt;em&gt;Deep learning for time series forecasting&lt;/em&gt;,
&lt;a href=&#34;https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting&#34;&gt;https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting&lt;/a&gt;,
Kaggle, 2019.&lt;/li&gt;
&lt;li&gt;R. Mulla, &lt;em&gt;[Tutorial] TIme series forecasting with XGBoost&lt;/em&gt;,
&lt;a href=&#34;https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost&#34;&gt;https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost&lt;/a&gt;,
Kaggle, 2019.&lt;/li&gt;
&lt;li&gt;Pedregosa, F. et al., &lt;em&gt;Scikit-learn: Machine Learning in Python&lt;/em&gt;,
Journal of Machine Learning Research, 12, 2825&amp;ndash;2830, 2011.&lt;/li&gt;
&lt;li&gt;F. Grubbs, &lt;em&gt;Sample Criteria for Testing Outlying Observations&lt;/em&gt;,
Annals of Mathematical Statistics 21(1):27–58, DOI:10.1214/aoms/1177729885, 1950.&lt;/li&gt;
&lt;li&gt;B. Rosner, &lt;em&gt;Percentage Points for a Generalized ESD Many-Outlier Procedure&lt;/em&gt;,
Technometrics 25(2):165–172, 1983.&lt;/li&gt;
&lt;li&gt;A. de Myttenaere, B. Golden, B. Le Grand, and F. Rossi, &lt;em&gt;Mean absolute percentage
error for regression models&lt;/em&gt;, Neurocomputing, 2016.&lt;/li&gt;
&lt;li&gt;A. Kumar, &lt;em&gt;Mean squared error or R-squared - Which one to use?&lt;/em&gt;
&lt;a href=&#34;https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/&#34;&gt;https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/&lt;/a&gt;, 2022.&lt;/li&gt;
&lt;li&gt;C. M. Bishop, and N. M. Nasrabadi, &lt;em&gt;Pattern recognition and machine learning&lt;/em&gt;,
New York: Springer, 2006.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Genetic Algorithms &amp; Island Models</title>
      <link>https://gdetor.github.io/posts/genetic_algorithms/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/genetic_algorithms/</guid>
      <description>In this post, we explore genetic algorithms (GAs) and the so-called island model (IM). GAs and the IM are optimization methods used to maximize or minimize a cost function.
What is Optimization? Let&amp;rsquo;s see an example of an optimization problem we all face every day. Let&amp;rsquo;s assume you&amp;rsquo;d like to go and grab a couple of coffee from your favorite coffee shop. Typically, you ask Google to find the fastest way to the store from your current location.</description>
      <content>&lt;p&gt;In this post, we explore genetic algorithms (GAs) and the so-called
island model (IM). GAs and the IM are optimization methods used to
maximize or minimize a cost function.&lt;/p&gt;
&lt;h2 id=&#34;what-is-optimization&#34;&gt;What is Optimization?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see an example of an optimization problem we all face every day. Let&amp;rsquo;s
assume you&amp;rsquo;d like to go and grab a couple of coffee from your favorite coffee
shop. Typically, you ask Google to find the fastest way to the store from your
current location. But let&amp;rsquo;s forget about technology for now.&lt;/p&gt;
&lt;p&gt;You only have a map. Yes, a paper map! They are still around! You first try to
find the shortest paths from your current location to the coffee shop. If
you&amp;rsquo;re a scrooge, you will define the &amp;ldquo;optimal path&amp;rdquo; as &amp;ldquo;the path I consume the
least fuel.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;An optimization problem is a search for the &amp;ldquo;best&amp;rdquo; set of parameters, where
&amp;ldquo;best&amp;rdquo; is defined as the minimum or maximum of some cost function of interest.
Here, that function is our fuel consumption. However, we could have also
designated it to be the distance from start to finish.&lt;/p&gt;
&lt;p&gt;Mathematically speaking the problem of a minimization can be formulated as
follows [1]:
Given a function $f:A\subseteq \mathbb{R} \rightarrow \mathbb{R}$ we are
searching for an element $ {\bf x}^* $ such that
$$ f({\bf x}^*) \leq f({\bf x}) $$&lt;/p&gt;
&lt;p&gt;for all $ {\bf x} \in A $. Similarly, a maximization would be the search for
a ${\bf x}^* $ such that
$$ f({\bf x}^*) \geq f({\bf x}) $$
for all $ {\bf x} \in A $.&lt;/p&gt;
&lt;p&gt;In both cases, we are searching for a global optimum (either a global minimum
or a global maximum). However, it is not always possible to find a global
optimum point in real-life cases. Instead, we can settle for a local minimum or
maximum. For example, that&amp;rsquo;s the compromise we often make when training neural
networks with backpropagation [2].&lt;/p&gt;
&lt;p&gt;Figure 1A shows the global minimum of the cost function, $f(x)=x^2$, with a
magenta color. Figure 1B displays what is known as the Rastrigin function in
one dimension. It is evident that this function has multiple local minima
(&lt;em&gt;e.g.,&lt;/em&gt;, magenta disc)  and maxima (green disc) as well as one global minimum
at $(0, 0)$ (black disc).&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/fun_extremes.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. Global and local extremes for (A) $f(x)=x^2$, where the magenta disc indicates the global minimum at $(0, 0)$. (B) For Rastrigin $f(x) = 10 + x^2-10\cos(2\pi x)$, there is a global mimimum at $(0, 0)$ and many local mimima and maxima (for instance see the magenta and green discs, respectively).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Sometimes, it&amp;rsquo;s easier to solve a minimization problem from a computational
standpoint. In that case, we would minimize the function $ -f $.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-genetic-algorithm&#34;&gt;What is a Genetic Algorithm?&lt;/h2&gt;
&lt;p&gt;A genetic algorithm is an optimization method that mimics evolution to optimize
a cost function. The entire set of the cost function parameters is called the
genome. Each parameter consists of a gene. Because GA mimics how evolution
works, they require a population of individuals. Each individual is nothing
more than a randomly initialized genome. Any GA starts optimizing a cost
function after initializing a population of genomes (individuals).&lt;/p&gt;
&lt;p&gt;In most GA implementations, an individual is a a data structure that holds a
genome (vector of bits, integers, floats, etc.), the corresponding cost to its
genome, a unique ID, a flag indicating whether the current individual is about
to mate (after a selection process), and other relevant information that the
developer deems necessary.&lt;/p&gt;
&lt;p&gt;When the GA optimizes a cost function, it usually applies three basic operators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt; This operator selects two individuals from a population
(&lt;em&gt;i.e.,&lt;/em&gt; a set of many individuals) to mate and eventually procreate. The
selected individuals are called parents. Some selection operators
are k-tournament, roulette-wheel, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Crossover&lt;/strong&gt; This operator mises the genomes of the selected parents. A
crossover operator will combine a part of the first parent&amp;rsquo;s genome with a
part of the second parent&amp;rsquo;s genome. Some crossover operators are one-point
crossover, two-points crossover, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mutation&lt;/strong&gt; Finally, the potential offspring&amp;rsquo;s (or child&amp;rsquo;s) genome is
subject to a mutation, which will further change the offspring&amp;rsquo;s genome.
Some of the most used mutation operators are delta, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically, a GA will repeat every generation&amp;rsquo;s operations mentioned above
(another fancy term for iteration). At every iteration, potential offspring
will replace their parents. Usually, the best-performing offspring will replace
the most poor-performing parents in terms of fitness. We call that kind of
replacement &amp;ldquo;elite&amp;rdquo; replacement. Another idea of replacing the parents is
randomly choosing some of the parents and replacing them.&lt;/p&gt;
&lt;p&gt;When the GA exhausts the predefined number of generations, the algorithm
terminates. We can evaluate its performance by inspecting the average fitness
(the average cost function value over all the individuals) or the best-so-far
fitness (BSF). Figure 2A shows a typical example of average fitness and 2B a
BSF. Upon the algorithm&amp;rsquo;s termination, the individual with the best fitness
function provides the genome (set of parameters) optimizes the cost function
[3].&lt;/p&gt;
&lt;h2 id=&#34;how-a-genetic-algorithm-works&#34;&gt;How a Genetic Algorithm works?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use an example to try to understand how GAs work.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s try to solve the XOR problem using a feed-forward neural network.  The
XOR (exclusive OR) is a binary operator that returns true if and only if the
operands are different. This means that if $ X = 1 $ (or $ X = 0 $) and $ Y = 0
$ (or $ Y = 1 $) then $ X \oplus Y = 1 $ (same when ). If both $ X $ and $ Y $
are zeros or ones, then $ X \oplus Y = 0 $.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/xor_net.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. XOR neural networ.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Our neural network will consist of two input units (since the XOR operator is a
binary one), two hidden units, and one output unit. Please see [CITE HERE] for
more details on why we choose such an architecture. Figure 2 shows the neural
network we are about to use.  Let&amp;rsquo;s optimize this neural network that will
serve as an XOR operator. To facilitate the demonstration, let&amp;rsquo;s split the
process into four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Define the cost function and the size of our genome.
Since the input to the XOR function is two-dimensional, we have a
genome of size two. Regarding the cost function, we first define an
error term as
$$ \epsilon = | y_{\text{target} - y_{\text{pred}}} | $$
where $ y_{\text{target} $ is the true value that the XOR function returns
and $x_{\text{pred}$ is the value of our network&amp;rsquo;s output. Since we have four
pairs of inputs $ (0, 1), (1, 0), (1, 1), (0, 0) $, and four real outputs
$1, 1, 0, 0$, respectively, we define the cost function as:
$$ f({\bf x}) = \sum_{i=1}^{4} \epsilon_i $$
Ideally, we expect the cost function to be zero to get our
optimal solution. That&amp;rsquo;s the case in Figure 2, where we see the best-so-far
fitness and the average fitness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once we have defined the cost function (that&amp;rsquo;s always the most challenging
part), we determine the number of genes per individual, which is $9$ in
this case. The neural network has the two input units connected via a $ 2
\times 2 $ matrix with the two hidden units. Next, the hidden units
connect via a $ 2\times 1 $ matrix to the output unit. Moreover, the
hidden and output units have a bias term (3 bias terms in total).
Therefore, the entire network has nine parameters (six weights and three
bias terms) we have to optimize. So, we initialize the neural network with
small random weights, define the number of individuals (population size)
to be $ 20 $, and define the size of the genome to be $ 9 $. Remember,
this is the number of cost function parameters to optimize. We set the
number of generations to $ 5000 $,  the number of offspring to $ 10 $, and
the replacement size to $ 5 $, which means $ 5 $ parents will be replaced
by $ 5 $ offsprings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We need to decide what operators we will use for our GA.
In this particular example, we use a k-tournament selection,
a one-point crossover, and a delta mutation operator.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;k-tournament selection&lt;/strong&gt; This operator will randomly choose
$ k $ individuals from the population. It will, then, choose the best
individual, based on the fitness from the tournament with probability
$ p $, choose the second-best individual with probability $ p(1-p) $,
the third-best with probability $ p(1-p)^2 $, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-point crossover&lt;/strong&gt; will take the genes of two parents as input,
it will randomly pick a number from zero to the size of genes, and it
will cut the parents&amp;rsquo; genes at that point. Then it will swap the
sliced genes between the two parents, and thus the offspring will carry
genetic information from both parents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delta mutation&lt;/strong&gt; operator will draw uniformly a number from the
interval $ [0, 1] $ for each gene in an individual&amp;rsquo;s genome. If that
number is greater than a probability $ p $ (usually $ p = \frac{1}{2} $).
A predefined increment will increase the value of the gene.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we set our Optimization in motion. The GA will first evaluate
the cost function of each individual. It will sort the individuals based
on their fitness values. To this end, we feed the XOR input to the neural
network, and we collect the output $ y_{\text{pred}} $. Then we use the
output to compute the cost function value for that individual. The next step
for our GA is to choose two parents based on the k-tournament selection. The
genome of the two selected parents will be crossed over using the
one-point operator. That will give rise to a new genome, a child or
offspring. The offspring&amp;rsquo;s genome will undergo a mutation based on the
delta operator. Finally, the GA will add the offspring to a list.  The
selection, crossover, and mutation processes continue until the number
of offspring have been exhausted. Then, the best performing offspring
will replace the poorest-performing (maximum cost function value)
parents (elite replacement). And the entire process repeats for $ 4999$
more generations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the convergence of the process described in step $ 4 $, the Optimization
of the XOR fitness terminates, and we can inspect the results. Figure 3 shows
the best-so-far fitness (BSF) and the average fitness, respectively. The first
observation is that the cost function value indeed converges to zero. Thus, our
GA&amp;rsquo;s best genome is optimal, and our neural network solves the XOR problem. The
second observation is that we could have used only $500$ generations for this
particular instance. The difficulty of the problem at hand usually determines
the number of generations and the initial values of the genome.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/bsf_avg_fit.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 3. BSF and average fitness for the optimization of the XOR fitness function.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Bellow, you can find the source code of the Python script we used to optimize
the XOR problem. As you can see, we combine Pytorch and PyGAIM to build
a neural network and optimize the weights and the biases.&lt;/p&gt;
&lt;p&gt;The first snippet shows what packages we need to import, the class of the
neural network, and a function that we will use to measure the accuracy
of our optimization process.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pylab &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; random &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; shuffle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; nn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home/gdetorak/packages/gaim/pygaim&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pygaim &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GAOptimize, c2numpy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;XOR_NET&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        super(XOR_NET, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sigmoid()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1(x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2(out)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate the XOR_NET class&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; XOR_NET()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# src: Input XOR&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# tgt: Output XOR&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;src &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;f&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;f&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;accuracy&lt;/span&gt;(genome):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Measures the accuracy of the XOR_NET. Runs over 100 times
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    and compares the network output against the target pattern
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    each time.      
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Assign genomes to network weights&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w1)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w2)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(src[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tgt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(dst[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net(inp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(tgt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            count &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# print(y.item(), tgt.item())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; / 100&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; count)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The fitness function takes the genome (C array) as input and
returns the negative of fitness value (or loss) since we perform
a minimization. It presents each time all four XOR patterns to
the neural network and computes the loss for each pattern.
Finally, it sums up all four individual losses and returns the
total loss.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fitness&lt;/span&gt;(x, length):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Fitness function. Receives the genome (x) from GAIM, passes it through
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    net.forward (Pytorch) and computes the absolute loss. 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c2numpy(x, length)				&lt;span style=&#34;color:#75715e&#34;&gt;# Convert C array to Numpy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]						&lt;span style=&#34;color:#75715e&#34;&gt;# First layer weights (2x2)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]					&lt;span style=&#34;color:#75715e&#34;&gt;# First layer bias (2X1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]					&lt;span style=&#34;color:#75715e&#34;&gt;# Second layer weights (2x1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;:]						&lt;span style=&#34;color:#75715e&#34;&gt;# Second layer bias (1x1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Assign the new values to network weights	&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w1)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w2)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;						
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shuffle(index)					&lt;span style=&#34;color:#75715e&#34;&gt;# shuffle the index&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# loop over all four patterns in a random order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; index:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_numpy(src[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tgt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_numpy(dst[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net(inp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; tgt[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; float(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item())			&lt;span style=&#34;color:#75715e&#34;&gt;# return -loss (minimize)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The final snippet provides the code to call the GAOptimize function
of PyGAIM, to plot the results and measure the performance (accuracy)
of the neural network on solving XOR.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    genome_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ga &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GAOptimize(fitness,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_generations&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    population_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    genome_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;genome_size,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_offsprings&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_replacements&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[float(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(genome_size)],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[float(&lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(genome_size)],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    mutation_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    mutation_var&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;.1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    genome, _, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ga&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ga&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    test_weights(genome)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;GAIM [4] is a C++ library for genetic algorithms and the island model.
It implements the most fundamental selection, crossover, and mutation
operators. It also provides an MPI and POSIX threads implementation
of the island model. Finally, it comes with a Python interface
called PyGAIM that simplifies GA-based optimization problem setup,
and PyGAIM provides a scikit-learn-like interface.&lt;/p&gt;
&lt;p&gt;For more information about GAIM, its source code, and examples,
you can visit its &lt;a href=&#34;https://github.com/gdetor/gaim&#34;&gt;Github&lt;/a&gt; repository.&lt;/p&gt;
&lt;h2 id=&#34;what-is-an-island-model&#34;&gt;What is an Island Model?&lt;/h2&gt;
&lt;p&gt;An island model is a computational method that runs multiple instances of GAs
on the same optimization problem in a distributed and parallel fashion. In some
cases, each island (another fancy word for process, thread, or computational
node in a cluster) can run a part of the optimization problem [5].&lt;/p&gt;
&lt;p&gt;What is essential in an island model is the periodic exchange of individuals
between islands. According to a predetermined time interval, a migration of a
subpopulation takes place. This circulation of individuals between islands
relies on specific communication protocols and predetermined topologies
(&lt;em&gt;e.g.,&lt;/em&gt;  ring topology). In this case, the islands are connected, forming a
ring, meaning the current island (node) connects to the one on its right (or
left).&lt;/p&gt;
&lt;p&gt;Figure 4 shows three basic topologies, (A) all-to-all, where every island
connects to all other ones, (B) ring, and (C) star topology, where one island
serves as a communication hub [5, 6, 7].&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/island_model.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 4. Island model topologies. (A) all-to-all, (B) ring, (C) star.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Each island begins with a population of $N$ individuals where $$ N =
\frac{K}{M} $$ where $ K $ is the number of all individuals, and $M$ is the
number of islands.
Every island will initialize a GA based on all the parameters and procedures
described earlier. When a time counter exhausts, migration takes place. The IM
algorithm selects a subpopulation on each island via some selection method
(random, elite - the best performing individuals, etc.). The selected genomes
move to the neighboring, connected island or islands. The newly arrived ones
replace local individuals via a replacement method (random, poor - the
worst-performing individuals, etc.). IM fills the vacant spots on the source
island with new offspring or randomly generated individuals [6]. The number of
individuals moved at every migration interval is called the &amp;ldquo;migration size.&amp;rdquo;
The reader can refer to [8, 9] for more information on how the migration
interval and the migration size affect the performance of an island model.&lt;/p&gt;
&lt;p&gt;The island model offers a means of faster convergence since each island can
potentially follow a different evolutionary trajectory covering different parts
of the search space. Furthermore, exchanging individuals between islands can
help the overall optimization process avoid being stuck in some local
minimum/maximum. That doesn&amp;rsquo;t mean that the island model is impervious to local
extrema.&lt;/p&gt;
&lt;h2 id=&#34;when-should-we-use-gas&#34;&gt;When should we use GAs?&lt;/h2&gt;
&lt;p&gt;As we have seen, GAs can optimize virtually any function given
a well-defined cost function (and that&amp;rsquo;s the most challenging part of using
GAs). However, there are some cases where we should try to use a GA
instead of any other conventional optimization method. These instances are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the search space is massive, then GAs are suitable for optimizing a
function in that space (for instance, non-linear functions).&lt;/li&gt;
&lt;li&gt;Another issue with Optimization is the cost function. The cost function may
be discontinuous (having gaps), or it may be non-differentiable. GAs do not
use derivatives and are therefore immune to such issues.&lt;/li&gt;
&lt;li&gt;When a cost function is too complex, GAs have more chances than the vanilla
optimization methods to avoid local minima and correctly find the global
optimum. A genetic algorithm can simultaneously explore the search space in
multiple directions, even if some offspring will never discover an optimal
solution to the problem.&lt;/li&gt;
&lt;li&gt;Finally, GAs are agnostic to the problem at hand. They do not require any
information about the system or the function they optimize to solve the
optimization problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-are-some-of-the-gas-applications&#34;&gt;What are some of the GAs applications?&lt;/h2&gt;
&lt;p&gt;Here, you can find some of the GA&amp;rsquo;s applications in real life. Although the
the following list is not complete; you will glimpse where and how GAs are used.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; As discussed in this post, GAs can optimize almost any function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; GAs can tune ML/DL models to discover optimal neural
network parameters. Moreover, they can design neural networks (searching
for the optimal neural network topology [10]).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Path and trajectory planning&lt;/strong&gt; GAs can aid in the designing and planning
of paths and trajectories for autonomous robotic platforms, vehicles, or
manipulators, such as robotic arms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNA Analysis&lt;/strong&gt; GAs can analyze the structure of DNA samples.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finance&lt;/strong&gt; GAs are an excellent tool for analyzing and forecasting stock prices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aerospace engineering&lt;/strong&gt; GAs can aid in the process of designing
aircraft.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Traveling salesman problem (TSP)&lt;/strong&gt; The
&lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;TSP&lt;/a&gt; is
well-defined in combinatorial Optimization and has many applications in
real-life issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022geneticalg,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Genetic algorithms and island models&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/genetics_algorithms&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;D. A. Pierre, &lt;em&gt;Optimization theory with applications&lt;/em&gt;,
Courier Corporation, 1986.&lt;/li&gt;
&lt;li&gt;I. Goodfellow, B. Yoshua, and A. Courville, &lt;em&gt;Deep learning&lt;/em&gt;,
MIT press, 2016.&lt;/li&gt;
&lt;li&gt;K. De Jong, &lt;em&gt;Evolutionary computation&lt;/em&gt;, Wiley Interdisciplinary Reviews:
Computational Statistics, 2009, 1.1: 52-56.&lt;/li&gt;
&lt;li&gt;G. Is. Detorakis and A. Burton, &lt;em&gt;GAIM: A C++ library for Genetic Algorithms
and Island Models&lt;/em&gt;, Journal of Open Source Software, 2019, 4.44: 1839.&lt;/li&gt;
&lt;li&gt;D. Whitley, S. Rana, and R.B. Heckendorn, &lt;em&gt;The island model genetic
algorithm: On separability, population size and convergence&lt;/em&gt;,
Journal of computing and information technology, 7:1, 33&amp;ndash;47, 1999.&lt;/li&gt;
&lt;li&gt;D. Sudholt, &lt;em&gt;Parallel evolutionary algorithms&lt;/em&gt;, Springer Handbook of
Computational Intelligence, 929&amp;ndash;959, 2015.&lt;/li&gt;
&lt;li&gt;W. N. Martin, J. Lienig, and J. P. Cohoon, &lt;em&gt;Parallel Genetic Algorithms Based
on Punctuated Equilibria&lt;/em&gt;, Handbook of Evolutionary Computation, IOP Publishing group,&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Z. Skolicki, &lt;em&gt;An analysis of island models in evolutionary computation&lt;/em&gt;,
Proceedings of the 7th annual workshop on Genetic and evolutionary computation,
386&amp;ndash;389, 2005.&lt;/li&gt;
&lt;li&gt;Z. Skolicki, and K. De Jong, &lt;em&gt;The influence of migration sizes and intervals
on island models&lt;/em&gt;, Proceedings of the 7th annual conference on Genetic and
evolutionary computation, 1295&amp;ndash;1302, 2005.&lt;/li&gt;
&lt;li&gt;K. O. Stanley, and R. Miikkulainen, &lt;em&gt;Efficient evolution of neural network
topologies&lt;/em&gt;, Proceedings of the 2002 Congress on Evolutionary Computation.
2, 1757&amp;ndash;1762, 2002.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Tips &amp; Tricks (Linux/Vim/Git/Programming)</title>
      <link>https://gdetor.github.io/posts/tipsntricks/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/tipsntricks/</guid>
      <description>In this post you can find various simple tricks and tips for Linux, Vim, Git, Python and many other stuff. All the material provided in this page are free and can be redistributed and/or modified. There is no any warranty that they work for you or are suitable to your need. The author of this page is not responsible for any damage this material may cause.
Joining dictinary keys and values to a list in Python (Programming) &amp;#39;_&amp;#39;.</description>
      <content>&lt;p&gt;In this post you can find various simple tricks and tips for Linux, Vim,
Git, Python and many other stuff. All the material provided in this page are
free and can be redistributed and/or modified. There is no any warranty that
they work for you or are suitable to your need. The author of this page is
not responsible for any damage this material may cause.&lt;/p&gt;
&lt;h3 id=&#34;joining-dictinary-keys-and-values-to-a-list-in-python-programming&#34;&gt;Joining dictinary keys and values to a list in Python (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{!s}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{!r}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(key, val) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (key, val) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;check-the-health-status-of-your-hard-drive-linux&#34;&gt;Check the health status of your hard drive (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ badblocks -w -s -o error.log /dev/sdX     &lt;span style=&#34;color:#75715e&#34;&gt;# get X using lsblk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-load-reload-modules-ipython&#34;&gt;How to load (reload) modules (iPython)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;load_ext autoreload
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;autoreload &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;power-off-an-external-hard-drive-linux&#34;&gt;Power-off an external hard drive (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ udiskctl power-off -b /dev/sdX    &lt;span style=&#34;color:#75715e&#34;&gt;# obtain X using lsblk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rsync-your-data-linux&#34;&gt;rsync your data (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ rsync -avzhP src dst --exclude-from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;file.txt --inlude&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.git/config&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;recover-data-linux&#34;&gt;Recover data (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ddrescue -dr3 /dev/sdX imagename.image logfile
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;random-execution-of-a-linux-command-linux&#34;&gt;Random execution of a Linux command (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;$((&lt;/span&gt;RANDOM &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;))&lt;/span&gt; -eq &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; command
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;open-a-file-in-vim-with-no-compatibility-vim&#34;&gt;Open a file in VIM with no compatibility (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ vim -u NONE -u NORC fname
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;io-devices-latency-linux&#34;&gt;IO devices latency (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo ioping -R /dev/sdX         &lt;span style=&#34;color:#75715e&#34;&gt;# (seek rate)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo ioping -RL /dev/sdX        &lt;span style=&#34;color:#75715e&#34;&gt;# (sequential speed)    &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;copy-a-file-to-the-clipboard-linux&#34;&gt;Copy a file to the clipboard (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ xclip -sel clip &amp;lt; file
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;import-date-in-vim-vim&#34;&gt;Import date in VIM (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-vim&#34; data-lang=&#34;vim&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;:&lt;span style=&#34;color:#a6e22e&#34;&gt;read&lt;/span&gt; !&lt;span style=&#34;color:#a6e22e&#34;&gt;date&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;import-the-output-of-a-linux-command-in-vim-vim&#34;&gt;Import the output of a Linux command in Vim (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-vim&#34; data-lang=&#34;vim&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; @&lt;span style=&#34;color:#a6e22e&#34;&gt;a&lt;/span&gt;=&lt;span style=&#34;color:#a6e22e&#34;&gt;system&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;In&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;press&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Ctrl&lt;/span&gt;+&lt;span style=&#34;color:#a6e22e&#34;&gt;R&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;then&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;convert-a-bunch-of-png-files-to-a-video-linux&#34;&gt;Convert a bunch of png files to a video (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ffmpeg -framerate 1/5 -patter_type glob -i &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.png&amp;#34;&lt;/span&gt; -vf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fps=95,format=yuv420p&amp;#34;&lt;/span&gt; output.mp4
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;convert-a-photo-into-ascii-art-linux&#34;&gt;Convert a photo into ASCII art (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ jp2a photo.jpg | tee photo.ascii
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;read-gz-files-without-extracting-them-linux&#34;&gt;Read .gz files without extracting them (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ zmore file.gz
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ zless file.gz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-load-huge-files-in-python-programming&#34;&gt;How to load huge files in Python (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mmap
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HUGE.txt&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mmap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mmap(f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filend, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, access&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mmap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ACCESS_READ, offset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mm[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;:])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;git-credential-storage-git&#34;&gt;Git credential storage (Git)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git config --global credential.helper cache &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--timeout S&amp;#39;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# S -&amp;gt; seconds&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;include-a-help-flag-in-makefiles-programming&#34;&gt;Include a help flag in Makefiles (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-make&#34; data-lang=&#34;make&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;.PHONY&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    help
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;help&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cat makefile | grep -oP &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;^#\K(.*)&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;copy-a-file-into-multiple-directories-linux&#34;&gt;Copy a file into multiple directories (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo dir1 dir2 | xargs -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; cp file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;parallel cp file ::: dir1 dir2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-run-a-elf-programming&#34;&gt;How to run a ELF (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ touch file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ cp /bin/ls .
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ chmod -x ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;-bash: ./ls: Permission denied
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ strings ./ls | head -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/lib64/ld-linux-x86-64.so.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ /lib64/ld-linux-x86-64.so.2 ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file ls
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;set-an-alert-message-when-a-job-finishes-linux&#34;&gt;Set an alert message when a job finishes (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt; execute_something_that_takes_time; xmessage DONE; &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&amp;amp;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;dennis-ritchie-linuxprogrammin&#34;&gt;Dennis Ritchie (Linux/Programmin)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ curl -L git.io/unix
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;play-tetris-linux&#34;&gt;Play tetris (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ssh netris&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;rocketnine&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;space
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;python-scope-programming&#34;&gt;Python scope (Programming)&lt;/h3&gt;
&lt;p&gt;LEGB:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;ocal (within a function)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;nclosing functions locals&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G&lt;/strong&gt;lobal (module)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;uilt-in (python) preassigned names&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c-pointers-programming&#34;&gt;C Pointers (Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ptr++ Evaluates ptr and then increments ptr by 1xbase_type_size&lt;/li&gt;
&lt;li&gt;*ptr++ Evaluates ptr, increments it, and dereferences the evaluated value&lt;/li&gt;
&lt;li&gt;int *ptr[10] Array of 10 pointers to integer (int *)&lt;/li&gt;
&lt;li&gt;int (*ptr)[10] Pointer to an array of 10 integers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;delete-the-last-column-of-a-text-file-linux&#34;&gt;Delete the last column of a text file (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NF {NF-=1};1&amp;#39;&lt;/span&gt;&amp;lt; input_file &amp;gt; output_file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NF {NF--};1&amp;#39;&lt;/span&gt;&amp;lt; input_file &amp;gt; output_file
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;automatic-variables-in-make-programming&#34;&gt;Automatic variables in Make (Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$@: The target filename.&lt;/li&gt;
&lt;li&gt;$*: The target filename without the file extension.&lt;/li&gt;
&lt;li&gt;$&amp;lt;: The first prerequisite filename.&lt;/li&gt;
&lt;li&gt;$^: The filenames of all the prerequisites, separated by spaces, discard duplicates.&lt;/li&gt;
&lt;li&gt;$+: Similar to $^, but includes duplicates.&lt;/li&gt;
&lt;li&gt;$?: The names of all prerequisites that are newer than the target, separated by spaces.&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Research Demos</title>
      <link>https://gdetor.github.io/posts/demos/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/demos/</guid>
      <description>The following videos demonstrate a computation model of primary somatosensory cortex undergoing self-organization. The model relies on Neural Field [1]. The complete mathematical/computational model as well as all the details and results are given in [2] and [3].
 Self-organization of receptive fields (RFs)    Evolution of a single RF during self-organization    Evolution of multiple RFs during self-organization    References  Dynamics of pattern formation in lateral-inhibition type neural fields A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance and Reorganization of Ordered Topographic Maps Structure of receptive fields in a computational model of area 3b of primary sensory cortex  </description>
      <content>&lt;p&gt;The following videos demonstrate a computation model of primary somatosensory
cortex undergoing self-organization. The model relies on Neural Field [1].
The complete mathematical/computational model as well as all the details
and results are given in [2] and [3].&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;self-organization-of-receptive-fields-rfs&#34;&gt;Self-organization of receptive fields (RFs)&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/JU0PKFpagUo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;evolution-of-a-single-rf-during-self-organization&#34;&gt;Evolution of a single RF during self-organization&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UzosJK8YOU0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;evolution-of-multiple-rfs-during-self-organization&#34;&gt;Evolution of multiple RFs during self-organization&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/F8JOQs2MYN4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF00337259&#34;&gt;Dynamics of pattern formation in lateral-inhibition type neural fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance and Reorganization of Ordered Topographic Maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/full&#34;&gt;Structure of receptive fields in a computational model of area 3b of primary sensory cortex&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Tweets</title>
      <link>https://gdetor.github.io/posts/news/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/news/</guid>
      <description>&amp;quot;I, Pencil&amp;quot; An essay published in 1958 which beautifully describes the amount of human coordination, cooperation and trade that needs to happen for the creation of a seemingly simple object, a lead pencil. ✏️
It&amp;#39;s a great read: https://t.co/P4Ha7HpoGp pic.twitter.com/vPPZuCZ5cB
&amp;mdash; Fermat&amp;#39;s Library (@fermatslibrary) April 6, 2021  </description>
      <content>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;I, Pencil&amp;quot; &lt;br&gt;&lt;br&gt;An essay published in 1958 which beautifully describes the amount of human coordination, cooperation and trade that needs to happen for the creation of a seemingly simple object, a lead pencil.  ✏️&lt;br&gt;&lt;br&gt;It&amp;#39;s a great read: &lt;a href=&#34;https://t.co/P4Ha7HpoGp&#34;&gt;https://t.co/P4Ha7HpoGp&lt;/a&gt; &lt;a href=&#34;https://t.co/vPPZuCZ5cB&#34;&gt;pic.twitter.com/vPPZuCZ5cB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Fermat&amp;#39;s Library (@fermatslibrary) &lt;a href=&#34;https://twitter.com/fermatslibrary/status/1379424914445377536?ref_src=twsrc%5Etfw&#34;&gt;April 6, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</content>
    </item>
    
    <item>
      <title></title>
      <link>https://gdetor.github.io/staticart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/staticart/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>About</title>
      <link>https://gdetor.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/about/</guid>
      <description>Hi, my name is Georgios Is. Detorakis, and I’m a computational neuroscientist (🧠) and machine learning engineer (🤖). On this website, you can find information about my research, any software I have developed, and some artwork related to neuroscience I have created over the years.
Currently, I am working as a full-time machine learning engineer developing algorithms for time-series analysis and forecasting (with applications in biosignals and financial data).
Here you can find my complete academic CV, industry CV (or a less extensive résumé).</description>
      <content>&lt;p&gt;Hi, my name is Georgios Is. Detorakis, and I’m a computational neuroscientist (🧠)
and machine learning engineer (🤖). On this website, you can find information about
my research, any software I have developed, and some artwork related to
neuroscience I have created over the years.&lt;/p&gt;
&lt;p&gt;Currently, I am working as a full-time machine learning engineer developing
algorithms for time-series analysis and forecasting (with applications in
biosignals and financial data).&lt;/p&gt;
&lt;p&gt;Here you can find my complete &lt;a href=&#34;https://gdetor.github.io/cv/gid_academic_cv.pdf&#34;&gt;academic CV&lt;/a&gt;, &lt;a href=&#34;https://gdetor.github.io/cv/gid_industry_cv.pdf&#34;&gt;industry CV&lt;/a&gt;
(or a less extensive &lt;a href=&#34;https://gdetor.github.io/cv/gid_resume.pdf&#34;&gt;résumé&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;You can email me at g d e t o r _ a t _ g m a i l _ c o m&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
     Live long and prosper 🖖 
&lt;/div&gt;

</content>
    </item>
    
    <item>
      <title>Artwork</title>
      <link>https://gdetor.github.io/artwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/artwork/</guid>
      <description>   Basal Ganglia Circuitry Extracellular Recordings Neural Populations              Closed-loop DBS on Basal Ganglia Crossbar Array Ball and Stick Neuron&amp;rsquo;s Model              Topological Data Analysis Topologies XOR Neural Network           </description>
      <content>&lt;!-- raw HTML omitted --&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Basal Ganglia Circuitry&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Extracellular Recordings&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Neural Populations&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/bg.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/bg.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/extracellular.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/extracellular.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/np.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/np.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Closed-loop DBS on Basal Ganglia&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Crossbar Array&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Ball and Stick Neuron&amp;rsquo;s Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/closed_dbs.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/closed_dbs.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/crossbar.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/crossbar.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/ball-stick.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/ball-stick.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Topological Data Analysis&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Topologies&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;XOR Neural Network&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/persistance.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/persistance_.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/topologies.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/topologies_.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/xor_net.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/xor_net_.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://gdetor.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/publications/</guid>
      <description>Research Articles (peer-reviewed)  OpenPelt: Python Framework for Thermoelectric Temperature Control System Development R. Parise and G. Is. Detorakis
The Journal of Open Source Software, 7(73), 4306, DOI:https://doi.org/10.21105/joss.04306
[Article]
  Randomized Self Organizing Map
N. P. Rougier and G. Is. Detorakis
Neural Computation, 33(8), 2021, DOI:https://doi.org/10.1162/neco_a_01406 [Article]
  Stability analysis of a neural field self-organizing map
G. Detorakis, A. Chaillet and N.P. Rougier
The Journal of Mathematical Neuroscience, 10(20), 2020, DOI:https://doi.</description>
      <content>&lt;h2 id=&#34;research-articles-peer-reviewed&#34;&gt;Research Articles (peer-reviewed)&lt;/h2&gt;
&lt;ol start=&#34;39&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenPelt: Python Framework for Thermoelectric Temperature Control System
Development&lt;/strong&gt;
R. Parise and G. Is. Detorakis&lt;br&gt;
The Journal of Open Source Software, 7(73), 4306, DOI:https://doi.org/10.21105/joss.04306&lt;br&gt;
&lt;a href=&#34;https://joss.theoj.org/papers/10.21105/joss.04306&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Randomized Self Organizing Map&lt;/strong&gt;&lt;br&gt;
N. P. Rougier and G. Is. Detorakis&lt;br&gt;
Neural Computation, 33(8), 2021, DOI:https://doi.org/10.1162/neco_a_01406
[Article]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stability analysis of a neural field self-organizing map&lt;/strong&gt;&lt;br&gt;
G. Detorakis, A. Chaillet and N.P. Rougier&lt;br&gt;
The Journal of Mathematical Neuroscience, 10(20), 2020, DOI:https://doi.org/10.1186/s13408-020-00097-6&lt;br&gt;
&lt;a href=&#34;https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00097-6&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GAIM: A C++ library for Genetic Algorithms and Island Models&lt;/strong&gt;&lt;br&gt;
G. Detorakis, A. Burton&lt;br&gt;
The Journal of Open Source Software, 4(44), 1839, 2019, DOI:https://doi.org/10.21105/joss.01839&lt;br&gt;
&lt;a href=&#34;https://www.theoj.org/joss-papers/joss.01839/10.21105.joss.01839.pdf&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Memory-efficient Synaptic Connectivity for Spike-Timing-Dependent Plasticity&lt;/strong&gt;&lt;br&gt;
B. U. Pedroni, S. Joshi, S. Deiss, S. Sheik, G. Detorakis, S. Paul, C. Augustine, E. O. Neftci, G. Cauwenberghs&lt;br&gt;
Frontiers in Neuroscience, DOI: &lt;a href=&#34;https://doi.org/10.3389/fnins.2019.00357&#34;&gt;https://doi.org/10.3389/fnins.2019.00357&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2019.00357/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contrastive Hebbian Learning with Random Feedback Weights&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, E. Neftci&lt;br&gt;
Neural Networks 114, 2019, doi: &lt;a href=&#34;https://doi.org/10.1016/j.neunet.2019.01.008&#34;&gt;https://doi.org/10.1016/j.neunet.2019.01.008&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S089360801930019X&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural and Synaptic Array Transceiver: A Brain-Inspired Computing Framework for Embedded Learning&lt;/strong&gt;&lt;br&gt;
G. Detorakis, S. Sheik, C. Augustine, S. Paul, B.U. Pedroni, N. Dutt, J. Krichmar, G. Cauwenberghs, E. Neftci&lt;br&gt;
Frontiers in Neuroscience, 2018&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2018.00583/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event-Driven Random Back-Propagation: Enabling Neuromorphic Deep Learning Machines&lt;/strong&gt;&lt;br&gt;
E. Neftci, S. Paul, C. Augustine, G. Detorakis&lt;br&gt;
Frontiers in Neuroscience 11, 2017, doi: &lt;a href=&#34;https://doi.org/10.3389/fnins.2017.00324&#34;&gt;https://doi.org/10.3389/fnins.2017.00324&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2017.00324/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust stabilization of delayed neural fields with partial measurement and actuation&lt;/strong&gt;&lt;br&gt;
A. Chaillet, G. Is. Detorakis, S. Palfi and S. Senova&lt;br&gt;
Automatica 83, 2017, doi: &lt;a href=&#34;https://doi.org/10.1016/j.automatica.2017.05.011&#34;&gt;https://doi.org/10.1016/j.automatica.2017.05.011&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0005109817302868&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Closed-loop stimulation of a delayed neural fields model of parkinsonian STN-GPe network: a theoretical and computational study&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis, A. Chaillet, S. Palfi and S. Senova&lt;br&gt;
Frontiers in Neuroscience 9(237), 2015, doi: &lt;a href=&#34;https://doi.org/10.3389/fnins.2015.00237&#34;&gt;https://doi.org/10.3389/fnins.2015.00237&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2015.00237/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structure of Receptive Fields in a Computational Model of Area 3b of Primary Sensory Cortex&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis and N. P. Rougier&lt;br&gt;
Frontiers in Computational Neuroscience, 8, 2014, doi: &lt;a href=&#34;https://doi.org/10.3389/fncom.2014.00076&#34;&gt;https://doi.org/10.3389/fncom.2014.00076&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance, and Reorganization of Ordered Topographic Maps&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis and N.P. Rougier&lt;br&gt;
PLoS ONE 7(7):e40257, doi: &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0040257&#34;&gt;https://doi.org/10.1371/journal.pone.0040257&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reproducible-science-articles-peer-reviewed&#34;&gt;Reproducible Science Articles (peer-reviewed)&lt;/h2&gt;
&lt;ol start=&#34;18&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sustainable computational science: the ReScience initiative&lt;/strong&gt;&lt;br&gt;
N.P. Rougier, K. Hinsen, F. Alexandre, T. Arildsen, L. Barba, F.C. Y. Benureau, C. Titus Brown, Pierre de Buyl,
O. Caglayan, A.P. Davison, M.A. Delsuc, G. Detorakis, A.K. Diem, D. Drix, P. Enel, B. Girard, O. Guest, M.G. Hall,
R.N. Henriques, X. Hinaut, K.S. Jaron, M. Khamassi, A. Klein, T. Manninen, P. Marchesi, D. McGlinn, C. Metzner,
O.L. Petchey, H.E. Plesser, T. Poisot, K. Ram, Y. Ram, E. Roesch, C. Rossant, V. Rostami, A. Shifman, J. Stachelek,
M. Stimberg, F. Stollmeier, F. Vaggi, G. Viejo, J. Vitay, A. Vostinar, R. Yurchak, T. Zito&lt;br&gt;
PeerJ Computer Science, 3, e142, 2017&lt;br&gt;
&lt;a href=&#34;https://peerj.com/articles/cs-142/&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Re] A Generalized Linear Integrate-And-Fire Neural Model Produces Diverse Spiking Behaviors&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis&lt;br&gt;
The ReScience Journal, 2017, DOI: &lt;a href=&#34;https://doi.org/10.5281/zenodo.1003214&#34;&gt;https://doi.org/10.5281/zenodo.1003214&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://rescience.github.io/bibliography/detorakis_2017.html&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Re] Multiple dynamical modes of thalamic relay neurons: rhythmic bursting and intermittent phase-locking&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis&lt;br&gt;
The ReScience Journal, 2:1, 2016, DOI: &lt;a href=&#34;https://doi.org/10.5281/zenodo.61697&#34;&gt;https://doi.org/10.5281/zenodo.61697&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://rescience.github.io/bibliography/detorakis_2016.html&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;international-conferences-peer-reviewed&#34;&gt;International Conferences (peer-reviewed)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inherent Weight Normalization in Stochastic Neural Networks&lt;/strong&gt;&lt;br&gt;
G. Detorakis, S. Dutta, A. Khanna, M. Jerry, S. Datta, and E. Neftci&lt;br&gt;
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.&lt;br&gt;
&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Recurrent Neural Network Based Model of Predictive Smooth Pursuit Eye Movement in Primates&lt;/strong&gt;&lt;br&gt;
H. Kashyap, G. Detorakis, N. Dutt, J. Krichmar,and E. Neftci&lt;br&gt;
IJCNN 2018, Rio de Janeiro, Brazil.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8489652&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incremental Stability of Spatiotemporal Delayed Dynamics and Application to Neural Fields&lt;/strong&gt;&lt;br&gt;
G. Detorakis and A. Chaillet&lt;br&gt;
IEEE CDC 2017, Melbourne, Australia.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8264558&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event-Driven Random Backpropagation: Enabling Neuromorphic Deep Learning Machines&lt;/strong&gt;&lt;br&gt;
E. Neftci, C. Augustine, S. Paul, G. Detorakis,&lt;br&gt;
IEEE ISCAS 2017, Baltimore, MD, USA.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8050529&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forward Table-Based Presynaptic Event-Triggered Spike-Timing-Dependent Plasticity&lt;/strong&gt;&lt;br&gt;
B. U. Pedroni, S. Sheik, S. Joshi, G. Detorakis, S. Paul, C. Augustine, E. Neftci, G. Cauwenberghs,&lt;br&gt;
IEEE BioCAS 2016, Shanghai, China.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7833861&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPySort: Neuronal Spike Sorting with Python&lt;/strong&gt;&lt;br&gt;
C. Pouzat and G.Is. Detorakis,&lt;br&gt;
Euroscipy 2014, Cambridge, United Kingdom.&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/1412.6383&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Organizing Dynamic Neural Fields&lt;/strong&gt;&lt;br&gt;
N. P. Rougier and G. Is. Detorakis&lt;br&gt;
3rd International Conference on Cognitive Neurodynamics, Hokkaido, Japan, 2011.&lt;br&gt;
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-007-4792-0_38&#34;&gt;[Article]&lt;/a&gt;
&lt;a href=&#34;https://hal.inria.fr/inria-00587508/document&#34;&gt;[pdf]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;international-conferences&#34;&gt;International Conferences&lt;/h2&gt;
&lt;ol start=&#34;22&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A neural network model of predictive smooth pursuit eye movement in primates&lt;/strong&gt;&lt;br&gt;
H.J. Kashyap, G. Detorakis, N. Dutt, J.L. Krichmar, E. Neftci&lt;br&gt;
SfN, San Diego (CA, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Random Contrastive Hebbian Learning as a Biologically Plausible Learning Scheme&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, and E. Neftci&lt;br&gt;
OCNS, Seattle (WA, USA), 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Three-factor embedded learning on neuromorphic systems&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, R. Parise, S. Sheik, C. Augustine, S. Paul, B. U. Pedroni, N. Dutt, J.Krichmar, G. Cauwenberghs, and E. Neftci&lt;br&gt;
COSYNE, Denver (CO, USA), 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedded Learning on Neuromorphic Systems: Towards a Unified Computing Framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, R. Parise, S. Sheik, C. Augustine, S. Paul, B. Pedroni, N. Dutt, J. Krichmar, G. Cauwenberghs and E. Neftci&lt;br&gt;
NICE, Portland (OR, USA), 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedded learning on neuromorphic systems: Towards a unified computing framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, R. Parise, C. Augustine, S. Paul, E. Neftci&lt;br&gt;
IEED ICCAD HALO Workshop, Irvine (CA, USA), 2017&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NeuroLachesis: A Neuromorphic Framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, D. Barsever, E. Neftci&lt;br&gt;
Scipy 2017, Austin (TX, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust stabilization of delayed neural fields by proportional feedback using input-to-state stability and small gain theorem&lt;/strong&gt;&lt;br&gt;
A. Chaillet, G. Is. Detorakis, S. Palfi, S. Senova&lt;br&gt;
ICMNS 2016, Juan-les-Pins, France&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Closed-loop disruption of oscillations in a targeted frequency band for a delayed neural field STN-GPe model&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and A. Chaillet&lt;br&gt;
FENS Featured Regional Meeting 2015, Thessaloniki, Greece&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incremental stability of delayed neural fields: a unifying framework for endogenous and exogenous sources of pathological oscillations&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and A. Chaillet&lt;br&gt;
CNS, Prague, Czech Republic, 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Closed-loop regulation of the activity of delayed neural fields with only partial measurement and stimulation&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and A. Chaillet&lt;br&gt;
ICMNS, Antibes - Juan les Pins, France, 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A global stability analysis for delayed neural fields&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis, A. Chaillet and I. Haidar&lt;br&gt;
BCCN 2014, Göttingen, Germany&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A computational view of the primary somatosensory cortex&lt;/strong&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
CNS Annual Meeting, Paris, France, 2013&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural Fields and Cortical Plasticity&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
BCCN, Freiburg, Germany, 2011&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;minor-conferences&#34;&gt;Minor Conferences&lt;/h2&gt;
&lt;ol start=&#34;35&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedded learning on neuromorphic systems: Towards a unified computing framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, C. Augustine, S. Paul, E. Neftci&lt;br&gt;
24th Joint Symposium on Neural Computation, San Diego(CA, USA), 2017&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On the relation between neuronal size and extracellular spike amplitude and its consequence on extracellular recordings interpretation&lt;/strong&gt;&lt;br&gt;
C. Pouzat and G. Is. Detorakis&lt;br&gt;
MathStatNeuro Workshop, Nice(France), 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPySort&lt;/strong&gt;&lt;br&gt;
C. Pouzat and G. Is. Detorakis&lt;br&gt;
GDR Multielectrode systems and signal processing for Neuroscience, Gif-sur-Yvette(France), 2014&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Skin Topographic Maps in SI&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
Progress in Neural Field Theory, Reading, United Kingdom, 2012&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Skin Topographic Maps in SI&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
Workshop on Cognitive and Dynamics in Neural Systems: Mathematical and Computational Modeling (CONAS), Lyon, France, 2012&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;book-chapters&#34;&gt;Book Chapters&lt;/h2&gt;
&lt;ol start=&#34;21&#34;&gt;
&lt;li&gt;&lt;strong&gt;ISS-stabilization of delayed neural fields by small-gain arguments&lt;/strong&gt;&lt;br&gt;
A. Chaillet, G. Is. Detorakis, S. Palfi, and S. Senova&lt;br&gt;
Delays and Interconnections: Methodology, Algorithms and Applications, Springer, 2019&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;popular-science&#34;&gt;Popular Science&lt;/h2&gt;
&lt;ol start=&#34;40&#34;&gt;
&lt;li&gt;&lt;strong&gt;Optogenetics to unravel the mechanisms of Parkinsonian symptoms and to optimize deep brain stimulation&lt;/strong&gt;&lt;br&gt;
A. Chaillet, D. Da Silva, G. Detorakis, C. Pouzat, S. Senova&lt;br&gt;
ERCIM News, Special issue on cyber-physical systems, number 97, April 2014&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://gdetor.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/research/</guid>
      <description>Neuromorphic Computing Georgios was involved in developing neuromorphic devices capable of embedded and online learning. He was one of the core developers of the software simulator for the Neural and Synaptic Array Transceiver Framework (in collaboration with Intel Corporation Research Labs, the Universities of California Irvine, and San Diego). He investigated how natural mechanisms of biological brains can lead to more efficient and biologically plausible machine learning algorithms suitable for neuromorphic devices.</description>
      <content>&lt;h2 id=&#34;neuromorphic-computing&#34;&gt;Neuromorphic Computing&lt;/h2&gt;
&lt;p&gt;Georgios was involved in developing neuromorphic devices capable of embedded
and online learning. He was one of the core developers of the software simulator
for the Neural and Synaptic Array Transceiver Framework (in collaboration with Intel
Corporation Research Labs, the Universities of California Irvine, and San Diego).
He investigated how natural mechanisms of biological brains can lead to more
efficient and biologically plausible machine learning algorithms suitable for
neuromorphic devices.&lt;/p&gt;
&lt;h2 id=&#34;parkinsons-disease&#34;&gt;Parkinson&amp;rsquo;s Disease&lt;/h2&gt;
&lt;p&gt;Another research interest is Parkinson&amp;rsquo;s disease (PD). He has combined
neuroscience and control theory to study PD and propose potential treatments.
He has developed computational models based on neural fields theory for the
basal ganglia and PD and has investigated closed-loop deep brain stimulation
(DBS) with applications on PD treatments.&lt;/p&gt;
&lt;h2 id=&#34;cortical-plasticity--self-organizing-maps&#34;&gt;Cortical Plasticity &amp;amp; Self-organizing Maps&lt;/h2&gt;
&lt;p&gt;His research as a Ph.D. student focused on cortical plasticity and self-organization.
He proposed a mathematical/computational model for studying self-organization
in the primary somatosensory cortex. The model relied on the neural field&amp;rsquo;s theory.
Simulation of his model showed how the cerebral cortex builds up topographic maps.
Furthermore, he demonstrated how the brain maintains topographic maps and how they
get reorganized in the face of a lesion.&lt;/p&gt;
&lt;h2 id=&#34;rhythmical-motor-control&#34;&gt;Rhythmical Motor Control&lt;/h2&gt;
&lt;p&gt;Finally, as a Master&amp;rsquo;s student, he studied rhythmical motor control and human
tremor. He has applied signal processing theory and analysis on neurophysiological
signals like EEG and EMG, trying to determine the role of human tremor.
Furthermore, he has investigated Central Pattern Generators (CPGs) and their
role in biped locomotion.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;He also has some experience in:&lt;/p&gt;
&lt;h3 id=&#34;natural-language-processing-nlp&#34;&gt;Natural Language Processing (NLP)&lt;/h3&gt;
&lt;p&gt;Georgios has worked as Data Science Architect for adNomus Inc. focusing
mainly on Natural Language Processing (NLP). He is developing NLP algorithms
with applications on Recommendation Systems and Content Analysis.&lt;/p&gt;
&lt;h3 id=&#34;recording-techniques&#34;&gt;Recording Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Electroencephalogram (EEG) &lt;a href=&#34;https://en.wikipedia.org/wiki/Electroencephalography&#34;&gt;?&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Electromyogram (EMG) &lt;a href=&#34;https://en.wikipedia.org/wiki/Electromyography&#34;&gt;?&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;And minor experience in intracellular and extracellular in vivo recordings &lt;a href=&#34;https://en.wikipedia.org/wiki/Electrophysiology&#34;&gt;?&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Software</title>
      <link>https://gdetor.github.io/software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/software/</guid>
      <description>Miscellaneous omnipyseed Omnipyseed is a tiny and simple Python package that can be used to seed random number generators (RNGs) of Python, Numpy, and Pytorch.
 Programming Language: Python Source Code:  .inline-svg { display: inline-block; height: 1.0rem; width: 1.0rem; top: 0.1rem; position: relative; }     PyPi  Machine/Deep Learning Time series collection There are many ways to perform time series forecasting. One of the most recent (modern) ways is to use artificial neural networks (or even deep neural networks) to model time series and perform predictions usign those models.</description>
      <content>&lt;h2 id=&#34;miscellaneous&#34;&gt;Miscellaneous&lt;/h2&gt;
&lt;h3 id=&#34;omnipyseed&#34;&gt;omnipyseed&lt;/h3&gt;
&lt;p&gt;Omnipyseed is a tiny and simple Python package that can be used to seed random
number generators (RNGs) of Python, Numpy, and Pytorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/omnipyseed&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.org/project/omnipyseed/&#34;&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machinedeep-learning&#34;&gt;Machine/Deep Learning&lt;/h2&gt;
&lt;h3 id=&#34;time-series-collection&#34;&gt;Time series collection&lt;/h3&gt;
&lt;p&gt;There are many ways to perform time series forecasting. One of the most recent
(modern) ways is to use artificial neural networks (or even deep neural
networks) to model time series and perform predictions usign those models.
The present repository provides three mainstream artificial neural networks,
an LSTM, a MLP, and a TCN for time series forecasting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://gitlab.com/gdetor/time_series_forecasting&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 512 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M105.2 24.9c-3.1-8.9-15.7-8.9-18.9 0L29.8 199.7h132c-.1 0-56.6-174.8-56.6-174.8zM.9 287.7c-2.6 8 .3 16.9 7.1 22l247.9 184-226.2-294zm160.8-88l94.3 294 94.3-294zm349.4 88l-28.8-88-226.3 294 247.9-184c6.9-5.1 9.7-14 7.2-22zM425.7 24.9c-3.1-8.9-15.7-8.9-18.9 0l-56.6 174.8h132z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autoencoders&#34;&gt;Autoencoders&lt;/h3&gt;
&lt;p&gt;Autoencoders are a type of artificial neural networks that can learn efficient
codings of unlabeled data. Autoencoders learn the code by trying to map the
identity function (&lt;strong&gt;e.g.&lt;/strong&gt;, they try to reconstruct the input at their output
layer). This repository contains four types of autoencoders: (i) a standard
linear autoencoder (AE and Variational AE), (ii) an LSTM autoencoder (AE and
VAE), (iii) a convolutional autoencoder, and finally a causal convolutional
autoencoder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://gitlab.com/gdetor/autoencoders&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 512 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M105.2 24.9c-3.1-8.9-15.7-8.9-18.9 0L29.8 199.7h132c-.1 0-56.6-174.8-56.6-174.8zM.9 287.7c-2.6 8 .3 16.9 7.1 22l247.9 184-226.2-294zm160.8-88l94.3 294 94.3-294zm349.4 88l-28.8-88-226.3 294 247.9-184c6.9-5.1 9.7-14 7.2-22zM425.7 24.9c-3.1-8.9-15.7-8.9-18.9 0l-56.6 174.8h132z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pytorch-time2vec&#34;&gt;Pytorch-Time2Vec&lt;/h3&gt;
&lt;p&gt;Time2Vec is an algorithm that provides a learning representation of time, which
is model agnostic and can be used to encode temporal dynamics in many different
applications (&lt;em&gt;e.g.&lt;/em&gt;, in Transformers for time series).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://arxiv.org/abs/1907.05321&#34;&gt;&amp;ldquo;Time2Vec: Learning a vector representation of time&amp;rdquo;,
Kazemi et al., arXiv, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pytorch_time2vec&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;neural-sampling-machines&#34;&gt;Neural Sampling Machines&lt;/h3&gt;
&lt;p&gt;NSMs or Neural Sampling Machines is a Pytorch implementation of the algorithms
proposed in&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://papers.nips.cc/paper/2019/hash/dfce06801e1a85d6d06f1fdd4475dacd-Abstract.html&#34;&gt;&amp;ldquo;Inherent Weight Normalization in Stochastic Neural Networks&amp;rdquo;, Detorakis et al.,
NeurIPS 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/nmi-lab/neural_sampling_machines&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;random-contrastive-hebbian-learning-rchl&#34;&gt;Random Contrastive Hebbian Learning (rCHL)&lt;/h3&gt;
&lt;p&gt;rCHL is a learning algorithm that relies on the contrastive Hebbian learning
algorithm. The major difference is that the feedback pathway
does not use any kind of learnable weights. Instead, it exploits fixed random
weights.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S089360801930019X&#34;&gt;&amp;ldquo;Contrastive Hebbian Learning with Random Feedback Weights&amp;rdquo;, Detorakis et al.,
Neural Networks, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C and Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://gitlab.com/gdetor/rCHL&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 512 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M105.2 24.9c-3.1-8.9-15.7-8.9-18.9 0L29.8 199.7h132c-.1 0-56.6-174.8-56.6-174.8zM.9 287.7c-2.6 8 .3 16.9 7.1 22l247.9 184-226.2-294zm160.8-88l94.3 294 94.3-294zm349.4 88l-28.8-88-226.3 294 247.9-184c6.9-5.1 9.7-14 7.2-22zM425.7 24.9c-3.1-8.9-15.7-8.9-18.9 0l-56.6 174.8h132z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;restricted-bolzmann-machine-in-c&#34;&gt;Restricted Bolzmann Machine (in C)&lt;/h3&gt;
&lt;p&gt;Restricted Boltzmann Machine is an artificial neural network with generative
capabilities. Usually, it consists of two layers and can learn a probability
distribution over its set of inputs through a contrastive divergence algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;http://people.stat.sfu.ca/~dac5/BoltzmannMachines.pdf&#34;&gt;&amp;ldquo;Bolzmann Machines&amp;rdquo;, Hinton 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C and Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/cRBM&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;annealed-importance-sampling-pyais&#34;&gt;Annealed Importance Sampling (pyAIS)&lt;/h3&gt;
&lt;p&gt;When one has to compute the partition function Z of a probability distribution
(Z = \sum_{x}f(x), where p(x) = \frac{1}{Z}f(x)) often applies an Importance
Sampling (IS) method. The issue with IS is the choice of its single
hyperparameter, namelly the proposal distribution. Annealed Importance Sampling
overcomes that problem by creating intermediate distributions. AIS does this
by &amp;ldquo;moving&amp;rdquo; the proposal distribution until one gets a fair approximation of the
target (original) distribution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Papers:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/physics/9803008&#34;&gt;&amp;ldquo;Annealed Importance Sampling&amp;rdquo;, R.M. Neal, 1998&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~rsalakhu/papers/bm.pdf&#34;&gt;&amp;ldquo;Learning and Evaluating Boltzmann Machines&amp;rdquo;, R. Salakhutdinov, 2008&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pyAIS&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;self-organizing-maps-dx-dy-representations&#34;&gt;Self-organizing maps Dx-Dy representations&lt;/h3&gt;
&lt;p&gt;SOM-DyDx, is a python script that implements Demartines dy-dx representation
method. This method is useful to inspect if a self-organized map
is well-organized.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &amp;ldquo;Organization measures and representations of the Kohonen maps&amp;rdquo;, P. Demartines, 1992&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Numpy)&lt;/li&gt;
&lt;li&gt;Source Code:&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neuromorphic-computing&#34;&gt;Neuromorphic Computing&lt;/h2&gt;
&lt;h3 id=&#34;neural-and-synaptic-array-transceiver-nsat&#34;&gt;Neural and Synaptic Array Transceiver (NSAT)&lt;/h3&gt;
&lt;p&gt;Neural and Synaptic Array Transceiver is a Neuromorphic Computational Framework
facilitating flexible and efficient embedded learning by matching algorithmic
requirements and neural and synaptic dynamics. NSAT supports event-driven
supervised, unsupervised and reinforcement learning algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2018.00583/full&#34;&gt;Neural and Synaptic Array Transceiver: A Brain-Inspired
Computing Framework for Embedded Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C/Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/nmi-lab/NSAT&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nsatcarl&#34;&gt;NSATCarl&lt;/h3&gt;
&lt;p&gt;NSATCarl is a simple C++ library that brings together the Neural and Synaptic
Array Transceiver (NSAT) and the neural simulator &lt;a href=&#34;https://www.socsci.uci.edu/~jkrichma/CARLsim/&#34;&gt;CARLsim&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: C++&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/NSATcarl&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;optimization&#34;&gt;Optimization&lt;/h2&gt;
&lt;h3 id=&#34;genetic-algorithms-and-island-models-gaim&#34;&gt;Genetic Algorithms and Island Models (GAIM)&lt;/h3&gt;
&lt;p&gt;Genetic Algorithms (GAs) are optimization metaheuristics inspired by the theory
of evolution and the process of natural selection. GAs belong to a larger class
of heuristics called evolutionary algorithms (EA). Island Model (IM) is a
collection of algorithms that enable many populations to evolve over the same
optimization problem providing faster convergence to a solution than a simple GA.
Furthermore, IMs can overcome the problem of local minima due to a migration
policy between islands (different populations).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://joss.theoj.org/papers/10.21105/joss.01839&#34;&gt;&amp;ldquo;GAIM: A C++ library for Genetic Algorithms and
Island Models, G. Is. Detorakis and A. Burton, JOSS, 2019&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C++/Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/gaim&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamical-systems&#34;&gt;Dynamical Systems&lt;/h2&gt;
&lt;h3 id=&#34;empirical-dynamic-modeling-empyred&#34;&gt;Empirical Dynamic Modeling (empyred)&lt;/h3&gt;
&lt;p&gt;Empirical Dynamic Modeling (EDM) is an equation-free framework for modeling
non-linear dynamic systems. EDM relies on Takens&amp;rsquo; (1981) theorem of temporal
embeddings and reconstruction of system attractors from temporal embeddings.
&lt;strong&gt;Empyred&lt;/strong&gt; is a pure Python implementation of the EDM methods: Simplex and SMAP.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Papers:
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Nonlinear forecasting as a way of distinguishing chaos from measurement error in time series&amp;rdquo; (Sugihara and May, 1990)&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Nonlinear forecasting for the classification of natural time series&amp;rdquo;, (Sugihara, 1994).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Relevant Material: &lt;a href=&#34;https://deepeco.ucsd.edu/nonlinear-dynamics-research/edm/#page-content&#34;&gt;Sugihara&amp;rsquo;s Lab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Numpy, Scipy, Sklearn, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/empyred&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neuroscience&#34;&gt;Neuroscience&lt;/h2&gt;
&lt;h3 id=&#34;correlated-spike-trains-corrspiketrains&#34;&gt;Correlated Spike Trains (CorrSpikeTrains)&lt;/h3&gt;
&lt;p&gt;This is a Python implementation of the paper &amp;ldquo;Generation of correlated spike
trains&amp;rdquo;. Both the doubly stochastic processes and the mixture method described
in the original paper are implemented.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf&#34;&gt;&amp;ldquo;Generation of correlated spike trains&amp;rdquo;, R. Brette,
Neural Computation, 2009&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/CorrSpikeTrains&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;spike-sorting---spysort&#34;&gt;Spike sorting - SPySort&lt;/h3&gt;
&lt;p&gt;Spike sorting is a class of algorithms for classifying spikes into clusters
based on a similarity measure. Usually, spike sorting identifies the waveforms
of neural spiking in signals collected from extracellular recordings. SpySort
is a Python package that implements a spike sorting algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://arxiv.org/abs/1412.6383&#34;&gt;&amp;ldquo;SPySort: Neuronal Spike Sorting with Python&amp;rdquo;,
C. Pouzat and G.Is. Detorakis, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/SPySort&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;neural-fields-and-deep-brain-stimulation&#34;&gt;Neural Fields and Deep Brain Stimulation&lt;/h3&gt;
&lt;p&gt;This project regards a deep brain stimulation (DBS) model of the globus pallidus
and the subthalamic nucleus of the basal ganglia. The model explores the effects
of optogenetic DBS stimulation in closed-loop treatments of motor symptoms of
Parkinson’s disease. The model relies on neural fields and a simple proportional
controller.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2015.00237/full&#34;&gt;&amp;ldquo;Closed-loop stimulation of a delayed neural fields model
of parkinsonian STN-GPe network: a theoretical and computational study&amp;rdquo;,
Detorakis et al, Frontiers in Neuroscience, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/neuralfieldDBSmodel&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;primary-somatosensory-cortex-and-structure-of-its-receptice-fields&#34;&gt;Primary Somatosensory Cortex and Structure of its Receptice Fields&lt;/h3&gt;
&lt;p&gt;This repo contains a computational model of area 3b of the somatosensory cortex.
The model relies on the neural field&amp;rsquo;s theory and reproduces an exact experimental
protocol described in DiCarlo et al., 1998. The model describes the structure
of receptive fields in area 3b of the primary somatosensory cortex and how
attention mechanisms affect that structure. The source code, beyond the model,
provides tools to analyze the results and generate the figures from the original
paper. The model is capable of obtaining similar results as the original experiment
described in DiCarlo et al..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/fulL&#34;&gt;&amp;ldquo;Structure of receptive fields in a computational model
of area 3b of primary sensory cortex&amp;rdquo;, G.Is. Detorakis and N.P. Rougier, Frontiers in Neuroscience, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/SI-RF-Structure&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;self-organization-and-sensory-topographic-maps&#34;&gt;Self-Organization and Sensory Topographic Maps&lt;/h3&gt;
&lt;p&gt;A collection of Python scripts implement a self-organization model of the
primary somatosensory cortex. The model relies on the neural field&amp;rsquo;s theory
and describes the dynamics of the primary somatosensory cortex of monkeys.
It forms, maintains, and reorganizes somatosensory topographic maps following
a biologically plausible Oja-like learning rule.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;&amp;ldquo;A Neural Field Model of the Somatosensory Cortex:
Formation, Maintenance and Reorganization of Ordered Topographic Maps&amp;rdquo;,
G.Is. Detorakis and N.P. Rougier, PloS ONE, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/SITopMaps&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;numerical-analysis&#34;&gt;Numerical Analysis&lt;/h2&gt;
&lt;h3 id=&#34;pseudospectra-analysis-for-rectangular-matrices-pygpsa&#34;&gt;Pseudospectra Analysis for Rectangular Matrices (pygpsa)&lt;/h3&gt;
&lt;p&gt;Pseudospectra of a matrix (or an operator) is a set containing its spectrum
and the &amp;ldquo;pseudo&amp;rdquo;-eigenvalues. Pseudospectra is particularly useful for
understanding and/or revealing information about non-normal matrices (operators).
A Python script that computes the pseudospectra of a rectangular matrix.
For more information about pseudospectra visit the &lt;a href=&#34;https://www.cs.ox.ac.uk/pseudospectra/&#34;&gt;Pseudospectra Gateway&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;http://people.maths.ox.ac.uk/~trefethen/publication/PDF/2002_101.pdf&#34;&gt;&amp;ldquo;Pseudospectra of rectangular matrices&amp;rdquo;, T.G. Wright
and L.N. Trefethen, IMA Journal of Numerical Analysis, 22, 501&amp;ndash;519, 2002&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pygpsa&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;crkmethods&#34;&gt;CRKMethods&lt;/h3&gt;
&lt;p&gt;Runge-Kutta (RK) methods are iterative methods used in the temporal discretization
of (numerical) approximations of Ordinary Differential Equations (ODEs).
CRKMethods is a collection of explicit RK methods: (i) Foreward Euler’s method,
(ii) RK45, (iii) Refined RK45, and (iv) Fehlberg’s method. Each method is
implemented as a single step, so the end-user has to run over N timesteps to
get the final solution. Furthermore, the end-user is responsible for providing
the time-step (dt). For more details, please have a look at the file &lt;em&gt;main.c&lt;/em&gt; in
the &lt;em&gt;src&lt;/em&gt; directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: C&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/CRKMethods&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
