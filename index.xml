<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GID Webpage</title>
    <link>https://gdetor.github.io/</link>
    <description>Recent content on GID Webpage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>â“’  GID, 2021-2022</copyright>
    <lastBuildDate>Thu, 13 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gdetor.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://gdetor.github.io/posts/welcome/</link>
      <pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/welcome/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>Autocorrelation Functions for Time Series</title>
      <link>https://gdetor.github.io/posts/acf_pacf/</link>
      <pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/acf_pacf/</guid>
      <description>This post aims to provide some theoretical background on autocorrelation functions and how we can use it to analyse time series. Furthremore, we show how to use the autocorrelation (ACF) function and the Partial Autocorrelation (PACF) functions to determine the parameters for an ARMA model.
Brief introduction to time series In a previous post (see here) we introduced some basic definitions and terminology about time series. To avoid forcing the reader to move back and forth we repeat those very same definitions here.</description>
      <content>&lt;p&gt;This post aims to provide some theoretical background on autocorrelation functions
and how we can use it to analyse time series. Furthremore, we show how to use
the autocorrelation (ACF) function and the Partial Autocorrelation (PACF) functions
to determine the parameters for an ARMA model.&lt;/p&gt;
&lt;h2 id=&#34;brief-introduction-to-time-series&#34;&gt;Brief introduction to time series&lt;/h2&gt;
&lt;p&gt;In a previous post (see &lt;a href=&#34;https://gdetor.github.io/posts/errors&#34;&gt;here&lt;/a&gt;) we
introduced some basic definitions and terminology about time series. To avoid
forcing the reader to move back and forth we repeat those very same definitions
here.&lt;/p&gt;
&lt;p&gt;In layman&amp;rsquo;s terms, a time series is a series of observations (or data points)
indexed in time order [1, 2].
A few examples of time series are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The daily closing price of a stock in the stock market&lt;/li&gt;
&lt;li&gt;The number of air passengers per month&lt;/li&gt;
&lt;li&gt;The biosignals recorded from electroencephalogram or electrocardiogram&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 1 shows the number of air passengers per month from January 1949
to September 1960.  In all the examples in this post, we are going to
use this dataset, so if you would like to try the examples by yourself;
you can download the data from Kaggle
&lt;a href=&#34;https://www.kaggle.com/datasets/rakannimer/air-passengers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/passengers.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. An example of a time series showing the number of air passengers per month.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;A more rigorous definition of a time series found in [1] (Chapter 1, pg 1)
is given below:&lt;/p&gt;
&lt;p&gt;Let $ k \in \mathbf{N}, T \subseteq \mathbf{R} $. A function
$$ x: T \rightarrow \mathbf{R}^k, \hspace{2mm} t \rightarrow x_t $$
or equivalently, a set of indexed elements of $ \mathbf{R}^k $,
$$  \begin{Bmatrix} x_t | x_t \in \mathbf{R}^k, \hspace{2mm} t \in T \end{Bmatrix}  $$
is called an observed time series (or time series).
Sometimes, we write $ x_t(t \in T) $ or $ (x_t)_{t\in T} $.&lt;/p&gt;
&lt;p&gt;When $ k = 1 $ the time series is called &lt;em&gt;univariate&lt;/em&gt;, otherwise is
called &lt;em&gt;multivariate&lt;/em&gt;.
$ T $ determines if the time series is [1]:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;discrete&lt;/strong&gt; $ T $ is countable, and $ \forall a &amp;lt; b \in \mathbb{R}: T \cap[a, b] $
is finite,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;continuous&lt;/strong&gt; $ T = [a, b], a &amp;lt; b \in \mathbb{R}, T = \mathbb{R}_{+}
\hspace{2mm} \text{or} \hspace{2mm} T = \mathbb{R} $,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;equidistant&lt;/strong&gt; $ T $ is discrete, and $ \exists u \hspace{2mm} s.t. \hspace{2mm} t_{j+1} - t_j = u $.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;From now on and for simplicity&amp;rsquo;s sake we will use the following notation for
a time series: $ \begin{Bmatrix} y[1], y[2], \ldots , y[N] \end{Bmatrix} $, where
$ N \in \mathbb{N} $ or $ y[t] $, where $t=1, \ldots , N $.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;what-is-autocorrelation-acf&#34;&gt;What is autocorrelation (ACF)&lt;/h2&gt;
&lt;p&gt;The Autocorrelation function (from now on, we will refer to the autocorrelation
function as ACF) for a given time series $ x[t] $ at times $ t_1, t_2, \ldots, t_N $
with mean $ \mu_t $ and the time series $ x[s] $ at time $ s_1, s_2, \ldots, s_N $
with mean $ \mu_s $ is givenb by following formula:&lt;/p&gt;
&lt;p&gt;$$ \rho(s, t) = \frac{\mathbb{E}[(x[s]-\mu_s)(x[t]-\mu_t)]}{\mathbb{E}[(x[s]-\mu_s)^2] \mathbb{E}[(x[t]-\mu_t)^2]}.  \qquad (1)  $$&lt;/p&gt;
&lt;p&gt;Another way to express the ACF is through the autocovariance function.
By plugging the equation of correlation&lt;/p&gt;
&lt;p&gt;$$ \gamma(s, t) = \text{cov}(x[s], x[t]) = \mathbb{E}[(x[s]-\mu_s)(x[t]-\mu_t)], $$
into equation (1) we obtain
$$ \rho(s, t) = \frac{\gamma(s, t)}{\sqrt{\gamma(s, s) \gamma(t, t)}}.  $$&lt;/p&gt;
&lt;p&gt;The above definitions are valid for time series of real numbers. If the time
series are complex then we have to replace the term $ \mathbb{E}[(x[s]-\mu_s)(x[t]-\mu_t)] $
with the complex conjugate $\mathbb{E}[(x[s]-\mu_s)\bar{(x[t]-\mu_t)}] $.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From the definitions above one can conclude, first, the autocovariance of
a time series with itself is the variance of the signal (&lt;em&gt;i.e.&lt;/em&gt;, $ \gamma(t, t) = \text{Var}[x[t]] $)
and second, the autocorrelation of a time series x[t] with itself is one (&lt;em&gt;i.e.&lt;/em&gt;, $ \rho(t, t) = 1 $).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s now explain the autocorrelation in layperson&amp;rsquo;s terms and give some
intuition on the information we obtain by using it. Let&amp;rsquo;s start with the
very notion of a time series. In a time series, the data points are ordered
temporally. Thus, extra information is stored in a time series, meaning we
can determine how past data (historical data) can affect present or future
values or the correlation between those points. Therefore, autocorrelation
or serial correlation is nothing more than the correlation of a data point
with its past values (previous times).&lt;/p&gt;
&lt;h3 id=&#34;how-to-compute-acf&#34;&gt;How to compute ACF&lt;/h3&gt;
&lt;p&gt;Therefore, to estimate the ACF, we start by evaluating the correlation
of the original time series with itself (zero lag). Then we introduce a
lag into our original time series and estimate the correlation between
the original time series and its lagged version. We repeat those steps until
we exhaust the predetermined number of lags. The table below shows an example
of a time series and its first three lagged signals. From now on, we will
use the letter $ k $ to indicate any lag.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;($ k = 0 $)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$ k = 1 $&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$ k = 2 $&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$ k = 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NaN&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NaN&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NaN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NaN&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NaN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NaN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the ACF for the data given in the table above.
First, we will write our custom ACF function in the Python programming
language, and then we will introduce the ACF function of the &lt;em&gt;statsmodels&lt;/em&gt;
Python package.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rho&lt;/span&gt;(x, nlags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Create a list with all the lagged versions of input signal x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x_lagged &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [x[:(len(x) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(nlags)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Compute the mean of input x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x_bar &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Initialize a list that contains the correlations for each lag&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    acf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Loop over all lags and use equation (1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; lag &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(nlags):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        nominator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((x[lag:] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x_bar) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (x_lagged[lag] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x_bar))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum() 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        denominator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x_bar)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        acf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(nominator &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; denominator)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; acf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; rho(x, nlags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5714285714285714&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.17857142857142858&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.14285714285714285&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above code snippet shows how to implement a naive autocorrelation
algorithm. Now, let&amp;rsquo;s compute the ACF but this time using the &lt;em&gt;statsmodels&lt;/em&gt;
Python package (if you haven&amp;rsquo;t installed the statsmodels package, you will
find all the installation information &lt;a href=&#34;https://www.statsmodels.org/dev/install.html&#34;&gt;here&lt;/a&gt;).
The package &lt;em&gt;statsmodels&lt;/em&gt; provides tools for the estimation of different
statistical models, as well as for conducting statistical tests and statistical
data exploration. We will use the TSA submodule, which contains time series
analysis and prediction tools.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; statsmodels.tsa.stattools &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; acf       &lt;span style=&#34;color:#75715e&#34;&gt;# import ACF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; acf(x, nlags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, fft&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;array([ &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;        ,  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.57142857&lt;/span&gt;,  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.17857143&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.14285714&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we expected, the ACF of the statsmodels package returns the same
results as our custom-made function does. The only difference is that
we used four lags in our custom function, and that&amp;rsquo;s because we count
the zero lag ($ k = 0 $) as well. For more information about the ACF
function you can check the &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.acf.html&#34;&gt;docs&lt;/a&gt;
of &lt;em&gt;statsmodels&lt;/em&gt; package.&lt;/p&gt;
&lt;p&gt;One important note regarding ACF and later the PACF is the number of
lags we have to use. The &lt;em&gt;statsmodels&lt;/em&gt; module computes the number of
lags using the following formula&lt;/p&gt;
&lt;p&gt;$$ \text{nlags} = \min \begin{Bmatrix} 10 \log_{10}(N), N - 1  \end{Bmatrix}, $$&lt;/p&gt;
&lt;p&gt;where $ N $ is the number of observations in the time series (raw data).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ACF is often used to identify non-randomness in a time series and to
aid in determining the parameters of a mathematical model for that time series.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;what-is-partial-autocorrelation-pacf&#34;&gt;What is partial autocorrelation (PACF)&lt;/h2&gt;
&lt;p&gt;Partial autocorrelation, or PACF, is more challenging to understand than ACF.
Let&amp;rsquo;s understand the intuition behind the PACF, and then we will see how to
estimate the PACF for a time series. Remember that the ACF reveals how a
measurement (or a data point) at a given time index $ t $ affects a data point
at time $ t - k $ ($ k $ is the lag). In that case, the formula for estimating
ACF considers all the intermediate data points $ t + 1, t + 2, \ldots, t + k - 1 $
until it reaches the $ t + k $. Now, the PACF ignores the information in the
intermediate data points; hence it is called partial.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now give a more rigorous mathematical definition of PACF. Assume that we
have a time series x[t], then the PACF is defined as&lt;/p&gt;
&lt;p&gt;$$ \rho_{\text{partial}}(t, t) = \text{corr}(x[t+1], x[t]) \quad for k = 1, $$&lt;/p&gt;
&lt;p&gt;$$ \rho_{\text{partial}}(t, k) = \text{corr}(x[t+k] - \hat{x}[t+k], x[t] - \hat{x}[t]), \quad for k \geq 2, $$&lt;/p&gt;
&lt;p&gt;where $ \hat{x}[t+k] $ and $ \hat{x}[t] $ are linear combinations of all the
intermediate data points (&lt;em&gt;i.e.&lt;/em&gt;, $ x[t+1], x[t+2], \ldots, x[t+k-1]$) that
minimize the mean squared error (see &lt;a href=&#34;https://gdetor.github.io/posts/errors&#34;&gt;here&lt;/a&gt;)
of $ x[t+k] $ and $ x[t] $, respectively.&lt;/p&gt;
&lt;h3 id=&#34;how-to-compute-pacf&#34;&gt;How to compute PACF&lt;/h3&gt;
&lt;p&gt;Estimation of the PACF can be done with several algorithms. The most known
are (i) the Yule-Walker approach, (ii) the ordinary least squares (OLS),
and the Levinson-Durbin recursion method. The package &lt;em&gt;statsmodels&lt;/em&gt; implements
all those methods and a few more (see &lt;em&gt;statsmodels&lt;/em&gt; &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.pacf.html&#34;&gt;docs&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take the previous very simple example and estimate the PCAF for the
time series $ x[t] = 1, 2, 3, 4, 5, 6, 7 $.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; statsmodels.tsa.stattools &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; acf       &lt;span style=&#34;color:#75715e&#34;&gt;# import ACF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; acf(x, nlags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, method&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ols&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;array([ &lt;span style=&#34;color:#ae81ff&#34;&gt;1.00000000e+00&lt;/span&gt;,  &lt;span style=&#34;color:#ae81ff&#34;&gt;1.00000000e+00&lt;/span&gt;,  &lt;span style=&#34;color:#ae81ff&#34;&gt;1.55431223e-15&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3.33333333e-01&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code snippet above, we specified the algorithm for estimating the PCAF
to be the ordinary least squares. It is relatively simple to implement the OLS
method from scratch. We will use the function &lt;code&gt;lagmat&lt;/code&gt; of statsmodels which
creates a $ 2d $ array of lags, and we will separate the zero lag signal,
which is the original data. Finally, we will use the &lt;em&gt;Numpy&lt;/em&gt; function &lt;code&gt;lstsq&lt;/code&gt;
to estimate the parameters of the least squares and thus compute the PACF.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; numpy.linalg &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; lstsq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; statsmodels.tsa.stattools &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; lagmat 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Compute the lags (x_lagged) and keep the original time series in x0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; x_lagged, x0 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lagmat(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, original&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sep&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Prepend x_lagged with a column of ones (add constant)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; x_lagged &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x_lagged &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones((&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)), x_lagged, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The first element of PACF is always 1.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pacf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run the least squares over lags&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; lag &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, lags &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;     params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lstsq(x_lagged[lag:, :(lag&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)], x0[lag:], rcond&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;     pacf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(params[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; print(pacf)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.000000000000001&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.5543122344752192e-15&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.33333333333333415&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Keep in mind that here we used the method of least squares, and if one
wants to use a different approach to estimate the PACF, the values will
differ from the ones in the code snippet above. Moreover, the function
&lt;code&gt;lstsq&lt;/code&gt; receives an argument named &lt;code&gt;rcond&lt;/code&gt; which is a cut-off ratio
for small singular values of &lt;code&gt;x_lagged&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;visualization-of-acf-and-pacf&#34;&gt;Visualization of ACF and PACF&lt;/h2&gt;
&lt;p&gt;Up to now, we have defined ACF and PACF and learned how to estimate them
given a time series. Now we will explore the visualization of ACF and PACF
and learn how to interpret their graphical representations. To better
demonstrate the power of ACF and PACF visualization, we will apply those
two methods to the airport passengers&amp;rsquo; data set (see Figure 1). To simplify
things we will use the functions &lt;code&gt;plot_acf&lt;/code&gt; and &lt;code&gt;plot_pacf&lt;/code&gt; provided by the
Python module &lt;em&gt;statsmodels&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pylab &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Import the plot_acf, and plot_pacf functtions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; statsmodels.graphics.tsaplots &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; plot_acf, plot_pacf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load the data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;genfromtxt(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PATH_TO_FILE/passengers.csv&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fig &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;211&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Call the function for estimating and plotting the ACF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plot_acf(x, ax1, fft&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ticks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ax1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_xticks()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;i&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xticklabels(ticks, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;, weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ticks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ax1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_yticks()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_yticklabels(ticks, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;, weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Autocorrelation (ACF)&amp;#34;&lt;/span&gt;, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;212&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Call the function plot_pacf to estimate and visualize the PACF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We use the method &amp;#34;ols&amp;#34; as we did in a previous example&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plot_pacf(x, ax2, method&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ols&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ticks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ax2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_xticks()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;i&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xticklabels(ticks, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;, weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ticks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ax2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_yticks()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_yticklabels(ticks, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;, weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Partial Autocorrelation (PACF)&amp;#34;&lt;/span&gt;, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After running the script above, we get the plots of ACF and PACF,
as depicted in Figure 2. The left panel shows the ACF, and the right
is the PACF. Blue-shaded areas indicate the confidence interval cone,
and each point outside that cone is considered significant ($ 95 $%
confidence). In simple words, whatever is within that cone is considered
to be zero.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/acf_pacf.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. ACF and PACF on airport passengers data set. Left panel - ACF, right panel PACF.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;We observe that the first component in the ACF and PACF plots equals one,
as we expected (and have already discussed). A second apparent observation
is the reflection of the original&amp;rsquo;s data periodicity in the ACF plot. The
most important part of those graphic representations is the number of
statistically significant correlations (outside the cone). Data points in the ACF
plot reflect the correlation between a data point in our time series and its
past values. Therefore, the number of significant correlations indicates how
far in the past one should look to predict a future point using some
mathematical/statistical model.&lt;/p&gt;
&lt;p&gt;In the following sections, we will learn how to use the ACF and PACF
plots to make assumptions about our models and determine their parameters.&lt;/p&gt;
&lt;h2 id=&#34;what-is-an-arma-and-an-arima-model&#34;&gt;What is an ARMA and an ARIMA model?&lt;/h2&gt;
&lt;p&gt;We will begin our discussion on ARMA and ARIMA models by introducing
the autoregressive (AR) and the moving average (MA) models.&lt;/p&gt;
&lt;h3 id=&#34;autoregressive-model---arp&#34;&gt;Autoregressive Model - AR(p)&lt;/h3&gt;
&lt;p&gt;The AR(p) model is based on linear regression. It assumes that the
current value $ x[t] $ is dependent on previous (past) values
(observations). Due to this ``linear&amp;rsquo;&amp;rsquo; relationship to the past,
AR(p) relies on linear regression. More precisely, it is described
by the following equation:&lt;/p&gt;
&lt;p&gt;$$ x[t] = \sum_{i=1}^{p} \phi [i] x[t-i] + \epsilon [t], \qquad (2)  $$&lt;/p&gt;
&lt;p&gt;where $ \phi[1], \ldots, \phi[p] $ are the parameters of the model,
and $ \epsilon[t] $ is white noise. We will refer to equation (2) as
AR($ p $) or AR of order $p$. The order $ p $ controls how many terms
in the right-hand side of equation (2) will contribute to the estimation
of $ x[t] $.&lt;/p&gt;
&lt;h3 id=&#34;moving-average-model---maq&#34;&gt;Moving Average Model - MA(q)&lt;/h3&gt;
&lt;p&gt;On the other hand, the MA(q) model relies on past error terms to
estimate the current value $ x[t] $, and it is given by&lt;/p&gt;
&lt;p&gt;$$ x[t] = \mu + \sum_{i=1}^{q} \theta[i] \epsilon[t - i] + \epsilon[t], \qquad (3) $$&lt;/p&gt;
&lt;p&gt;where $ \mu $ is the mean of the time series (all observations up to $ t - 1 $),
$ \theta[1], \ldots, \theta[q] $ are the parameters of the model, and
$ \epsilon[t], \epsilon[t-1], \ldots, \epsilon[t-q] $ are white noise error
terms. Because the error terms are white noise, no linearity is involved in
the MA(q) model as in the AR(p) model. The parameter $ q $ is the order of
the MA(q) model and determines how many terms the value $ x[t] $ will depend
on.&lt;/p&gt;
&lt;h3 id=&#34;autoregressive-moving-average---armap-q&#34;&gt;Autoregressive-moving-average - ARMA(p, q)&lt;/h3&gt;
&lt;p&gt;In the case where we combine an AR(p) model with a MA(q), we obtain an
autoregressive-moving average model, ARMA(p, q). In the ARMA(p, q) model,
$ p $ is the autoregressive order, and $ q $ is the order of the moving
average. ARMA(p, q) model is described by:&lt;/p&gt;
&lt;p&gt;$$ x[t] = \epsilon[t] + \sum_{i=1}^{p} \phi[i] x[t-i] + \sum_{i=1}^{q} \theta[i] \epsilon[t-i].  \qquad (4) $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lag (L) and backshift (B) operators&lt;/strong&gt;. In a time series context, the
backshift operator acts on a time series element and returns the previous
element as its name reveals. For instance, if $ x[t] = \begin{Bmatrix} x[1], x[2], \ldots, \end{Bmatrix} $
is a time series then $ Bx[t] = x[t-1] $ for each $ t &amp;gt; 1 $. The lag
operator $ L $ acts in the same way $ Lx[t] = x[t-1] $. Furthermore,
both operators can be raised in integer powers, $ L^k x[t] = t[t-k] $.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another way to write equation (4) is to use the lag operator, and thus
$$ \Big(1 - \sum_{i=1}^{p} \phi[i] L^i \Big) x[t]  =\Big(1 + \sum_{i=1}^{q} \theta[i] L^i \Big) \epsilon[t]. $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Stationarity&lt;/strong&gt; A wide-sense stationary time series has a constant
mean and variance (autocovariance) over time. For instance, the passengers&#39;
data in Figure 1 are non-stationary since an upward trend indicates that
the mean increases over time. However, because the variance does not change,
we will call that time series non-stationary in the sense of mean. You can
You can find more details about stationarity &lt;a href=&#34;https://en.wikipedia.org/wiki/Stationary_process#Weak_or_wide-sense_stationarity&#34;&gt;here&lt;/a&gt;
and in [1].&lt;br&gt;
&lt;strong&gt;Differencing&lt;/strong&gt;
The time series in Figure 1 are non-stationary in the sense of mean (see
&lt;strong&gt;Stationarity&lt;/strong&gt;). One way to eliminate the non-stationarity is to
apply a differencing transform, $ x&amp;rsquo;[t] = x[t] - x[t-1] $ (for instance,
in Python, we could use the &lt;code&gt;diff()&lt;/code&gt; function to apply the differencing
transform on the original time series.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;autoregressive-integrated-moving-average---arimap-d-q&#34;&gt;Autoregressive Integrated Moving Average - ARIMA(p, d, q)&lt;/h3&gt;
&lt;p&gt;ARIMA(p, q, d) is a generalzation of ARMA(p, q) model. ARIMA can be seen as
an improvement of ARMA for time series that show non-stationarity in the sense
of mean. The integrated part of the ARIMA model corresponds to the differencing
of the time series observations. Differencing non-stationary time series can
cancel the non-stationarity (in many time series applications, one has to apply
a differencing transformation on the data to remove the non-stationarity [CITE]).&lt;/p&gt;
&lt;p&gt;$$ x[t] - \alpha[1] x[t-1] - \cdots - \alpha[p] x[t-p] = \epsilon[t] + \theta[1]\epsilon[t-1] + \cdots + \theta[q] \epsilon[t-q], \qquad (5) $$&lt;/p&gt;
&lt;p&gt;and using the lag operator we rewrite equation (5)&lt;/p&gt;
&lt;p&gt;$$ \Big(1 - \sum_{i=1}^{p} \alpha[i] L^i \Big) x[t] = \Big( 1 + \sum_{i=1}^{q} \theta[i] L^i \Big) \epsilon[t]. $$&lt;/p&gt;
&lt;p&gt;The parameters of the autoregressive part are $ \alpha[i] $ and of the moving
average are $ \theta[i] $. $ \epsilon[t] $ are error &lt;em&gt;i.i.d&lt;/em&gt; variables (sampled
from a normal distribution with zero mean).&lt;/p&gt;
&lt;h4 id=&#34;examples&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;Two basic examples of ARIMA models are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The random walk $ x[t] = x[t-1] + \epsilon[t] $ is an ARIMA(0, 1, 0),&lt;/li&gt;
&lt;li&gt;and white noise $ x[t] = \epsilon[t] $ can be described as an ARIMA(0, 0, 0).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-use-acf-and-pacf-in-time-series-analysis&#34;&gt;How to use ACF and PACF in time series analysis&lt;/h2&gt;
&lt;p&gt;In this section, we present several applications of the ACF and PACF. The most
practical applications are the determination of the order of AR(p), MA(q), ARMA(p, q),
and ARIMA(p, d, q) models.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin by examining some basic examples of AR, MA, ARMA, and ARIMA models
and see what conclusions we can draw. For simplicity&amp;rsquo;s sake, we first consider
a signal generated from a normal distribution with zero mean, white noise,
Figure 3A. When we estimate its ACF and PACF (Figure 3B and 3C), we notice that
both are zero (remember that the first component always equals one and can be
ignored). The absence of strong correlations (non-zero elements) indicates that
our data come from a random process.&lt;/p&gt;
&lt;p&gt;The second example demonstrates what information we can obtain from the ACF and
PACF applied to an AR(1) model. Remember that an AR(1) model is an autoregressive
model with order $ p = 1 $, meaning that the data shown in Figure 3D have been
generated by $ x[t] = 0.6 * x[t-1] + \epsilon[t] $, where $ \epsilon[t] $ is
white noise. First, let&amp;rsquo;s take a look at the ACF plot in Figure 3E. We notice
four lags with strong correlations besides the zero lag component, meaning there
is a structure in the data, and they are not completely random. Moreover, we see
that the non-significant components (lags $ k &amp;gt; 1 $) follow a geometric decay.
The PACF plot in panel 3F of Figure 3 has only one strong correlation at lag $ k = 1 $,
verifying that the data come from an AR(1) model.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/acf_examples.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 3. Estimation of ACF and PACF for three models. Each row shows a white noise, an AR(1), and a MA(1) raw data, the ACF, and the PACF. The blue-shaded area indicates the confidence interval cone ($ 95 $% confidence).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Let&amp;rsquo;s look now into a moving average model of order $ p = 1 $. In this case,
the data shown in Figure 3G are described by the equation
$ x[t] = 10 + 0.7 \epsilon[t-1] + \epsilon[t] $, which is MA(1) model. By
examining the ACF plot in Figure 3H, we see strong correlations besides lag
$ k = 0 $, which means that the data come from a process that is not completely
random. In the PACF plot in Figure 3I, several lag components have strong
correlations, and the PACF decays geometrically.&lt;/p&gt;
&lt;p&gt;AThe final example regards a periodic time series with period $ T $. Figure 3J
shows a sinusoidal signal $ g(t) = \sin(2 \pi t \frac{1}{T}) + \epsilon $ with
additive white noise, $ \epsilon $, and $ T = \frac{1}{10} $. Because
there are several components with strong correlations in the ACF and PACF plots,
we know there is some structure in the data. The ACF plot in Figure 3K reflects
the periodicity of the time series. We can infer that the signal&amp;rsquo;s frequency is
$ f = 10 $ (where an entire cycle has been completed). Moreover, the lag component
on the frequency ($ k = 10 $) is the strongest in the graph. Therefore, if we
needed to know the frequency of the raw data without knowing the model, we could
infer the period by looking into the ACF plot.&lt;/p&gt;
&lt;h3 id=&#34;a-few-remarks-on-acf-and-pacf-plots&#34;&gt;A few remarks on ACF and PACF plots&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now draw some conclusions and give some remarks regarding the ACF and
PACF and how to use them to identify an underlying model in raw data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When a time series (or a signal) contains a moving average term (MA),
we can determine the exact order of the MA by counting the components
with strong correlations in an ACF plot. That&amp;rsquo;s true even when the
underlying model is a high-order MA.&lt;/li&gt;
&lt;li&gt;There is an autoregressive term (AR) in the raw data when the PACF plot
has strong correlations up to some lag $ k &amp;gt; 0 $. The number of those
elements determines the order $ p $ of the AR($ p $) model.&lt;/li&gt;
&lt;li&gt;When a strong positive correlation exists and the PACF oscillates to zero,
then there is a moving average, MA, term. The same holds when the zero lag
is negative, and the PACF decays geometrically to zero.&lt;/li&gt;
&lt;li&gt;Another case is when the PACF has strong correlations up to a lag $ k $,
then decays geometrically to zero. The data can be modeled in that case by
an ARMA(p, q) model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-do-we-need-models-like-arp-maq-and-others-&#34;&gt;Why do we need models like AR(p), MA(q), and others ?&lt;/h2&gt;
&lt;p&gt;So far, we have seen the ACF and PACF and how to estimate them given
a time series. Moreover, we have seen how to use the ACF and PACF plots
in determining the parameters of models such as AR(p) or MA(q). However,
there is one question we still need to answer. &lt;em&gt;Why do we need models
like AR(p) in the first place?&lt;/em&gt; The answer is for describing raw data
with a mathematical model that will allow us to make predictions (forecasting)
of future values of our system.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s be more specific. Assume that we obtain some raw data (observations)
like the ones in Figure 1 showing the number of airport passengers per
month for the years $ 1949 - 1960 $. Suppose we would like to know the number
of passengers in the next month or in two months. In other words, we would like
to predict the number of passengers in the future using known historical (past)
observations.&lt;/p&gt;
&lt;p&gt;One way to predict future values would be to assume that the number of
passengers will keep growing following the current trend. However, that
would need to be more accurate since we do not have a model for the
underlying processes that describe our observations. A solution to that
problem would be to apply the analysis we have already used. First, we
would estimate the ACF and PACf and visualize the results. From ACF and
PACF plots, we would find out the type of model (AR(p), MA(q), or other)
that best fits our observations. Once we know that our observations follow
a, let&amp;rsquo;s assume, AR(p) model with order $ p $, we can use that model to
make predictions. More precisely, we could use equation (2) to fit the data
and then use the fitted equation to make predictions for future values.
Below, we give a Python code demonstrating how we can make predictions
using the &lt;em&gt;statsmodels&lt;/em&gt; module.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s create the data set, which follows an AR(1) model. For that
reason, we will use the model in Figure 2D. The equation that describes
the dynamics is $ x[t] = 0.6 * x[t-1] + \epsilon[t] $ (see the section
about the AR(p) model). In a real-life situation, we won&amp;rsquo;t create the data
set; instead, we will have to obtain them somehow, for instance, collect
them by ourselves.&lt;/p&gt;
&lt;p&gt;In the first code snippet, we show how to build the data set from
scratch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pylab &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; statsmodels.tsa.ar_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AutoReg
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seed(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)	&lt;span style=&#34;color:#75715e&#34;&gt;# fix the RNG seed for reproducibility&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create raw data using an AR(1) model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt; 	&lt;span style=&#34;color:#75715e&#34;&gt;# total number of observations&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;]	&lt;span style=&#34;color:#75715e&#34;&gt;# initial value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(N):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The next step is to split the raw data into two separate sets. One
for training (fitting) the parameters of our AR model and one for
testing the fitted model. The training data set is usually larger
than the testing one. Therefore, we use $ 70 $% of the data for
training and the remaining $ 30 $% for testing. Remember that
we have already performed our analysis in Figures 2E and 2F, where
we estimated the ACF and PACF. Thus, we know that the underlying
process for those observations is an AR(1), and we need three
lags to predict the future.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Split the original data set into train/test data sets&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;K &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(N &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;)   &lt;span style=&#34;color:#75715e&#34;&gt;# size of training data set&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[:K]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[K:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fit the AR model using 3 lags (see Figures 2D, E, F)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We use the same AR(1) model. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ARmodel &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoReg(train_data, lags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print a summary of the fitting process&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(ARmodel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, we use the method &lt;code&gt;predict&lt;/code&gt; of the class &lt;code&gt;AutoReg&lt;/code&gt; to predict
five steps into the future (the horizon is five). As we see in Figure 4,
although the prediction is not perfect, it captures the main pattern.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;horizon &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;# How many values into the future we want to predict&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prediction &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ARmodel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(start&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;K, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(K &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; horizon), dynamic&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fig &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;111&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(prediction, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;, label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;prediction&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(test_data[:horizon], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;k&amp;#39;&lt;/span&gt;, label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;target&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/pred_examples.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 4. Prediction of an AR(1) process using *statsmodels* `AutoReg`. The black curve indicates actual observations and the red one is the predictions.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we defined the autocorrelation and the partial autocorrelation
functions, ACF and PACF. We introduced the AR(p), MA(q), ARMA(p, q), and ARIMA(p, d, q)
models, and we showed how one could use the ACF and PACF to determine the
parameters p, q, and d. Moreover, we provided a few examples and Python code
snippets on how to use the &lt;em&gt;statsmodels&lt;/em&gt; &lt;code&gt;acf&lt;/code&gt; and &lt;code&gt;pacf&lt;/code&gt; functions. Finally,
we give a simple example of using models such as AR(p) to make predictions for
future values.&lt;/p&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022acfpacf,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Autocorrelation functions for time series&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/acf_pacf&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;J. Beran, &lt;em&gt;Mathematical Foundations of Time Series Analysis A Concise
Introduction&lt;/em&gt;, Springer, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Time_series&#34;&gt;&amp;ldquo;Time series&amp;rdquo;&lt;/a&gt;, Wikipedia,
Wikimedia Foundation, May 2 2022.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Autocorrelation&#34;&gt;&amp;ldquo;Autocorrelation&amp;rdquo;&lt;/a&gt;, Wikipedia,
Wikimedia Foundation, May 2 2022.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Partial_autocorrelation_function&#34;&gt;&amp;ldquo;Partial autocorrelation function&amp;rdquo;&lt;/a&gt;, Wikipedia,
Wikimedia Foundation, May 2 2022.&lt;/li&gt;
&lt;li&gt;S. Wang, C. Li, and A. Lim, &lt;em&gt;Why Are the ARIMA and SARIMA not Sufficient&lt;/em&gt;, arXiv:1904.07632, 2019.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Time series forecasting error metrics</title>
      <link>https://gdetor.github.io/posts/errors/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/errors/</guid>
      <description>In this post, we are going to explore the basic error measures used in time-series forecasting. Error measures provide a way to quantify the quality of a forecasting algorithm (e.g., the performance). First, we briefly introduce time series and the fundamental terms of forecasting. Second, we will introduce the most commonly used error measures and give some examples. Finally, we provide a complete example of using errors in a real-life forecasting scenario.</description>
      <content>&lt;p&gt;In this post, we are going to explore the basic error measures
used in time-series forecasting. Error measures provide a way
to quantify the quality of a forecasting algorithm (&lt;em&gt;e.g.&lt;/em&gt;, the
performance). First, we briefly introduce time series and the
fundamental terms of forecasting. Second, we will introduce the
most commonly used error measures and give some examples. Finally,
we provide a complete example of using errors in a real-life
forecasting scenario.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-time-series&#34;&gt;What is a time series&lt;/h2&gt;
&lt;p&gt;A time series is a series of data points indexed in time order in
layman&amp;rsquo;s terms [1, 2].
A few examples of time series are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The daily closing price of a stock in the stock market&lt;/li&gt;
&lt;li&gt;The number of air passengers per month&lt;/li&gt;
&lt;li&gt;The biosignals recorded from electroencephalogram or electrocardiogram&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 1 shows the number of air passengers per month from January 1949
to September 1960.  In all the examples in this post, we are going to
use this dataset, so if you would like to try the examples by yourself;
you can download the data from Kaggle
&lt;a href=&#34;https://www.kaggle.com/datasets/rakannimer/air-passengers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/passengers.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. An example of a time series showing the number of air passengers per month.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;A more rigorous definition of a time series found in [1] (Chapter 1, pg 1)
is given below:&lt;/p&gt;
&lt;p&gt;Let $ k \in \mathbf{N}, T \subseteq \mathbf{R} $. A function
$$ x: T \rightarrow \mathbf{R}^k, \hspace{2mm} t \rightarrow x_t $$
or equivalently, a set of indexed elements of $ \mathbf{R}^k $,
$$  { x_t | x_t \in \mathbf{R}^k, \hspace{2mm} t \in T }  $$
is called an observed time series (or time series).
Sometimes, we write $ x_t(t \in T) $ or $ (x_t)_{t\in T} $.&lt;/p&gt;
&lt;p&gt;When $ k = 1 $ the time series is called &lt;em&gt;univariate&lt;/em&gt;, otherwise is
called &lt;em&gt;multivariate&lt;/em&gt;.
$ T $ determines if the time series is [1]:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;discrete&lt;/strong&gt; $ T $ is countable, and $ \forall a &amp;lt; b \in \mathbb{R}: T \cap[a, b] $
is finite,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;continuous&lt;/strong&gt; $ T = [a, b], a &amp;lt; b \in \mathbb{R}, T = \mathbb{R}_{+}
\hspace{2mm} \text{or} \hspace{2mm} T = \mathbb{R} $,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;equidistant&lt;/strong&gt; $ T $ is discrete, and $ \exists u \hspace{2mm} s.t. \hspace{2mm} t_{j+1} - t_j = u $.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;From now on and for simplicity&amp;rsquo;s sake we will use the following notation for
a time series: $ y[1], y[2], \ldots , y[N] $, where
$ N \in \mathbb{N} $ or $ y[t] $, where $t=1, \ldots , N $.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;more-terminology&#34;&gt;More terminology&lt;/h2&gt;
&lt;p&gt;Before we dive into the post, let&amp;rsquo;s give some valuable terminology for the
unfamiliar reader.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Observed data&lt;/strong&gt; ($ (y_t)_{t\in T} $ or $ y[t] $) This is the
data we obtain by observing a system or a process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictive model&lt;/strong&gt; Is a mathematical representation of observed data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt; ($ y[t] $) This is the gound truth signal we use to train
a predictor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Horizon&lt;/strong&gt; ($ h $) Is the number of points or steps we predict in
the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt; ($ \hat{y}[t] = y[t+h] $) This is a value that predictor
returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forecasting&lt;/strong&gt; Is the process of prediction future values from historical
and present data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlier&lt;/strong&gt; It is a significantly different value from other values
in a time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error&lt;/strong&gt; ($ \epsilon[t] $) is the difference between the target signal
and the prediction of our model. The error is given by
$ \epsilon[t] = y[t] - \hat{y}[t] $.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seasonality&lt;/strong&gt; ($ S $) Seasonality is the periodic appearance of specific
patterns over the same periodâ€”for instance, increasing prices before and
over the Christmas holidays.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;four-basic-predictors&#34;&gt;Four basic predictors&lt;/h2&gt;
&lt;p&gt;So far, we have seen what a time series and the basic terminology is.
Now, we will explore some essential predictors or models and see how
we can use them to perform forecasting.&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;ve already seen, a predictor is a statistical (or mathematical)
model that receives as input historical and present data and returns one
(one-step ahead forecasting, $ h = 1$) or multiple future values
(multi-step ahead prediction, $ h &amp;gt; 1 $).
The development of predictors is out of the scope of this post, so we
will not see how to build, train, test/validate, and use a predictor (here
are a few references where the reader can find more details on that matter
[3, 4, 5, 6]). However, we will introduce the four elementary
predictors since some error measures use some of them to estimate the
prediction errors.&lt;/p&gt;
&lt;h4 id=&#34;naive-predictor&#34;&gt;Naive predictor&lt;/h4&gt;
&lt;p&gt;The most straightforward predictor we can imagine is the &lt;em&gt;naive&lt;/em&gt; one,
and it gets the last observed value and returns it as the predicted
value.&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t + h | t] = y[t]. $$&lt;/p&gt;
&lt;h4 id=&#34;seasonal-predictor&#34;&gt;Seasonal predictor&lt;/h4&gt;
&lt;p&gt;We can use the seasonal predictor when we know that our time series
has a seasonal component (seasonality). It is a natural extension of
the naive one, and we can describe it as:&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = y[t+h-S(P+1)]. $$&lt;/p&gt;
&lt;p&gt;$ P $ is $ \Big[\frac{h-1}{S}\Big] $, where $ \Big[ x \Big] $ is the
integer part of $ x $. $ P $ reflects the number of years&amp;ndash;365 days&amp;ndash;
have passed prior to time $ t + h $.&lt;/p&gt;
&lt;h4 id=&#34;average-predictor&#34;&gt;Average predictor&lt;/h4&gt;
&lt;p&gt;This predictor receives historical and present values as input,
computes their average (or mean), and returns it as a prediction.&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = \bar{y} = \frac{1}{N} \sum_{t=1}^{N} y[t] .$$&lt;/p&gt;
&lt;h4 id=&#34;drift-predictor&#34;&gt;Drift predictor&lt;/h4&gt;
&lt;p&gt;Another variation of the naive predictor, only this time
we allow to the predicted value to &lt;em&gt;drift&lt;/em&gt; (fluctuate) over time,&lt;/p&gt;
&lt;p&gt;$$ \hat{y}[t+h|t] = y[t] + \frac{h}{t-1} \sum_{j=2}^{t} (y[j]-y[j-1]) = y[t] + h\Big( \frac{y[t] - y[1]}{t - 1} \Big). $$&lt;/p&gt;
&lt;p&gt;You can picture this predictor as a line drawn from the first
observation to the last one and beyond, where beyond is the prediction.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the predictions in each of the cases mentioned above for
the air passengers data (brown line).
The blue line represents the naive predictor, the green line the
seasonal, the orange line shows the average predictor, and finally,
the pink line indicates the drift predictor.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/naive_predictors.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. Forecasts of montly air passengers. Naive predictor (blue line), naive seasonal (green), average (orange), drift (pink).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;h2 id=&#34;forecasting-error-measures&#34;&gt;Forecasting error measures&lt;/h2&gt;
&lt;p&gt;So why do we need error measures? The general idea is to quantify
the distance between an actual observation (target) and a predicted
one. Particularly when we train a model to learn how to predict
future values, we have to measure the error between the actual
observations and the predicted ones, so the minimization of the
error leads to a better model.&lt;/p&gt;
&lt;p&gt;When we teach a model, we need to use some penalties to help the
model improve the predictions. The error measures listed below do
precisely that. They measure how far the model&amp;rsquo;s predictions are
from the ground truth and penalize the model accordingly. Usually,
the smaller the error, the better the predictor.&lt;/p&gt;
&lt;p&gt;Another reason we need error measures is to evaluate the performance
of our model in real-life scenarios. We might have a trained model
that performs some forecasting, and we would like to investigate the
quality of its predictions. In this case, we can measure the error
between the historical data we will collect in the future and the
model&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;We already said that developing and training a predictor is out of
the scope of the present post. Therefore, we will use historical
data and add some Gaussian noise to them to fake the predictions.
Furthermore, we adopt the discrete-time signals time indexing, meaning
that $ y[t] $ is the value of the time series at time index $ t $.
A similar way would be $ y_t $, where $ t $ is the time index.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt; $ y[t] $ is the target signal, $ \hat{y}[t] $ the prediction
signal and $ \epsilon[t] $ the error signal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And now, we are ready to introduce the error measures and some examples
demonstrating their behavior.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;In the following sections, we use some basic examples to demonstrate how the
reader can implement the error measures in Python.
We provide a custom implementation of the error measure and a Sklearn one in
every case.
We provide a custom implementation of the error measure and a Sklearn one in
every case. The reader should rely more on the Sklearn [7] implementation since
it&amp;rsquo;s generic and optimized. We provide the custom implementation so the reader
can understand better the mathematical formulas.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seed(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_true &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])        &lt;span style=&#34;color:#75715e&#34;&gt;# This is y (target)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2&lt;/span&gt;])      &lt;span style=&#34;color:#75715e&#34;&gt;# This is y_hat (prediction)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mean-absolute-error-mae&#34;&gt;Mean Absolute Error (MAE)&lt;/h3&gt;
&lt;p&gt;The MAE is the most straightforward error measure, and as it&amp;rsquo;s
name signifies it is just the difference between a target (or desired)
value and model&amp;rsquo;s prediction. MAE is defined as:&lt;/p&gt;
&lt;p&gt;$$ \frac{1}{N} \sum_{t=1}^{N} |y[t] - \hat{y}[t]| = \frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] |. $$&lt;/p&gt;
&lt;p&gt;By observing the definition of MAE, we can see that MAE is
scale-dependent, meaning that both signals, target, and prediction,
must be of the same scale. Another drawback of MAE that we can identify
by looking at its definition its sensitivity to outliers (&lt;em&gt;e.g.&lt;/em&gt;,
values in the time series that stick further away from any other value).
Outliers can drag the MAE to higher values, thus affecting the error.
However, there are ways to handle outliers and fix that issue (see here [8, 9]).&lt;/p&gt;
&lt;h4 id=&#34;example-1&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_absolute_error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MAE&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; error &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; N
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MAE(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mean_absolute_error(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mean-absolute-percentage-error-mape&#34;&gt;Mean Absolute Percentage Error (MAPE)&lt;/h3&gt;
&lt;p&gt;MAPE computes the error between a target and a prediction signal
as a ratio of the error $ \epsilon[t] $ and the target signal. More
precisely,&lt;/p&gt;
&lt;p&gt;$$ MAPE = \frac{100\%}{N} \sum_{t=1}^{N} \frac{|y[t] - \hat{y}[t]|}{|y[t]|} = \frac{100}{N} \sum_{t=1}^{N} \frac{| \epsilon[t] |}{| y[t] |}. $$&lt;/p&gt;
&lt;p&gt;MAPE is a helpful error measure when it serves as a loss function
in training and validating a regression model [10]. This error measure is not
susceptible to global scaling of the target signal.&lt;/p&gt;
&lt;p&gt;Again by observing the definition of MAPE above, we can draw some
conclusions about this measure. MAPE can be problematic when the actual
values are zero or close to zero. We can see from the definition above
that when the denominator is close to zero or zero, the MAPE is too
large or cannot be defined. Moreover, MAPE is susceptible to skewness,
since the term $ \frac{1}{y[t]} $ depends only on the observed data
(not on the model&amp;rsquo;s predictions).&lt;/p&gt;
&lt;h4 id=&#34;example-2&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_absolute_percentage_error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MAPE&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;100.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; N) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MAPE(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;19.5555555555555557&lt;/span&gt;                 &lt;span style=&#34;color:#75715e&#34;&gt;# this is because we multiply by 100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mean_absolute_percentage_error(y, y_hat))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.19555555555555554&lt;/span&gt;                 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mean-squared-error-mse&#34;&gt;Mean Squared Error (MSE)&lt;/h3&gt;
&lt;p&gt;The MSE is one of the most used error measures in Machine and
Deep learning. It computes the average of the square of the errors
between target and prediction signals. We define the MSE as:&lt;/p&gt;
&lt;p&gt;$$ MSE = \frac{1}{N} \sum_{t=1}^{N} (y[t] - \hat{y}[t])^2 = \frac{1}{N} \sum_{t=1}^{N} \epsilon[t]^2 . $$&lt;/p&gt;
&lt;p&gt;If we take the square root of $ MSE $, we get the Root MSE or RMSE.
When the MSE is zero, we call the predictor (model) a perfect predictor.
MSE falls into the category of quadratic errors. Quadratic errors
tend to exaggerate the difference between the target and the model&amp;rsquo;s
prediction, rendering them suitable for training models since the
penalty applied to the model will be more prominent when the error
signal is significant [11].&lt;/p&gt;
&lt;p&gt;MSE combines the &lt;em&gt;bias&lt;/em&gt; and the &lt;em&gt;variance&lt;/em&gt; of a prediction. More
precisely, $ MSE = b^2 + Var $, where $b$ is the bias term and
$ Var $ is the variance. The bias reflects the assumptions the
model makes to simplify the process of finding answers. The more
assumptions a model makes, the larger the bias. On the other hand,
variance refers to how the answers given by the model are subject
to change when we present different training/testing data to the
model. Usually, linear models such as &lt;em&gt;Linear Regression&lt;/em&gt; and
&lt;em&gt;Logistic Regression&lt;/em&gt; have high bias and low variance. Nonlinear
models such as &lt;em&gt;Decision Trees&lt;/em&gt;,  &lt;em&gt;SVM&lt;/em&gt;, and &lt;em&gt;kNN&lt;/em&gt; have low
bias and high variance [12]. Ideally, we would like to find a balance
between bias and variance. That&amp;rsquo;s why sometimes we have to penalize
our model during training using regularization techniques (this is
out of the scope of the present post).&lt;/p&gt;
&lt;h4 id=&#34;example-3&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_squared_error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MSE&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; error &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; N
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MSE(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.08333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mean_squared_error(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.08333333333333333&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;symmetric-mean-absolute-percentage-error-smape&#34;&gt;Symmetric Mean Absolute Percentage Error (SMAPE)&lt;/h3&gt;
&lt;p&gt;SMAPE computes the error between the target and the prediction signals
as a ratio of the error with the sum of the absolute values of actual
and prediction values.
The mathematical definition for SMAPE is:&lt;/p&gt;
&lt;p&gt;$$ SMAPE = \frac{100\%}{N} \sum_{t=1}^{N} \frac{| \epsilon [t] |}{(| y[t]| + | \hat{y}[t] |)} $$&lt;/p&gt;
&lt;p&gt;And as we can see from that definition, SMAPE is bounded from
above and below, $ 0 \leq SMAPE \leq 100 $. Another remark we
can make based on  SMAPE&amp;rsquo;s definition: when both a target and
a prediction value are zero, the SMAPE is not defined. If only
the actual or target value is zero, $ SMAPE = 100 $. Finally,
SMAPE can cause troubles when  let&amp;rsquo;s say a prediction is
$ \hat{y}[t] = 10 $ the first time and $ \hat{y}[t] = 12 $ the
second time, while in both cases the target (actual) value is
$ y[t] = 11 $.  In the former case, $ SMAPE  = 4.7 % $ and in
the latter case $ SMAPE = 4.3 % $. We see that we get two different
error values for the same target when our predictor returns
different predictions.&lt;/p&gt;
&lt;h3 id=&#34;mean-absolute-scaled-error-mase&#34;&gt;Mean Absolute Scaled Error (MASE)&lt;/h3&gt;
&lt;p&gt;MASE is a metric that computes the error ratio between the
target and the model&amp;rsquo;s prediction to a naive predictor&amp;rsquo;s error (forecaster).&lt;/p&gt;
&lt;p&gt;The following equation gives the MASE,&lt;/p&gt;
&lt;p&gt;$$ MASE = \frac{\frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] | }{\frac{1}{N-1} \sum_{t=2}^{N} | y[t] - y[t-1] | } $$&lt;/p&gt;
&lt;p&gt;When we are dealing with time series with seasonality with period
$ S $ we can use the following MASE formula instead:&lt;/p&gt;
&lt;p&gt;$$ MASE = \frac{\frac{1}{N} \sum_{t=1}^{N} | \epsilon[t] | }{\frac{1}{N-S} \sum_{t=S+1}^{N} | y[t] - y[t-S] | }. $$&lt;/p&gt;
&lt;p&gt;MASE is scale-invariant, meaning that it&amp;rsquo;s immune to any scaling
we perform on the observed data. MASE is symmetric, which implies
that it penalizes equally the positive and the negative (as well
as big and small) forecast errors. When MASE error is greater than
one, the naive forecaster performs better than our model. MASE can
be problematic only when the actual (target) signal is only zero values.
In that case the naive predictor will be zero ad infinitum and thus
the MASE will be undefined.&lt;/p&gt;
&lt;h3 id=&#34;coefficient-of-determination-cod-or-r&#34;&gt;Coefficient of Determination (CoD) or RÂ²&lt;/h3&gt;
&lt;p&gt;The $ R^2 $ or Coefficient of Determination is an error measure
frequently used in evaluating regression models (&lt;em&gt;goodness of fit&lt;/em&gt;
or &lt;em&gt;best-fit line&lt;/em&gt;). $ R^2 $ counts how many of the target data
points approach the line formed by the regression [11].&lt;/p&gt;
&lt;p&gt;We define $ R^2 $ as&lt;/p&gt;
&lt;p&gt;$$ R^2 = 1 - \frac{\sum_{t=1}^{N}(y[t] - \hat{y}[t])^2 }{\sum_{t=1}^{N}(y[t] - \bar{y})^2} = 1 - \frac{MSE}{Var[y[t]]}, $$&lt;/p&gt;
&lt;p&gt;or alternatively&lt;/p&gt;
&lt;p&gt;$$ R^2 = \frac{SSR}{SST} = \frac{\sum_{t=1}^{N}(y[t] - \hat{y}[t])^2 }{\sum_{t=1}^{N}(y[t] - \bar{y})^2}. $$&lt;/p&gt;
&lt;p&gt;SSR is the sum of squares regression, and SST is the sum of squares
total. SSR represents the total variation of all the predicted values
found on the regression plane from the mean value of all the values
of response variables. SST reflects the total variation of actual
values (targets) from the mean value of all the values of response
variables.&lt;/p&gt;
&lt;p&gt;$R^2$ is bounded from above, $R^2 \leq 1$, since the fraction term
lives always in the interval $ [0, 1] $. In the case of training a
regression model $ R^2 $ is bounded from bellow $ 0 \leq R^2 \leq 1 $.
For the test/validation data, $ R^2 $ can be negative when MSE is
large or the total variance of the target (actual) signal is too
small. A negative $ R^2 $ implies that the term $ \bar{y} $ is a
better predictor than our model. Moreover, from the first definition
of $ R^2 $, we see a direct relation between $ R^2 $ and MSE. While
the $ R^2 $ increases, the MSE tends to approach zero. When we have
an ideal predictor, $ MSE = 0 $ and $ r^2 = 1 $.&lt;/p&gt;
&lt;h4 id=&#34;example-4&#34;&gt;Example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; r2_score
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; var
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;R2&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MSE(y_true, y_pred)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    variance &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; var(y_true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (mse &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; var)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(R2(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9351351351351351&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(r2_score(y_true, y_pred))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9351351351351351&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we briefly introduced the concept of time series and the most
frequently used error measures in forecasting. we described the pros and cons
of each measure so the reader can decide which one best suits their needs.
If you find any typos or errors, or you have any other comments, please feel
free to report them (you can find contact information &lt;a href=&#34;https://gdetor.github.io/about&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022errors-timeseries,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Time series and forecasting error measures&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/errors&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;J. Beran, &lt;em&gt;Mathematical Foundations of Time Series Analysis A Concise
Introduction&lt;/em&gt;, Springer, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Time_series&#34;&gt;&amp;ldquo;Time series&amp;rdquo;&lt;/a&gt;, Wikipedia,
Wikimedia Foundation, May 2 2022.&lt;/li&gt;
&lt;li&gt;A. Nielsen, &lt;em&gt;Practical time series analysis: Prediction with statistics
and machine learning&lt;/em&gt;, O&amp;rsquo;Reilly Media, 2019.&lt;/li&gt;
&lt;li&gt;R. J. Hyndman, and G. Athanasopoulos, &lt;em&gt;Forecasting: principles and practice&lt;/em&gt;,
OTexts, 2018.&lt;/li&gt;
&lt;li&gt;D. Oliveira, &lt;em&gt;Deep learning for time series forecasting&lt;/em&gt;,
&lt;a href=&#34;https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting&#34;&gt;https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting&lt;/a&gt;,
Kaggle, 2019.&lt;/li&gt;
&lt;li&gt;R. Mulla, &lt;em&gt;[Tutorial] TIme series forecasting with XGBoost&lt;/em&gt;,
&lt;a href=&#34;https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost&#34;&gt;https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost&lt;/a&gt;,
Kaggle, 2019.&lt;/li&gt;
&lt;li&gt;Pedregosa, F. et al., &lt;em&gt;Scikit-learn: Machine Learning in Python&lt;/em&gt;,
Journal of Machine Learning Research, 12, 2825&amp;ndash;2830, 2011.&lt;/li&gt;
&lt;li&gt;F. Grubbs, &lt;em&gt;Sample Criteria for Testing Outlying Observations&lt;/em&gt;,
Annals of Mathematical Statistics 21(1):27â€“58, DOI:10.1214/aoms/1177729885, 1950.&lt;/li&gt;
&lt;li&gt;B. Rosner, &lt;em&gt;Percentage Points for a Generalized ESD Many-Outlier Procedure&lt;/em&gt;,
Technometrics 25(2):165â€“172, 1983.&lt;/li&gt;
&lt;li&gt;A. de Myttenaere, B. Golden, B. Le Grand, and F. Rossi, &lt;em&gt;Mean absolute percentage
error for regression models&lt;/em&gt;, Neurocomputing, 2016.&lt;/li&gt;
&lt;li&gt;A. Kumar, &lt;em&gt;Mean squared error or R-squared - Which one to use?&lt;/em&gt;
&lt;a href=&#34;https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/&#34;&gt;https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/&lt;/a&gt;, 2022.&lt;/li&gt;
&lt;li&gt;C. M. Bishop, and N. M. Nasrabadi, &lt;em&gt;Pattern recognition and machine learning&lt;/em&gt;,
New York: Springer, 2006.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Genetic Algorithms &amp; Island Models</title>
      <link>https://gdetor.github.io/posts/genetic_algorithms/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/genetic_algorithms/</guid>
      <description>In this post, we explore genetic algorithms (GAs) and the so-called island model (IM). GAs and the IM are optimization methods used to maximize or minimize a cost function.
What is Optimization? Let&amp;rsquo;s see an example of an optimization problem we all face every day. Let&amp;rsquo;s assume you&amp;rsquo;d like to go and grab a couple of coffee from your favorite coffee shop. Typically, you ask Google to find the fastest way to the store from your current location.</description>
      <content>&lt;p&gt;In this post, we explore genetic algorithms (GAs) and the so-called
island model (IM). GAs and the IM are optimization methods used to
maximize or minimize a cost function.&lt;/p&gt;
&lt;h2 id=&#34;what-is-optimization&#34;&gt;What is Optimization?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see an example of an optimization problem we all face every day. Let&amp;rsquo;s
assume you&amp;rsquo;d like to go and grab a couple of coffee from your favorite coffee
shop. Typically, you ask Google to find the fastest way to the store from your
current location. But let&amp;rsquo;s forget about technology for now.&lt;/p&gt;
&lt;p&gt;You only have a map. Yes, a paper map! They are still around! You first try to
find the shortest paths from your current location to the coffee shop. If
you&amp;rsquo;re a scrooge, you will define the &amp;ldquo;optimal path&amp;rdquo; as &amp;ldquo;the path I consume the
least fuel.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;An optimization problem is a search for the &amp;ldquo;best&amp;rdquo; set of parameters, where
&amp;ldquo;best&amp;rdquo; is defined as the minimum or maximum of some cost function of interest.
Here, that function is our fuel consumption. However, we could have also
designated it to be the distance from start to finish.&lt;/p&gt;
&lt;p&gt;Mathematically speaking the problem of a minimization can be formulated as
follows [1]:
Given a function $f:A\subseteq \mathbb{R} \rightarrow \mathbb{R}$ we are
searching for an element $ {\bf x}^* $ such that
$$ f({\bf x}^*) \leq f({\bf x}) $$&lt;/p&gt;
&lt;p&gt;for all $ {\bf x} \in A $. Similarly, a maximization would be the search for
a ${\bf x}^* $ such that
$$ f({\bf x}^*) \geq f({\bf x}) $$
for all $ {\bf x} \in A $.&lt;/p&gt;
&lt;p&gt;In both cases, we are searching for a global optimum (either a global minimum
or a global maximum). However, it is not always possible to find a global
optimum point in real-life cases. Instead, we can settle for a local minimum or
maximum. For example, that&amp;rsquo;s the compromise we often make when training neural
networks with backpropagation [2].&lt;/p&gt;
&lt;p&gt;Figure 1A shows the global minimum of the cost function, $f(x)=x^2$, with a
magenta color. Figure 1B displays what is known as the Rastrigin function in
one dimension. It is evident that this function has multiple local minima
(&lt;em&gt;e.g.,&lt;/em&gt;, magenta disc)  and maxima (green disc) as well as one global minimum
at $(0, 0)$ (black disc).&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/fun_extremes.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 1. Global and local extremes for (A) $f(x)=x^2$, where the magenta disc indicates the global minimum at $(0, 0)$. (B) For Rastrigin $f(x) = 10 + x^2-10\cos(2\pi x)$, there is a global mimimum at $(0, 0)$ and many local mimima and maxima (for instance see the magenta and green discs, respectively).&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Sometimes, it&amp;rsquo;s easier to solve a minimization problem from a computational
standpoint. In that case, we would minimize the function $ -f $.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-genetic-algorithm&#34;&gt;What is a Genetic Algorithm?&lt;/h2&gt;
&lt;p&gt;A genetic algorithm is an optimization method that mimics evolution to optimize
a cost function. The entire set of the cost function parameters is called the
genome. Each parameter consists of a gene. Because GA mimics how evolution
works, they require a population of individuals. Each individual is nothing
more than a randomly initialized genome. Any GA starts optimizing a cost
function after initializing a population of genomes (individuals).&lt;/p&gt;
&lt;p&gt;In most GA implementations, an individual is a a data structure that holds a
genome (vector of bits, integers, floats, etc.), the corresponding cost to its
genome, a unique ID, a flag indicating whether the current individual is about
to mate (after a selection process), and other relevant information that the
developer deems necessary.&lt;/p&gt;
&lt;p&gt;When the GA optimizes a cost function, it usually applies three basic operators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt; This operator selects two individuals from a population
(&lt;em&gt;i.e.,&lt;/em&gt; a set of many individuals) to mate and eventually procreate. The
selected individuals are called parents. Some selection operators
are k-tournament, roulette-wheel, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Crossover&lt;/strong&gt; This operator mises the genomes of the selected parents. A
crossover operator will combine a part of the first parent&amp;rsquo;s genome with a
part of the second parent&amp;rsquo;s genome. Some crossover operators are one-point
crossover, two-points crossover, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mutation&lt;/strong&gt; Finally, the potential offspring&amp;rsquo;s (or child&amp;rsquo;s) genome is
subject to a mutation, which will further change the offspring&amp;rsquo;s genome.
Some of the most used mutation operators are delta, random, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically, a GA will repeat every generation&amp;rsquo;s operations mentioned above
(another fancy term for iteration). At every iteration, potential offspring
will replace their parents. Usually, the best-performing offspring will replace
the most poor-performing parents in terms of fitness. We call that kind of
replacement &amp;ldquo;elite&amp;rdquo; replacement. Another idea of replacing the parents is
randomly choosing some of the parents and replacing them.&lt;/p&gt;
&lt;p&gt;When the GA exhausts the predefined number of generations, the algorithm
terminates. We can evaluate its performance by inspecting the average fitness
(the average cost function value over all the individuals) or the best-so-far
fitness (BSF). Figure 2A shows a typical example of average fitness and 2B a
BSF. Upon the algorithm&amp;rsquo;s termination, the individual with the best fitness
function provides the genome (set of parameters) optimizes the cost function
[3].&lt;/p&gt;
&lt;h2 id=&#34;how-a-genetic-algorithm-works&#34;&gt;How a Genetic Algorithm works?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use an example to try to understand how GAs work.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s try to solve the XOR problem using a feed-forward neural network.  The
XOR (exclusive OR) is a binary operator that returns true if and only if the
operands are different. This means that if $ X = 1 $ (or $ X = 0 $) and $ Y = 0
$ (or $ Y = 1 $) then $ X \oplus Y = 1 $ (same when ). If both $ X $ and $ Y $
are zeros or ones, then $ X \oplus Y = 0 $.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/xor_net.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 2. XOR neural networ.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Our neural network will consist of two input units (since the XOR operator is a
binary one), two hidden units, and one output unit. Please see &lt;a href=&#34;https://dev.to/jbahire/demystifying-the-xor-problem-1blk&#34;&gt;here&lt;/a&gt; for more details on why we choose such an architecture.
Figure 2 shows the neural network we are about to use.  Let&amp;rsquo;s optimize this
neural network that will serve as an XOR operator. To facilitate the demonstration,
let&amp;rsquo;s split the process into four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Define the cost function and the size of our genome.
Since the input to the XOR function is two-dimensional, we have a
genome of size two. Regarding the cost function, we first define an
error term as
$$ \epsilon = | y_{\text{target} - y_{\text{pred}}} | $$
where $ y_{\text{target}} $ is the true value that the XOR function returns
and $x_{\text{pred}}$ is the value of our network&amp;rsquo;s output. Since we have four
pairs of inputs $ (0, 1), (1, 0), (1, 1), (0, 0) $, and four real outputs
$1, 1, 0, 0$, respectively, we define the cost function as:
$$ f({\bf x}) = \sum_{i=1}^{4} \epsilon_i $$
Ideally, we expect the cost function to be zero to get our
optimal solution. That&amp;rsquo;s the case in Figure 2, where we see the best-so-far
fitness and the average fitness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once we have defined the cost function (that&amp;rsquo;s always the most challenging
part), we determine the number of genes per individual, which is $9$ in
this case. The neural network has the two input units connected via a $ 2
\times 2 $ matrix with the two hidden units. Next, the hidden units
connect via a $ 2\times 1 $ matrix to the output unit. Moreover, the
hidden and output units have a bias term (3 bias terms in total).
Therefore, the entire network has nine parameters (six weights and three
bias terms) we have to optimize. So, we initialize the neural network with
small random weights, define the number of individuals (population size)
to be $ 20 $, and define the size of the genome to be $ 9 $. Remember,
this is the number of cost function parameters to optimize. We set the
number of generations to $ 5000 $,  the number of offspring to $ 10 $, and
the replacement size to $ 5 $, which means $ 5 $ parents will be replaced
by $ 5 $ offsprings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We need to decide what operators we will use for our GA.
In this particular example, we use a k-tournament selection,
a one-point crossover, and a delta mutation operator.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;k-tournament selection&lt;/strong&gt; This operator will randomly choose
$ k $ individuals from the population. It will, then, choose the best
individual, based on the fitness from the tournament with probability
$ p $, choose the second-best individual with probability $ p(1-p) $,
the third-best with probability $ p(1-p)^2 $, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-point crossover&lt;/strong&gt; will take the genes of two parents as input,
it will randomly pick a number from zero to the size of genes, and it
will cut the parents&amp;rsquo; genes at that point. Then it will swap the
sliced genes between the two parents, and thus the offspring will carry
genetic information from both parents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delta mutation&lt;/strong&gt; operator will draw uniformly a number from the
interval $ [0, 1] $ for each gene in an individual&amp;rsquo;s genome. If that
number is greater than a probability $ p $ (usually $ p = \frac{1}{2} $).
A predefined increment will increase the value of the gene.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we set our Optimization in motion. The GA will first evaluate
the cost function of each individual. It will sort the individuals based
on their fitness values. To this end, we feed the XOR input to the neural
network, and we collect the output $ y_{\text{pred}} $. Then we use the
output to compute the cost function value for that individual. The next step
for our GA is to choose two parents based on the k-tournament selection. The
genome of the two selected parents will be crossed over using the
one-point operator. That will give rise to a new genome, a child or
offspring. The offspring&amp;rsquo;s genome will undergo a mutation based on the
delta operator. Finally, the GA will add the offspring to a list.  The
selection, crossover, and mutation processes continue until the number
of offspring have been exhausted. Then, the best performing offspring
will replace the poorest-performing (maximum cost function value)
parents (elite replacement). And the entire process repeats for $ 4999$
more generations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the convergence of the process described in step $ 4 $, the Optimization
of the XOR fitness terminates, and we can inspect the results. Figure 3 shows
the best-so-far fitness (BSF) and the average fitness, respectively. The first
observation is that the cost function value indeed converges to zero. Thus, our
GA&amp;rsquo;s best genome is optimal, and our neural network solves the XOR problem. The
second observation is that we could have used only $500$ generations for this
particular instance. The difficulty of the problem at hand usually determines
the number of generations and the initial values of the genome.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/bsf_avg_fit.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 3. BSF and average fitness for the optimization of the XOR fitness function.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Bellow, you can find the source code of the Python script we used to optimize
the XOR problem. As you can see, we combine Pytorch and PyGAIM to build
a neural network and optimize the weights and the biases.&lt;/p&gt;
&lt;p&gt;The first snippet shows what packages we need to import, the class of the
neural network, and a function that we will use to measure the accuracy
of our optimization process.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pylab &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; random &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; shuffle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; nn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home/gdetorak/packages/gaim/pygaim&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pygaim &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GAOptimize, c2numpy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;XOR_NET&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        super(XOR_NET, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sigmoid()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1(x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2(out)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate the XOR_NET class&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; XOR_NET()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# src: Input XOR&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# tgt: Output XOR&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;src &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;f&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;f&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;accuracy&lt;/span&gt;(genome):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Measures the accuracy of the XOR_NET. Runs over 100 times
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    and compares the network output against the target pattern
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    each time.      
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Assign genomes to network weights&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genome[&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w1)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w2)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(src[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tgt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(dst[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net(inp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(tgt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            count &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# print(y.item(), tgt.item())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; / 100&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; count)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The fitness function takes the genome (C array) as input and
returns the negative of fitness value (or loss) since we perform
a minimization. It presents each time all four XOR patterns to
the neural network and computes the loss for each pattern.
Finally, it sums up all four individual losses and returns the
total loss.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fitness&lt;/span&gt;(x, length):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Fitness function. Receives the genome (x) from GAIM, passes it through
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    net.forward (Pytorch) and computes the absolute loss. 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c2numpy(x, length)				&lt;span style=&#34;color:#75715e&#34;&gt;# Convert C array to Numpy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]						&lt;span style=&#34;color:#75715e&#34;&gt;# First layer weights (2x2)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]					&lt;span style=&#34;color:#75715e&#34;&gt;# First layer bias (2X1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]					&lt;span style=&#34;color:#75715e&#34;&gt;# Second layer weights (2x1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;:]						&lt;span style=&#34;color:#75715e&#34;&gt;# Second layer bias (1x1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Assign the new values to network weights	&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w1)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(w2)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    net&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FloatTensor(b2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;						
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shuffle(index)					&lt;span style=&#34;color:#75715e&#34;&gt;# shuffle the index&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# loop over all four patterns in a random order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; index:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_numpy(src[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tgt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_numpy(dst[idx])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net(inp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; tgt[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; float(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item())			&lt;span style=&#34;color:#75715e&#34;&gt;# return -loss (minimize)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The final snippet provides the code to call the GAOptimize function
of PyGAIM, to plot the results and measure the performance (accuracy)
of the neural network on solving XOR.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    genome_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ga &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GAOptimize(fitness,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_generations&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    population_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    genome_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;genome_size,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_offsprings&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    n_replacements&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[float(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(genome_size)],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[float(&lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(genome_size)],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    mutation_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    mutation_var&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;.1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    genome, _, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ga&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ga&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    test_weights(genome)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;GAIM [4] is a C++ library for genetic algorithms and the island model.
It implements the most fundamental selection, crossover, and mutation
operators. It also provides an MPI and POSIX threads implementation
of the island model. Finally, it comes with a Python interface
called PyGAIM that simplifies GA-based optimization problem setup,
and PyGAIM provides a scikit-learn-like interface.&lt;/p&gt;
&lt;p&gt;For more information about GAIM, its source code, and examples,
you can visit its &lt;a href=&#34;https://github.com/gdetor/gaim&#34;&gt;Github&lt;/a&gt; repository.&lt;/p&gt;
&lt;h2 id=&#34;what-is-an-island-model&#34;&gt;What is an Island Model?&lt;/h2&gt;
&lt;p&gt;An island model is a computational method that runs multiple instances of GAs
on the same optimization problem in a distributed and parallel fashion. In some
cases, each island (another fancy word for process, thread, or computational
node in a cluster) can run a part of the optimization problem [5].&lt;/p&gt;
&lt;p&gt;What is essential in an island model is the periodic exchange of individuals
between islands. According to a predetermined time interval, a migration of a
subpopulation takes place. This circulation of individuals between islands
relies on specific communication protocols and predetermined topologies
(&lt;em&gt;e.g.,&lt;/em&gt;  ring topology). In this case, the islands are connected, forming a
ring, meaning the current island (node) connects to the one on its right (or
left).&lt;/p&gt;
&lt;p&gt;Figure 4 shows three basic topologies, (A) all-to-all, where every island
connects to all other ones, (B) ring, and (C) star topology, where one island
serves as a communication hub [5, 6, 7].&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;https://gdetor.github.io/images/island_model.png&#34;   style=&#34;border-radius: 8px;&#34;  /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Figure 4. Island model topologies. (A) all-to-all, (B) ring, (C) star.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;Each island begins with a population of $N$ individuals where $$ N =
\frac{K}{M} $$ where $ K $ is the number of all individuals, and $M$ is the
number of islands.
Every island will initialize a GA based on all the parameters and procedures
described earlier. When a time counter exhausts, migration takes place. The IM
algorithm selects a subpopulation on each island via some selection method
(random, elite - the best performing individuals, etc.). The selected genomes
move to the neighboring, connected island or islands. The newly arrived ones
replace local individuals via a replacement method (random, poor - the
worst-performing individuals, etc.). IM fills the vacant spots on the source
island with new offspring or randomly generated individuals [6]. The number of
individuals moved at every migration interval is called the &amp;ldquo;migration size.&amp;rdquo;
The reader can refer to [8, 9] for more information on how the migration
interval and the migration size affect the performance of an island model.&lt;/p&gt;
&lt;p&gt;The island model offers a means of faster convergence since each island can
potentially follow a different evolutionary trajectory covering different parts
of the search space. Furthermore, exchanging individuals between islands can
help the overall optimization process avoid being stuck in some local
minimum/maximum. That doesn&amp;rsquo;t mean that the island model is impervious to local
extrema.&lt;/p&gt;
&lt;h2 id=&#34;when-should-we-use-gas&#34;&gt;When should we use GAs?&lt;/h2&gt;
&lt;p&gt;As we have seen, GAs can optimize virtually any function given
a well-defined cost function (and that&amp;rsquo;s the most challenging part of using
GAs). However, there are some cases where we should try to use a GA
instead of any other conventional optimization method. These instances are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the search space is massive, then GAs are suitable for optimizing a
function in that space (for instance, non-linear functions).&lt;/li&gt;
&lt;li&gt;Another issue with Optimization is the cost function. The cost function may
be discontinuous (having gaps), or it may be non-differentiable. GAs do not
use derivatives and are therefore immune to such issues.&lt;/li&gt;
&lt;li&gt;When a cost function is too complex, GAs have more chances than the vanilla
optimization methods to avoid local minima and correctly find the global
optimum. A genetic algorithm can simultaneously explore the search space in
multiple directions, even if some offspring will never discover an optimal
solution to the problem.&lt;/li&gt;
&lt;li&gt;Finally, GAs are agnostic to the problem at hand. They do not require any
information about the system or the function they optimize to solve the
optimization problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-are-some-of-the-gas-applications&#34;&gt;What are some of the GAs applications?&lt;/h2&gt;
&lt;p&gt;Here, you can find some of the GA&amp;rsquo;s applications in real life. Although the
the following list is not complete; you will glimpse where and how GAs are used.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; As discussed in this post, GAs can optimize almost any function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; GAs can tune ML/DL models to discover optimal neural
network parameters. Moreover, they can design neural networks (searching
for the optimal neural network topology [10]).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Path and trajectory planning&lt;/strong&gt; GAs can aid in the designing and planning
of paths and trajectories for autonomous robotic platforms, vehicles, or
manipulators, such as robotic arms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNA Analysis&lt;/strong&gt; GAs can analyze the structure of DNA samples.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finance&lt;/strong&gt; GAs are an excellent tool for analyzing and forecasting stock prices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aerospace engineering&lt;/strong&gt; GAs can aid in the process of designing
aircraft.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Traveling salesman problem (TSP)&lt;/strong&gt; The
&lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;TSP&lt;/a&gt; is
well-defined in combinatorial Optimization and has many applications in
real-life issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;cited-as&#34;&gt;Cited as:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@article{detorakis2022geneticalg,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  title   = &amp;#34;Genetic algorithms and island models&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  author  = &amp;#34;Georgios Is. Detorakis&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  journal = &amp;#34;gdetor.github.io&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  year    = &amp;#34;2022&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  url     = &amp;#34;https://gdetor.github.io/posts/genetics_algorithms&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;D. A. Pierre, &lt;em&gt;Optimization theory with applications&lt;/em&gt;,
Courier Corporation, 1986.&lt;/li&gt;
&lt;li&gt;I. Goodfellow, B. Yoshua, and A. Courville, &lt;em&gt;Deep learning&lt;/em&gt;,
MIT press, 2016.&lt;/li&gt;
&lt;li&gt;K. De Jong, &lt;em&gt;Evolutionary computation&lt;/em&gt;, Wiley Interdisciplinary Reviews:
Computational Statistics, 2009, 1.1: 52-56.&lt;/li&gt;
&lt;li&gt;G. Is. Detorakis and A. Burton, &lt;em&gt;GAIM: A C++ library for Genetic Algorithms
and Island Models&lt;/em&gt;, Journal of Open Source Software, 2019, 4.44: 1839.&lt;/li&gt;
&lt;li&gt;D. Whitley, S. Rana, and R.B. Heckendorn, &lt;em&gt;The island model genetic
algorithm: On separability, population size and convergence&lt;/em&gt;,
Journal of computing and information technology, 7:1, 33&amp;ndash;47, 1999.&lt;/li&gt;
&lt;li&gt;D. Sudholt, &lt;em&gt;Parallel evolutionary algorithms&lt;/em&gt;, Springer Handbook of
Computational Intelligence, 929&amp;ndash;959, 2015.&lt;/li&gt;
&lt;li&gt;W. N. Martin, J. Lienig, and J. P. Cohoon, &lt;em&gt;Parallel Genetic Algorithms Based
on Punctuated Equilibria&lt;/em&gt;, Handbook of Evolutionary Computation, IOP Publishing group,&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Z. Skolicki, &lt;em&gt;An analysis of island models in evolutionary computation&lt;/em&gt;,
Proceedings of the 7th annual workshop on Genetic and evolutionary computation,
386&amp;ndash;389, 2005.&lt;/li&gt;
&lt;li&gt;Z. Skolicki, and K. De Jong, &lt;em&gt;The influence of migration sizes and intervals
on island models&lt;/em&gt;, Proceedings of the 7th annual conference on Genetic and
evolutionary computation, 1295&amp;ndash;1302, 2005.&lt;/li&gt;
&lt;li&gt;K. O. Stanley, and R. Miikkulainen, &lt;em&gt;Efficient evolution of neural network
topologies&lt;/em&gt;, Proceedings of the 2002 Congress on Evolutionary Computation.
2, 1757&amp;ndash;1762, 2002.&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Tips &amp; Tricks (Linux/Vim/Git/Programming)</title>
      <link>https://gdetor.github.io/posts/tipsntricks/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/tipsntricks/</guid>
      <description>In this post you can find various simple tricks and tips for Linux, Vim, Git, Python and many other stuff. All the material provided in this page has been suggested by many different sources such as Command Line Magic, Mastering Vim, kernel_perspective, nixCraft, and Linux Today. The material is free and can be redistributed and/or modified. There is no any warranty that they work for you or are suitable to your need.</description>
      <content>&lt;p&gt;In this post you can find various simple tricks and tips for Linux, Vim,
Git, Python and many other stuff. All the material provided in this page has
been suggested by many different sources such as
&lt;a href=&#34;https://twitter.com/climagic&#34;&gt;Command Line Magic&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/MasteringVim&#34;&gt;Mastering Vim&lt;/a&gt;,
&lt;a href=&#34;https://twitter.com/unix_byte&#34;&gt;kernel_perspective&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/nixcraft&#34;&gt;nixCraft&lt;/a&gt;,
and &lt;a href=&#34;https://twitter.com/linuxtoday&#34;&gt;Linux Today&lt;/a&gt;. The material is free and can
be redistributed and/or modified. There is no any warranty that
they work for you or are suitable to your need. The author of this page is
not responsible for any damage this material may cause.&lt;/p&gt;
&lt;h3 id=&#34;copymove-a-file-into-the-directory-you-were-just-in-linux&#34;&gt;Copy/move a file into the directory you were just in (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ cp &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;or mv&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; FILENAME &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$OLDPWD&lt;span style=&#34;color:#e6db74&#34;&gt;/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;find-big-files-linux&#34;&gt;Find big files (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ find / -type f -size +XXXM    &lt;span style=&#34;color:#75715e&#34;&gt;# where XXX is the desired size in MB&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;detect-any-vulnerabilities-on-your-system-linux&#34;&gt;Detect any vulnerabilities on your system (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ grep -r . /sys/devices/system/cpu/vulnerabilities/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;remove-duplicates-from-pythons-list-python&#34;&gt;Remove duplicates from Python&amp;rsquo;s list (Python)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;collection &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;collection &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(set(collection))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(collection)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;merge-two-dictionaries-in-python-python&#34;&gt;Merge two dictionaries in Python (Python)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dict1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dict2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;dict1, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;dict2}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;filter-a-list-in-python-python&#34;&gt;Filter a list in Python (Python)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mylist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;filtered_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(filter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, mylist))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(filtered_list)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;sort-a-python-dictionary-by-values&#34;&gt;Sort a Python dictionary by values&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grades &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Bob&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3.5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Alice&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Foo&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2.2&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sorted_grades &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {k:v &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(grades&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items(), key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(sorted_grades)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Foo&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2.2&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Bob&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3.5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Alice&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;sort-a-python-dictionary-by-keys&#34;&gt;Sort a Python dictionary by keys&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;grades &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Bob&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3.5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Alice&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Foo&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2.2&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sorted_grades &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {key:grades[key] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(grades&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keys())}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(sorted_grades)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Alice&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;, Bob&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3.5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Foo&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2.2&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;map-filter-and-reduce-python&#34;&gt;Map, Filter, and Reduce (Python)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# MAP&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;square&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; map(square, x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# res = map(lambda y:y*y, x)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(res)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;36&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# FILTER&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;less_than_10&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(filter(less_than_10, x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(res)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# REDUCE&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addition&lt;/span&gt;(x,y):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; functools &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; reduce
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; reduce(addition, x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(res)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;python-string-methods-python&#34;&gt;Python string methods (Python)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello, World!&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isalpha())    &lt;span style=&#34;color:#75715e&#34;&gt;# check if all characters are letters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hi&amp;#34;&lt;/span&gt;))    &lt;span style=&#34;color:#75715e&#34;&gt;# replace Hello with Hi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hi, World&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;World&amp;#34;&lt;/span&gt;))    &lt;span style=&#34;color:#75715e&#34;&gt;# count the occurrences of word World&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;find(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;!&amp;#34;&lt;/span&gt;))     &lt;span style=&#34;color:#75715e&#34;&gt;# return the position of character &amp;#39;!&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper())       &lt;span style=&#34;color:#75715e&#34;&gt;# capitalize all characters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;HELLO, WORLD&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower())       &lt;span style=&#34;color:#75715e&#34;&gt;# convert all characters to lower-case&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hello, world&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;reboot-a-linux-system-directly-to-the-firmware-setup-menu-linux&#34;&gt;Reboot a Linux system directly to the firmware setup menu (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ systemctl reboot --firmware-setup
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;joining-dictinary-keys-and-values-to-a-list-in-python-programming&#34;&gt;Joining dictinary keys and values to a list in Python (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{!s}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{!r}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(key, val) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (key, val) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;check-the-health-status-of-your-hard-drive-linux&#34;&gt;Check the health status of your hard drive (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ badblocks -w -s -o error.log /dev/sdX     &lt;span style=&#34;color:#75715e&#34;&gt;# get X using lsblk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-load-reload-modules-ipython&#34;&gt;How to load (reload) modules (iPython)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;load_ext autoreload
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;autoreload &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;power-off-an-external-hard-drive-linux&#34;&gt;Power-off an external hard drive (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ udiskctl power-off -b /dev/sdX    &lt;span style=&#34;color:#75715e&#34;&gt;# obtain X using lsblk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rsync-your-data-linux&#34;&gt;rsync your data (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ rsync -avzhP src dst --exclude-from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;file.txt --inlude&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.git/config&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;recover-data-linux&#34;&gt;Recover data (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ddrescue -dr3 /dev/sdX imagename.image logfile
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;random-execution-of-a-linux-command-linux&#34;&gt;Random execution of a Linux command (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;$((&lt;/span&gt;RANDOM &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;))&lt;/span&gt; -eq &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; command
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;open-a-file-in-vim-with-no-compatibility-vim&#34;&gt;Open a file in VIM with no compatibility (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ vim -u NONE -u NORC fname
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;io-devices-latency-linux&#34;&gt;IO devices latency (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo ioping -R /dev/sdX         &lt;span style=&#34;color:#75715e&#34;&gt;# (seek rate)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo ioping -RL /dev/sdX        &lt;span style=&#34;color:#75715e&#34;&gt;# (sequential speed)    &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;copy-a-file-to-the-clipboard-linux&#34;&gt;Copy a file to the clipboard (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ xclip -sel clip &amp;lt; file
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;import-date-in-vim-vim&#34;&gt;Import date in VIM (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-vim&#34; data-lang=&#34;vim&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;:&lt;span style=&#34;color:#a6e22e&#34;&gt;read&lt;/span&gt; !&lt;span style=&#34;color:#a6e22e&#34;&gt;date&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;import-the-output-of-a-linux-command-in-vim-vim&#34;&gt;Import the output of a Linux command in Vim (Vim)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-vim&#34; data-lang=&#34;vim&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; @&lt;span style=&#34;color:#a6e22e&#34;&gt;a&lt;/span&gt;=&lt;span style=&#34;color:#a6e22e&#34;&gt;system&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;In&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;press&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Ctrl&lt;/span&gt;+&lt;span style=&#34;color:#a6e22e&#34;&gt;R&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;then&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;convert-a-bunch-of-png-files-to-a-video-linux&#34;&gt;Convert a bunch of png files to a video (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ffmpeg -framerate 1/5 -patter_type glob -i &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.png&amp;#34;&lt;/span&gt; -vf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fps=95,format=yuv420p&amp;#34;&lt;/span&gt; output.mp4
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;convert-a-photo-into-ascii-art-linux&#34;&gt;Convert a photo into ASCII art (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ jp2a photo.jpg | tee photo.ascii
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;read-gz-files-without-extracting-them-linux&#34;&gt;Read .gz files without extracting them (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ zmore file.gz
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ zless file.gz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;get-memory-information-linux&#34;&gt;Get memory information (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo lshw -C memory -short
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ sudo dmidecode -t memory
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-load-huge-files-in-python-programming&#34;&gt;How to load huge files in Python (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mmap
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HUGE.txt&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mmap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mmap(f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filend, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, access&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mmap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ACCESS_READ, offset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mm[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;:])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;git-credential-storage-git&#34;&gt;Git credential storage (Git)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git config --global credential.helper cache &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--timeout S&amp;#39;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# S -&amp;gt; seconds&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;include-a-help-flag-in-makefiles-programming&#34;&gt;Include a help flag in Makefiles (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-make&#34; data-lang=&#34;make&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;.PHONY&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    help
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;help&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cat makefile | grep -oP &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;^#\K(.*)&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;copy-a-file-into-multiple-directories-linux&#34;&gt;Copy a file into multiple directories (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo dir1 dir2 | xargs -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; cp file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;parallel cp file ::: dir1 dir2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;how-to-run-a-elf-programming&#34;&gt;How to run a ELF (Programming)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ touch file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ cp /bin/ls .
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ chmod -x ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;-bash: ./ls: Permission denied
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ strings ./ls | head -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/lib64/ld-linux-x86-64.so.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ /lib64/ld-linux-x86-64.so.2 ./ls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file ls
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;set-an-alert-message-when-a-job-finishes-linux&#34;&gt;Set an alert message when a job finishes (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt; execute_something_that_takes_time; xmessage DONE; &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&amp;amp;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;dennis-ritchie-linuxprogrammin&#34;&gt;Dennis Ritchie (Linux/Programmin)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ curl -L git.io/unix
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;play-tetris-linux&#34;&gt;Play tetris (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ssh netris&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;rocketnine&lt;span style=&#34;color:#ae81ff&#34;&gt;\.&lt;/span&gt;space
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;python-scope-programming&#34;&gt;Python scope (Programming)&lt;/h3&gt;
&lt;p&gt;LEGB:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;ocal (within a function)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;nclosing functions locals&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G&lt;/strong&gt;lobal (module)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;uilt-in (python) preassigned names&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c-pointers-programming&#34;&gt;C Pointers (Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ptr++ Evaluates ptr and then increments ptr by 1xbase_type_size&lt;/li&gt;
&lt;li&gt;*ptr++ Evaluates ptr, increments it, and dereferences the evaluated value&lt;/li&gt;
&lt;li&gt;int *ptr[10] Array of 10 pointers to integer (int *)&lt;/li&gt;
&lt;li&gt;int (*ptr)[10] Pointer to an array of 10 integers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;delete-the-last-column-of-a-text-file-linux&#34;&gt;Delete the last column of a text file (Linux)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NF {NF-=1};1&amp;#39;&lt;/span&gt;&amp;lt; input_file &amp;gt; output_file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NF {NF--};1&amp;#39;&lt;/span&gt;&amp;lt; input_file &amp;gt; output_file
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;automatic-variables-in-make-programming&#34;&gt;Automatic variables in Make (Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$@: The target filename.&lt;/li&gt;
&lt;li&gt;$*: The target filename without the file extension.&lt;/li&gt;
&lt;li&gt;$&amp;lt;: The first prerequisite filename.&lt;/li&gt;
&lt;li&gt;$^: The filenames of all the prerequisites, separated by spaces, discard duplicates.&lt;/li&gt;
&lt;li&gt;$+: Similar to $^, but includes duplicates.&lt;/li&gt;
&lt;li&gt;$?: The names of all prerequisites that are newer than the target, separated by spaces.&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Simple mathematical tricks in Python</title>
      <link>https://gdetor.github.io/posts/math_tips/</link>
      <pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/math_tips/</guid>
      <description>In this post, you can find some helpful mathematical tips and tricks in Python programming language.
Positive definite matrix Check if a given matrix $ \bf{A} $ is positive definite. If all the eigenvalues of matrix $ \bf{A} $ are positive then the matrix is positive definite.
$ A = np.array([[1, 2], [2, 1]]) $ print(A) [[1 2] [2 1]] $ np.all(np.linalg.eigvals(A) &amp;gt; 0) False # A is not a positive definite matrix $ A = np.</description>
      <content>&lt;p&gt;In this post, you can find some helpful mathematical tips and tricks
in Python programming language.&lt;/p&gt;
&lt;h3 id=&#34;positive-definite-matrix&#34;&gt;Positive definite matrix&lt;/h3&gt;
&lt;p&gt;Check if a given matrix $ \bf{A} $ is positive definite. If all the eigenvalues
of matrix $ \bf{A} $ are positive then the matrix is positive definite.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; print(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;all(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;eigvals(A) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;       &lt;span style=&#34;color:#75715e&#34;&gt;# A is not a positive definite matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[[ &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; [&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; [ &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;all(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;eigvals(A) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# A is positive definite&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;random-matrix-with-predetermined-condition-number&#34;&gt;Random matrix with predetermined condition number&lt;/h3&gt;
&lt;p&gt;You can generate a random matrix with predetermined condition number by
following method:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; cond &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; n, m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, (m, n))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; print(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[[ &lt;span style=&#34;color:#ae81ff&#34;&gt;0.24143692&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.61944458&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; [ &lt;span style=&#34;color:#ae81ff&#34;&gt;0.49427012&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1.34003024&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; [&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.08271826&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.91021725&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(m, n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; U, S, V &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;svd(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; S &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; S[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; ((cond &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; cond) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (S[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; S) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (S[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; S[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; SMAT &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((m, n), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;complex) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-9&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; smat[:k, :k] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;diag(S)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; U &lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt; (SMAT &lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt; V&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Desired condition number: &amp;#34;&lt;/span&gt;, cond)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Desired condition number: &lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Actual condition number&amp;#34;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cond(B))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Actual condition number: &lt;span style=&#34;color:#ae81ff&#34;&gt;2.9999999999999973&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;integer-operations&#34;&gt;Integer operations&lt;/h3&gt;
&lt;p&gt;Fast integer division by two (rounded down). In this case, we have to
perform a bit shift to the right by $ k $. $ k $ indicates the power of two
$ 2^k $.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# n &amp;gt;&amp;gt; k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Fast integer multiplication with two by left-bit-shift.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# n &amp;lt;&amp;lt; k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Check if an integer $ n $ is even or odd by performing a binary and operation.
If the result of the following operation is 0, then n is even, otherwise is
an odd number.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# n &amp;amp; 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can find the maximum power-of-two that divides an integer $ n $ by
performing an and operation between the integers $ n $ and its additive inverse
$ -n $.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# -n &amp;amp; n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Research Demos</title>
      <link>https://gdetor.github.io/posts/demos/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/demos/</guid>
      <description>The following videos demonstrate a computation model of primary somatosensory cortex undergoing self-organization. The model relies on Neural Field [1]. The complete mathematical/computational model as well as all the details and results are given in [2] and [3].
Self-organization of receptive fields (RFs) Evolution of a single RF during self-organization Evolution of multiple RFs during self-organization References Dynamics of pattern formation in lateral-inhibition type neural fields A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance and Reorganization of Ordered Topographic Maps Structure of receptive fields in a computational model of area 3b of primary sensory cortex </description>
      <content>&lt;p&gt;The following videos demonstrate a computation model of primary somatosensory
cortex undergoing self-organization. The model relies on Neural Field [1].
The complete mathematical/computational model as well as all the details
and results are given in [2] and [3].&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;self-organization-of-receptive-fields-rfs&#34;&gt;Self-organization of receptive fields (RFs)&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/JU0PKFpagUo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;evolution-of-a-single-rf-during-self-organization&#34;&gt;Evolution of a single RF during self-organization&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UzosJK8YOU0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;evolution-of-multiple-rfs-during-self-organization&#34;&gt;Evolution of multiple RFs during self-organization&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/F8JOQs2MYN4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF00337259&#34;&gt;Dynamics of pattern formation in lateral-inhibition type neural fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance and Reorganization of Ordered Topographic Maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/full&#34;&gt;Structure of receptive fields in a computational model of area 3b of primary sensory cortex&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Tweets</title>
      <link>https://gdetor.github.io/posts/news/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/posts/news/</guid>
      <description>&amp;quot;I, Pencil&amp;quot; An essay published in 1958 which beautifully describes the amount of human coordination, cooperation and trade that needs to happen for the creation of a seemingly simple object, a lead pencil. âœï¸
It&amp;#39;s a great read: https://t.co/P4Ha7HpoGp pic.twitter.com/vPPZuCZ5cB
&amp;mdash; Fermat&amp;#39;s Library (@fermatslibrary) April 6, 2021 </description>
      <content>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;I, Pencil&amp;quot; &lt;br&gt;&lt;br&gt;An essay published in 1958 which beautifully describes the amount of human coordination, cooperation and trade that needs to happen for the creation of a seemingly simple object, a lead pencil.  âœï¸&lt;br&gt;&lt;br&gt;It&amp;#39;s a great read: &lt;a href=&#34;https://t.co/P4Ha7HpoGp&#34;&gt;https://t.co/P4Ha7HpoGp&lt;/a&gt; &lt;a href=&#34;https://t.co/vPPZuCZ5cB&#34;&gt;pic.twitter.com/vPPZuCZ5cB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Fermat&amp;#39;s Library (@fermatslibrary) &lt;a href=&#34;https://twitter.com/fermatslibrary/status/1379424914445377536?ref_src=twsrc%5Etfw&#34;&gt;April 6, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</content>
    </item>
    
    <item>
      <title></title>
      <link>https://gdetor.github.io/staticart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/staticart/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>About</title>
      <link>https://gdetor.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/about/</guid>
      <description>Hi, my name is Georgios Is. Detorakis, and Iâ€™m a computational neuroscientist (ðŸ§ ) and machine learning engineer (ðŸ¤–). On this website, you can find information about my research, any software I have developed, and some artwork related to neuroscience I have created over the years.
Currently, I am working as a full-time machine learning engineer developing algorithms for time-series analysis and forecasting (with applications in biosignals and financial data).
Here you can find my complete academic CV, industry CV (or a less extensive rÃ©sumÃ©).</description>
      <content>&lt;p&gt;Hi, my name is Georgios Is. Detorakis, and Iâ€™m a computational neuroscientist (ðŸ§ )
and machine learning engineer (ðŸ¤–). On this website, you can find information about
my research, any software I have developed, and some artwork related to
neuroscience I have created over the years.&lt;/p&gt;
&lt;p&gt;Currently, I am working as a full-time machine learning engineer developing
algorithms for time-series analysis and forecasting (with applications in
biosignals and financial data).&lt;/p&gt;
&lt;p&gt;Here you can find my complete &lt;a href=&#34;https://gdetor.github.io/cv/gid_academic_cv.pdf&#34;&gt;academic CV&lt;/a&gt;, &lt;a href=&#34;https://gdetor.github.io/cv/gid_industry_cv.pdf&#34;&gt;industry CV&lt;/a&gt;
(or a less extensive &lt;a href=&#34;https://gdetor.github.io/cv/gid_resume.pdf&#34;&gt;rÃ©sumÃ©&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;You can email me at g d e t o r _ a t _ g m a i l _ c o m&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
     Live long and prosper ðŸ–– 
&lt;/div&gt;

</content>
    </item>
    
    <item>
      <title>Artwork</title>
      <link>https://gdetor.github.io/artwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/artwork/</guid>
      <description> Basal Ganglia Circuitry Extracellular Recordings Neural Populations Closed-loop DBS on Basal Ganglia Crossbar Array Ball and Stick Neuron&amp;rsquo;s Model Topological Data Analysis Topologies XOR Neural Network </description>
      <content>&lt;!-- raw HTML omitted --&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Basal Ganglia Circuitry&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Extracellular Recordings&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Neural Populations&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/bg.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/bg.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/extracellular.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/extracellular.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/np.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/np.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Closed-loop DBS on Basal Ganglia&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Crossbar Array&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Ball and Stick Neuron&amp;rsquo;s Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/closed_dbs.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/closed_dbs.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/crossbar.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/crossbar.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/ball-stick.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/ball-stick.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Topological Data Analysis&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Topologies&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;XOR Neural Network&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/persistance.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/persistance_.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/topologies.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/topologies_.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://gdetor.github.io/staticart/img/xor_net.svg&#34;&gt;
  &lt;img src=&#34;https://gdetor.github.io/staticart/img/xor_net_.png&#34;  class=&#34;center&#34;  style=&#34;border-radius:8px&#34;  /&gt;

&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://gdetor.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/publications/</guid>
      <description>Research Articles (peer-reviewed) Neural sampling machine with stochastic synapse allows brain-like learning and inference
S. Dutta, G. Detorakis, A. Khanna, B. Grisafe, E. Neftci, and S. Datta
Nature Communications 13, 2571, 2022, DOI:10.1038/s41467-022-30305-8
[Article]
OpenPelt: Python Framework for Thermoelectric Temperature Control System Development R. Parise and G. Is. Detorakis
The Journal of Open Source Software, 7(73), 4306, DOI:https://doi.org/10.21105/joss.04306
[Article]
Randomized Self Organizing Map
N. P. Rougier and G. Is. Detorakis</description>
      <content>&lt;h2 id=&#34;research-articles-peer-reviewed&#34;&gt;Research Articles (peer-reviewed)&lt;/h2&gt;
&lt;ol start=&#34;40&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural sampling machine with stochastic synapse allows brain-like learning and inference&lt;/strong&gt;&lt;br&gt;
S. Dutta, G. Detorakis, A. Khanna, B. Grisafe, E. Neftci, and S. Datta&lt;br&gt;
Nature Communications 13, 2571, 2022, DOI:10.1038/s41467-022-30305-8&lt;br&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41467-022-30305-8&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenPelt: Python Framework for Thermoelectric Temperature Control System
Development&lt;/strong&gt;
R. Parise and G. Is. Detorakis&lt;br&gt;
The Journal of Open Source Software, 7(73), 4306, DOI:https://doi.org/10.21105/joss.04306&lt;br&gt;
&lt;a href=&#34;https://joss.theoj.org/papers/10.21105/joss.04306&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Randomized Self Organizing Map&lt;/strong&gt;&lt;br&gt;
N. P. Rougier and G. Is. Detorakis&lt;br&gt;
Neural Computation, 33(8), 2021, DOI:https://doi.org/10.1162/neco_a_01406
[Article]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stability analysis of a neural field self-organizing map&lt;/strong&gt;&lt;br&gt;
G. Detorakis, A. Chaillet and N.P. Rougier&lt;br&gt;
The Journal of Mathematical Neuroscience, 10(20), 2020, DOI:https://doi.org/10.1186/s13408-020-00097-6&lt;br&gt;
&lt;a href=&#34;https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00097-6&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GAIM: A C++ library for Genetic Algorithms and Island Models&lt;/strong&gt;&lt;br&gt;
G. Detorakis, A. Burton&lt;br&gt;
The Journal of Open Source Software, 4(44), 1839, 2019, DOI:https://doi.org/10.21105/joss.01839&lt;br&gt;
&lt;a href=&#34;https://www.theoj.org/joss-papers/joss.01839/10.21105.joss.01839.pdf&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Memory-efficient Synaptic Connectivity for Spike-Timing-Dependent Plasticity&lt;/strong&gt;&lt;br&gt;
B. U. Pedroni, S. Joshi, S. Deiss, S. Sheik, G. Detorakis, S. Paul, C. Augustine, E. O. Neftci, G. Cauwenberghs&lt;br&gt;
Frontiers in Neuroscience, DOI: &lt;a href=&#34;https://doi.org/10.3389/fnins.2019.00357&#34;&gt;https://doi.org/10.3389/fnins.2019.00357&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2019.00357/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contrastive Hebbian Learning with Random Feedback Weights&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, E. Neftci&lt;br&gt;
Neural Networks 114, 2019, doi: &lt;a href=&#34;https://doi.org/10.1016/j.neunet.2019.01.008&#34;&gt;https://doi.org/10.1016/j.neunet.2019.01.008&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S089360801930019X&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural and Synaptic Array Transceiver: A Brain-Inspired Computing Framework for Embedded Learning&lt;/strong&gt;&lt;br&gt;
G. Detorakis, S. Sheik, C. Augustine, S. Paul, B.U. Pedroni, N. Dutt, J. Krichmar, G. Cauwenberghs, E. Neftci&lt;br&gt;
Frontiers in Neuroscience, 2018&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2018.00583/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event-Driven Random Back-Propagation: Enabling Neuromorphic Deep Learning Machines&lt;/strong&gt;&lt;br&gt;
E. Neftci, S. Paul, C. Augustine, G. Detorakis&lt;br&gt;
Frontiers in Neuroscience 11, 2017, doi: &lt;a href=&#34;https://doi.org/10.3389/fnins.2017.00324&#34;&gt;https://doi.org/10.3389/fnins.2017.00324&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2017.00324/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust stabilization of delayed neural fields with partial measurement and actuation&lt;/strong&gt;&lt;br&gt;
A. Chaillet, G. Is. Detorakis, S. Palfi and S. Senova&lt;br&gt;
Automatica 83, 2017, doi: &lt;a href=&#34;https://doi.org/10.1016/j.automatica.2017.05.011&#34;&gt;https://doi.org/10.1016/j.automatica.2017.05.011&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0005109817302868&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Closed-loop stimulation of a delayed neural fields model of parkinsonian STN-GPe network: a theoretical and computational study&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis, A. Chaillet, S. Palfi and S. Senova&lt;br&gt;
Frontiers in Neuroscience 9(237), 2015, doi: &lt;a href=&#34;https://doi.org/10.3389/fnins.2015.00237&#34;&gt;https://doi.org/10.3389/fnins.2015.00237&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2015.00237/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structure of Receptive Fields in a Computational Model of Area 3b of Primary Sensory Cortex&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis and N. P. Rougier&lt;br&gt;
Frontiers in Computational Neuroscience, 8, 2014, doi: &lt;a href=&#34;https://doi.org/10.3389/fncom.2014.00076&#34;&gt;https://doi.org/10.3389/fncom.2014.00076&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/full&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Neural Field Model of the Somatosensory Cortex: Formation, Maintenance, and Reorganization of Ordered Topographic Maps&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis and N.P. Rougier&lt;br&gt;
PLoS ONE 7(7):e40257, doi: &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0040257&#34;&gt;https://doi.org/10.1371/journal.pone.0040257&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reproducible-science-articles-peer-reviewed&#34;&gt;Reproducible Science Articles (peer-reviewed)&lt;/h2&gt;
&lt;ol start=&#34;18&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sustainable computational science: the ReScience initiative&lt;/strong&gt;&lt;br&gt;
N.P. Rougier, K. Hinsen, F. Alexandre, T. Arildsen, L. Barba, F.C. Y. Benureau, C. Titus Brown, Pierre de Buyl,
O. Caglayan, A.P. Davison, M.A. Delsuc, G. Detorakis, A.K. Diem, D. Drix, P. Enel, B. Girard, O. Guest, M.G. Hall,
R.N. Henriques, X. Hinaut, K.S. Jaron, M. Khamassi, A. Klein, T. Manninen, P. Marchesi, D. McGlinn, C. Metzner,
O.L. Petchey, H.E. Plesser, T. Poisot, K. Ram, Y. Ram, E. Roesch, C. Rossant, V. Rostami, A. Shifman, J. Stachelek,
M. Stimberg, F. Stollmeier, F. Vaggi, G. Viejo, J. Vitay, A. Vostinar, R. Yurchak, T. Zito&lt;br&gt;
PeerJ Computer Science, 3, e142, 2017&lt;br&gt;
&lt;a href=&#34;https://peerj.com/articles/cs-142/&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Re] A Generalized Linear Integrate-And-Fire Neural Model Produces Diverse Spiking Behaviors&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis&lt;br&gt;
The ReScience Journal, 2017, DOI: &lt;a href=&#34;https://doi.org/10.5281/zenodo.1003214&#34;&gt;https://doi.org/10.5281/zenodo.1003214&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://rescience.github.io/bibliography/detorakis_2017.html&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Re] Multiple dynamical modes of thalamic relay neurons: rhythmic bursting and intermittent phase-locking&lt;/strong&gt;&lt;br&gt;
G. Is. Detorakis&lt;br&gt;
The ReScience Journal, 2:1, 2016, DOI: &lt;a href=&#34;https://doi.org/10.5281/zenodo.61697&#34;&gt;https://doi.org/10.5281/zenodo.61697&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://rescience.github.io/bibliography/detorakis_2016.html&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;international-conferences-peer-reviewed&#34;&gt;International Conferences (peer-reviewed)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inherent Weight Normalization in Stochastic Neural Networks&lt;/strong&gt;&lt;br&gt;
G. Detorakis, S. Dutta, A. Khanna, M. Jerry, S. Datta, and E. Neftci&lt;br&gt;
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.&lt;br&gt;
&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Recurrent Neural Network Based Model of Predictive Smooth Pursuit Eye Movement in Primates&lt;/strong&gt;&lt;br&gt;
H. Kashyap, G. Detorakis, N. Dutt, J. Krichmar,and E. Neftci&lt;br&gt;
IJCNN 2018, Rio de Janeiro, Brazil.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8489652&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incremental Stability of Spatiotemporal Delayed Dynamics and Application to Neural Fields&lt;/strong&gt;&lt;br&gt;
G. Detorakis and A. Chaillet&lt;br&gt;
IEEE CDC 2017, Melbourne, Australia.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8264558&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event-Driven Random Backpropagation: Enabling Neuromorphic Deep Learning Machines&lt;/strong&gt;&lt;br&gt;
E. Neftci, C. Augustine, S. Paul, G. Detorakis,&lt;br&gt;
IEEE ISCAS 2017, Baltimore, MD, USA.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8050529&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forward Table-Based Presynaptic Event-Triggered Spike-Timing-Dependent Plasticity&lt;/strong&gt;&lt;br&gt;
B. U. Pedroni, S. Sheik, S. Joshi, G. Detorakis, S. Paul, C. Augustine, E. Neftci, G. Cauwenberghs,&lt;br&gt;
IEEE BioCAS 2016, Shanghai, China.&lt;br&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7833861&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPySort: Neuronal Spike Sorting with Python&lt;/strong&gt;&lt;br&gt;
C. Pouzat and G.Is. Detorakis,&lt;br&gt;
Euroscipy 2014, Cambridge, United Kingdom.&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/1412.6383&#34;&gt;[Article]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Organizing Dynamic Neural Fields&lt;/strong&gt;&lt;br&gt;
N. P. Rougier and G. Is. Detorakis&lt;br&gt;
3rd International Conference on Cognitive Neurodynamics, Hokkaido, Japan, 2011.&lt;br&gt;
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-007-4792-0_38&#34;&gt;[Article]&lt;/a&gt;
&lt;a href=&#34;https://hal.inria.fr/inria-00587508/document&#34;&gt;[pdf]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;international-conferences&#34;&gt;International Conferences&lt;/h2&gt;
&lt;ol start=&#34;22&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A neural network model of predictive smooth pursuit eye movement in primates&lt;/strong&gt;&lt;br&gt;
H.J. Kashyap, G. Detorakis, N. Dutt, J.L. Krichmar, E. Neftci&lt;br&gt;
SfN, San Diego (CA, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Random Contrastive Hebbian Learning as a Biologically Plausible Learning Scheme&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, and E. Neftci&lt;br&gt;
OCNS, Seattle (WA, USA), 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Three-factor embedded learning on neuromorphic systems&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, R. Parise, S. Sheik, C. Augustine, S. Paul, B. U. Pedroni, N. Dutt, J.Krichmar, G. Cauwenberghs, and E. Neftci&lt;br&gt;
COSYNE, Denver (CO, USA), 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedded Learning on Neuromorphic Systems: Towards a Unified Computing Framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, R. Parise, S. Sheik, C. Augustine, S. Paul, B. Pedroni, N. Dutt, J. Krichmar, G. Cauwenberghs and E. Neftci&lt;br&gt;
NICE, Portland (OR, USA), 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedded learning on neuromorphic systems: Towards a unified computing framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, T. Bartley, R. Parise, C. Augustine, S. Paul, E. Neftci&lt;br&gt;
IEED ICCAD HALO Workshop, Irvine (CA, USA), 2017&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NeuroLachesis: A Neuromorphic Framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, D. Barsever, E. Neftci&lt;br&gt;
Scipy 2017, Austin (TX, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust stabilization of delayed neural fields by proportional feedback using input-to-state stability and small gain theorem&lt;/strong&gt;&lt;br&gt;
A. Chaillet, G. Is. Detorakis, S. Palfi, S. Senova&lt;br&gt;
ICMNS 2016, Juan-les-Pins, France&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Closed-loop disruption of oscillations in a targeted frequency band for a delayed neural field STN-GPe model&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and A. Chaillet&lt;br&gt;
FENS Featured Regional Meeting 2015, Thessaloniki, Greece&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incremental stability of delayed neural fields: a unifying framework for endogenous and exogenous sources of pathological oscillations&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and A. Chaillet&lt;br&gt;
CNS, Prague, Czech Republic, 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Closed-loop regulation of the activity of delayed neural fields with only partial measurement and stimulation&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and A. Chaillet&lt;br&gt;
ICMNS, Antibes - Juan les Pins, France, 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A global stability analysis for delayed neural fields&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis, A. Chaillet and I. Haidar&lt;br&gt;
BCCN 2014, GÃ¶ttingen, Germany&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A computational view of the primary somatosensory cortex&lt;/strong&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
CNS Annual Meeting, Paris, France, 2013&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural Fields and Cortical Plasticity&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
BCCN, Freiburg, Germany, 2011&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;minor-conferences&#34;&gt;Minor Conferences&lt;/h2&gt;
&lt;ol start=&#34;35&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedded learning on neuromorphic systems: Towards a unified computing framework&lt;/strong&gt;&lt;br&gt;
G. Detorakis, C. Augustine, S. Paul, E. Neftci&lt;br&gt;
24th Joint Symposium on Neural Computation, San Diego(CA, USA), 2017&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On the relation between neuronal size and extracellular spike amplitude and its consequence on extracellular recordings interpretation&lt;/strong&gt;&lt;br&gt;
C. Pouzat and G. Is. Detorakis&lt;br&gt;
MathStatNeuro Workshop, Nice(France), 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPySort&lt;/strong&gt;&lt;br&gt;
C. Pouzat and G. Is. Detorakis&lt;br&gt;
GDR Multielectrode systems and signal processing for Neuroscience, Gif-sur-Yvette(France), 2014&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Skin Topographic Maps in SI&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
Progress in Neural Field Theory, Reading, United Kingdom, 2012&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Skin Topographic Maps in SI&lt;/strong&gt;&lt;br&gt;
G.Is. Detorakis and N.P. Rougier&lt;br&gt;
Workshop on Cognitive and Dynamics in Neural Systems: Mathematical and Computational Modeling (CONAS), Lyon, France, 2012&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;book-chapters&#34;&gt;Book Chapters&lt;/h2&gt;
&lt;ol start=&#34;21&#34;&gt;
&lt;li&gt;&lt;strong&gt;ISS-stabilization of delayed neural fields by small-gain arguments&lt;/strong&gt;&lt;br&gt;
A. Chaillet, G. Is. Detorakis, S. Palfi, and S. Senova&lt;br&gt;
Delays and Interconnections: Methodology, Algorithms and Applications, Springer, 2019&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;popular-science&#34;&gt;Popular Science&lt;/h2&gt;
&lt;ol start=&#34;40&#34;&gt;
&lt;li&gt;&lt;strong&gt;Optogenetics to unravel the mechanisms of Parkinsonian symptoms and to optimize deep brain stimulation&lt;/strong&gt;&lt;br&gt;
A. Chaillet, D. Da Silva, G. Detorakis, C. Pouzat, S. Senova&lt;br&gt;
ERCIM News, Special issue on cyber-physical systems, number 97, April 2014&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://gdetor.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/research/</guid>
      <description>Neuromorphic Computing Georgios was involved in developing neuromorphic devices capable of embedded and online learning. He was one of the core developers of the software simulator for the Neural and Synaptic Array Transceiver Framework (in collaboration with Intel Corporation Research Labs, the Universities of California Irvine, and San Diego). He investigated how natural mechanisms of biological brains can lead to more efficient and biologically plausible machine learning algorithms suitable for neuromorphic devices.</description>
      <content>&lt;h2 id=&#34;neuromorphic-computing&#34;&gt;Neuromorphic Computing&lt;/h2&gt;
&lt;p&gt;Georgios was involved in developing neuromorphic devices capable of embedded
and online learning. He was one of the core developers of the software simulator
for the Neural and Synaptic Array Transceiver Framework (in collaboration with Intel
Corporation Research Labs, the Universities of California Irvine, and San Diego).
He investigated how natural mechanisms of biological brains can lead to more
efficient and biologically plausible machine learning algorithms suitable for
neuromorphic devices.&lt;/p&gt;
&lt;h2 id=&#34;parkinsons-disease&#34;&gt;Parkinson&amp;rsquo;s Disease&lt;/h2&gt;
&lt;p&gt;Another research interest is Parkinson&amp;rsquo;s disease (PD). He has combined
neuroscience and control theory to study PD and propose potential treatments.
He has developed computational models based on neural fields theory for the
basal ganglia and PD and has investigated closed-loop deep brain stimulation
(DBS) with applications on PD treatments.&lt;/p&gt;
&lt;h2 id=&#34;cortical-plasticity--self-organizing-maps&#34;&gt;Cortical Plasticity &amp;amp; Self-organizing Maps&lt;/h2&gt;
&lt;p&gt;His research as a Ph.D. student focused on cortical plasticity and self-organization.
He proposed a mathematical/computational model for studying self-organization
in the primary somatosensory cortex. The model relied on the neural field&amp;rsquo;s theory.
Simulation of his model showed how the cerebral cortex builds up topographic maps.
Furthermore, he demonstrated how the brain maintains topographic maps and how they
get reorganized in the face of a lesion.&lt;/p&gt;
&lt;h2 id=&#34;rhythmical-motor-control&#34;&gt;Rhythmical Motor Control&lt;/h2&gt;
&lt;p&gt;Finally, as a Master&amp;rsquo;s student, he studied rhythmical motor control and human
tremor. He has applied signal processing theory and analysis on neurophysiological
signals like EEG and EMG, trying to determine the role of human tremor.
Furthermore, he has investigated Central Pattern Generators (CPGs) and their
role in biped locomotion.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;He also has some experience in:&lt;/p&gt;
&lt;h3 id=&#34;natural-language-processing-nlp&#34;&gt;Natural Language Processing (NLP)&lt;/h3&gt;
&lt;p&gt;Georgios has worked as Data Science Architect for adNomus Inc. focusing
mainly on Natural Language Processing (NLP). He is developing NLP algorithms
with applications on Recommendation Systems and Content Analysis.&lt;/p&gt;
&lt;h3 id=&#34;recording-techniques&#34;&gt;Recording Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Electroencephalogram (EEG) &lt;a href=&#34;https://en.wikipedia.org/wiki/Electroencephalography&#34;&gt;?&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Electromyogram (EMG) &lt;a href=&#34;https://en.wikipedia.org/wiki/Electromyography&#34;&gt;?&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;And minor experience in intracellular and extracellular in vivo recordings &lt;a href=&#34;https://en.wikipedia.org/wiki/Electrophysiology&#34;&gt;?&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Software</title>
      <link>https://gdetor.github.io/software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://gdetor.github.io/software/</guid>
      <description>Miscellaneous omnipyseed Omnipyseed is a tiny and simple Python package that can be used to seed random number generators (RNGs) of Python, Numpy, and Pytorch.
Programming Language: Python Source Code: PyPi Machine/Deep Learning Adversarial Validation This Python class implements the adversarial validation (AV) method. One can use the AV method to test if the train/test data sets come from the same distribution.
Original implementation: http://fastml.com/adversarial-validation-part-one/ Programming Language: Python (Numpy, Sklearn, XGBoost) Source Code: Pytorch TimeseriesLoader A Pytorch Dataset class for time series data sets.</description>
      <content>&lt;h2 id=&#34;miscellaneous&#34;&gt;Miscellaneous&lt;/h2&gt;
&lt;h3 id=&#34;omnipyseed&#34;&gt;omnipyseed&lt;/h3&gt;
&lt;p&gt;Omnipyseed is a tiny and simple Python package that can be used to seed random
number generators (RNGs) of Python, Numpy, and Pytorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/omnipyseed&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.org/project/omnipyseed/&#34;&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machinedeep-learning&#34;&gt;Machine/Deep Learning&lt;/h2&gt;
&lt;h3 id=&#34;adversarial-validation&#34;&gt;Adversarial Validation&lt;/h3&gt;
&lt;p&gt;This Python class implements the adversarial validation (AV) method. One can
use the AV method to test if the train/test data sets come from the same
distribution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original implementation: &lt;a href=&#34;http://fastml.com/adversarial-validation-part-one/&#34;&gt;http://fastml.com/adversarial-validation-part-one/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Numpy, Sklearn, XGBoost)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/adversarial_validation&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pytorch-timeseriesloader&#34;&gt;Pytorch TimeseriesLoader&lt;/h3&gt;
&lt;p&gt;A Pytorch Dataset class for time series data sets. The class provides several
methods for preprocessing raw time series data (Box-Cox transform,
normalization, standardization, mu-law).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Sklearn)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pytorch_timeseries_loader&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-collection&#34;&gt;Time series collection&lt;/h3&gt;
&lt;p&gt;There are many ways to perform time series forecasting. One of the most recent
(modern) ways is to use artificial neural networks (or even deep neural
networks) to model time series and perform predictions usign those models.
The present repository provides three mainstream artificial neural networks,
an LSTM, a MLP, and a TCN for time series forecasting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://gitlab.com/gdetor/time_series_forecasting&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 512 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M105.2 24.9c-3.1-8.9-15.7-8.9-18.9 0L29.8 199.7h132c-.1 0-56.6-174.8-56.6-174.8zM.9 287.7c-2.6 8 .3 16.9 7.1 22l247.9 184-226.2-294zm160.8-88l94.3 294 94.3-294zm349.4 88l-28.8-88-226.3 294 247.9-184c6.9-5.1 9.7-14 7.2-22zM425.7 24.9c-3.1-8.9-15.7-8.9-18.9 0l-56.6 174.8h132z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autoencoders-for-time-series&#34;&gt;Autoencoders for time series&lt;/h3&gt;
&lt;p&gt;Autoencoders are a type of artificial neural networks that can learn efficient
codings of unlabeled data. Autoencoders learn the code by trying to map the
identity function (&lt;strong&gt;e.g.&lt;/strong&gt;, they try to reconstruct the input at their output
layer). This repository contains four types of autoencoders: (i) a standard
linear autoencoder (AE and Variational AE), (ii) an LSTM autoencoder (AE and
VAE), (iii) a convolutional autoencoder, and finally a causal convolutional
autoencoder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib, Sklearn)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/autoencoders_for_time_series&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pytorch-time2vec&#34;&gt;Pytorch-Time2Vec&lt;/h3&gt;
&lt;p&gt;Time2Vec is an algorithm that provides a learning representation of time, which
is model agnostic and can be used to encode temporal dynamics in many different
applications (&lt;em&gt;e.g.&lt;/em&gt;, in Transformers for time series).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://arxiv.org/abs/1907.05321&#34;&gt;&amp;ldquo;Time2Vec: Learning a vector representation of time&amp;rdquo;,
Kazemi et al., arXiv, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pytorch_time2vec&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;neural-sampling-machines&#34;&gt;Neural Sampling Machines&lt;/h3&gt;
&lt;p&gt;NSMs or Neural Sampling Machines is a Pytorch implementation of the algorithms
proposed in&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://papers.nips.cc/paper/2019/hash/dfce06801e1a85d6d06f1fdd4475dacd-Abstract.html&#34;&gt;&amp;ldquo;Inherent Weight Normalization in Stochastic Neural Networks&amp;rdquo;, Detorakis et al.,
NeurIPS 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Pytorch, Numpy, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/nmi-lab/neural_sampling_machines&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;random-contrastive-hebbian-learning-rchl&#34;&gt;Random Contrastive Hebbian Learning (rCHL)&lt;/h3&gt;
&lt;p&gt;rCHL is a learning algorithm that relies on the contrastive Hebbian learning
algorithm. The major difference is that the feedback pathway
does not use any kind of learnable weights. Instead, it exploits fixed random
weights.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S089360801930019X&#34;&gt;&amp;ldquo;Contrastive Hebbian Learning with Random Feedback Weights&amp;rdquo;, Detorakis et al.,
Neural Networks, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C and Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://gitlab.com/gdetor/rCHL&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 512 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M105.2 24.9c-3.1-8.9-15.7-8.9-18.9 0L29.8 199.7h132c-.1 0-56.6-174.8-56.6-174.8zM.9 287.7c-2.6 8 .3 16.9 7.1 22l247.9 184-226.2-294zm160.8-88l94.3 294 94.3-294zm349.4 88l-28.8-88-226.3 294 247.9-184c6.9-5.1 9.7-14 7.2-22zM425.7 24.9c-3.1-8.9-15.7-8.9-18.9 0l-56.6 174.8h132z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;restricted-bolzmann-machine-in-c&#34;&gt;Restricted Bolzmann Machine (in C)&lt;/h3&gt;
&lt;p&gt;Restricted Boltzmann Machine is an artificial neural network with generative
capabilities. Usually, it consists of two layers and can learn a probability
distribution over its set of inputs through a contrastive divergence algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;http://people.stat.sfu.ca/~dac5/BoltzmannMachines.pdf&#34;&gt;&amp;ldquo;Bolzmann Machines&amp;rdquo;, Hinton 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C and Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/cRBM&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;annealed-importance-sampling-pyais&#34;&gt;Annealed Importance Sampling (pyAIS)&lt;/h3&gt;
&lt;p&gt;When one has to compute the partition function Z of a probability distribution
(Z = \sum_{x}f(x), where p(x) = \frac{1}{Z}f(x)) often applies an Importance
Sampling (IS) method. The issue with IS is the choice of its single
hyperparameter, namelly the proposal distribution. Annealed Importance Sampling
overcomes that problem by creating intermediate distributions. AIS does this
by &amp;ldquo;moving&amp;rdquo; the proposal distribution until one gets a fair approximation of the
target (original) distribution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Papers:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/physics/9803008&#34;&gt;&amp;ldquo;Annealed Importance Sampling&amp;rdquo;, R.M. Neal, 1998&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~rsalakhu/papers/bm.pdf&#34;&gt;&amp;ldquo;Learning and Evaluating Boltzmann Machines&amp;rdquo;, R. Salakhutdinov, 2008&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pyAIS&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;self-organizing-maps-dx-dy-representations&#34;&gt;Self-organizing maps Dx-Dy representations&lt;/h3&gt;
&lt;p&gt;SOM-DyDx, is a python script that implements Demartines dy-dx representation
method. This method is useful to inspect if a self-organized map
is well-organized.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &amp;ldquo;Organization measures and representations of the Kohonen maps&amp;rdquo;, P. Demartines, 1992&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Numpy)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/som_quality_measures&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neuromorphic-computing&#34;&gt;Neuromorphic Computing&lt;/h2&gt;
&lt;h3 id=&#34;neural-and-synaptic-array-transceiver-nsat&#34;&gt;Neural and Synaptic Array Transceiver (NSAT)&lt;/h3&gt;
&lt;p&gt;Neural and Synaptic Array Transceiver is a Neuromorphic Computational Framework
facilitating flexible and efficient embedded learning by matching algorithmic
requirements and neural and synaptic dynamics. NSAT supports event-driven
supervised, unsupervised and reinforcement learning algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2018.00583/full&#34;&gt;Neural and Synaptic Array Transceiver: A Brain-Inspired
Computing Framework for Embedded Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C/Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/nmi-lab/NSAT&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nsatcarl&#34;&gt;NSATCarl&lt;/h3&gt;
&lt;p&gt;NSATCarl is a simple C++ library that brings together the Neural and Synaptic
Array Transceiver (NSAT) and the neural simulator &lt;a href=&#34;https://www.socsci.uci.edu/~jkrichma/CARLsim/&#34;&gt;CARLsim&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: C++&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/NSATcarl&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;optimization&#34;&gt;Optimization&lt;/h2&gt;
&lt;h3 id=&#34;genetic-algorithms-and-island-models-gaim&#34;&gt;Genetic Algorithms and Island Models (GAIM)&lt;/h3&gt;
&lt;p&gt;Genetic Algorithms (GAs) are optimization metaheuristics inspired by the theory
of evolution and the process of natural selection. GAs belong to a larger class
of heuristics called evolutionary algorithms (EA). Island Model (IM) is a
collection of algorithms that enable many populations to evolve over the same
optimization problem providing faster convergence to a solution than a simple GA.
Furthermore, IMs can overcome the problem of local minima due to a migration
policy between islands (different populations).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://joss.theoj.org/papers/10.21105/joss.01839&#34;&gt;&amp;ldquo;GAIM: A C++ library for Genetic Algorithms and
Island Models, G. Is. Detorakis and A. Burton, JOSS, 2019&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: C++/Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/gaim&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamical-systems&#34;&gt;Dynamical Systems&lt;/h2&gt;
&lt;h3 id=&#34;neural-field-self-organizing-map-stability-analysis&#34;&gt;Neural Field Self-organizing Map Stability Analysis&lt;/h3&gt;
&lt;p&gt;This project is about establishing a stability condition for a class of
neural fields. The main idea is to provide a stability condition for a
self-organizing map based on neural fields, which allows us to know
&lt;em&gt;a priori&lt;/em&gt; if the learning will converge to a stable map.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00097-6&#34;&gt;&amp;ldquo;Stability analysis of a neural field self-organizing map&amp;rdquo;,
G. Detorakis, A. Chaillet, and N.P. Rougier,
The Journal of Mathematical Neuroscience, 10(1):1-20, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Numpy, Numba, Matplotlib, Sklearn)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/som_stability&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;empirical-dynamic-modeling-empyred&#34;&gt;Empirical Dynamic Modeling (empyred)&lt;/h3&gt;
&lt;p&gt;Empirical Dynamic Modeling (EDM) is an equation-free framework for modeling
non-linear dynamic systems. EDM relies on Takens&amp;rsquo; (1981) theorem of temporal
embeddings and reconstruction of system attractors from temporal embeddings.
&lt;strong&gt;Empyred&lt;/strong&gt; is a pure Python implementation of the EDM methods: Simplex and SMAP.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Papers:
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Nonlinear forecasting as a way of distinguishing chaos from measurement error in time series&amp;rdquo; (Sugihara and May, 1990)&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Nonlinear forecasting for the classification of natural time series&amp;rdquo;, (Sugihara, 1994).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Relevant Material: &lt;a href=&#34;https://deepeco.ucsd.edu/nonlinear-dynamics-research/edm/#page-content&#34;&gt;Sugihara&amp;rsquo;s Lab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python (Numpy, Scipy, Sklearn, Matplotlib)&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/empyred&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neuroscience&#34;&gt;Neuroscience&lt;/h2&gt;
&lt;h3 id=&#34;correlated-spike-trains-corrspiketrains&#34;&gt;Correlated Spike Trains (CorrSpikeTrains)&lt;/h3&gt;
&lt;p&gt;This is a Python implementation of the paper &amp;ldquo;Generation of correlated spike
trains&amp;rdquo;. Both the doubly stochastic processes and the mixture method described
in the original paper are implemented.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf&#34;&gt;&amp;ldquo;Generation of correlated spike trains&amp;rdquo;, R. Brette,
Neural Computation, 2009&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/CorrSpikeTrains&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;spike-sorting---spysort&#34;&gt;Spike sorting - SPySort&lt;/h3&gt;
&lt;p&gt;Spike sorting is a class of algorithms for classifying spikes into clusters
based on a similarity measure. Usually, spike sorting identifies the waveforms
of neural spiking in signals collected from extracellular recordings. SpySort
is a Python package that implements a spike sorting algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://arxiv.org/abs/1412.6383&#34;&gt;&amp;ldquo;SPySort: Neuronal Spike Sorting with Python&amp;rdquo;,
C. Pouzat and G.Is. Detorakis, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/SPySort&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;neural-fields-and-deep-brain-stimulation&#34;&gt;Neural Fields and Deep Brain Stimulation&lt;/h3&gt;
&lt;p&gt;This project regards a deep brain stimulation (DBS) model of the globus pallidus
and the subthalamic nucleus of the basal ganglia. The model explores the effects
of optogenetic DBS stimulation in closed-loop treatments of motor symptoms of
Parkinsonâ€™s disease. The model relies on neural fields and a simple proportional
controller.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2015.00237/full&#34;&gt;&amp;ldquo;Closed-loop stimulation of a delayed neural fields model
of parkinsonian STN-GPe network: a theoretical and computational study&amp;rdquo;,
Detorakis et al, Frontiers in Neuroscience, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/neuralfieldDBSmodel&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;primary-somatosensory-cortex-and-structure-of-its-receptice-fields&#34;&gt;Primary Somatosensory Cortex and Structure of its Receptice Fields&lt;/h3&gt;
&lt;p&gt;This repo contains a computational model of area 3b of the somatosensory cortex.
The model relies on the neural field&amp;rsquo;s theory and reproduces an exact experimental
protocol described in DiCarlo et al., 1998. The model describes the structure
of receptive fields in area 3b of the primary somatosensory cortex and how
attention mechanisms affect that structure. The source code, beyond the model,
provides tools to analyze the results and generate the figures from the original
paper. The model is capable of obtaining similar results as the original experiment
described in DiCarlo et al..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2014.00076/fulL&#34;&gt;&amp;ldquo;Structure of receptive fields in a computational model
of area 3b of primary sensory cortex&amp;rdquo;, G.Is. Detorakis and N.P. Rougier, Frontiers in Neuroscience, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/SI-RF-Structure&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;self-organization-and-sensory-topographic-maps&#34;&gt;Self-Organization and Sensory Topographic Maps&lt;/h3&gt;
&lt;p&gt;A collection of Python scripts implement a self-organization model of the
primary somatosensory cortex. The model relies on the neural field&amp;rsquo;s theory
and describes the dynamics of the primary somatosensory cortex of monkeys.
It forms, maintains, and reorganizes somatosensory topographic maps following
a biologically plausible Oja-like learning rule.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257&#34;&gt;&amp;ldquo;A Neural Field Model of the Somatosensory Cortex:
Formation, Maintenance and Reorganization of Ordered Topographic Maps&amp;rdquo;,
G.Is. Detorakis and N.P. Rougier, PloS ONE, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/SITopMaps&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;numerical-analysis&#34;&gt;Numerical Analysis&lt;/h2&gt;
&lt;h3 id=&#34;pseudospectra-analysis-for-rectangular-matrices-pygpsa&#34;&gt;Pseudospectra Analysis for Rectangular Matrices (pygpsa)&lt;/h3&gt;
&lt;p&gt;Pseudospectra of a matrix (or an operator) is a set containing its spectrum
and the &amp;ldquo;pseudo&amp;rdquo;-eigenvalues. Pseudospectra is particularly useful for
understanding and/or revealing information about non-normal matrices (operators).
A Python script that computes the pseudospectra of a rectangular matrix.
For more information about pseudospectra visit the &lt;a href=&#34;https://www.cs.ox.ac.uk/pseudospectra/&#34;&gt;Pseudospectra Gateway&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original Paper: &lt;a href=&#34;http://people.maths.ox.ac.uk/~trefethen/publication/PDF/2002_101.pdf&#34;&gt;&amp;ldquo;Pseudospectra of rectangular matrices&amp;rdquo;, T.G. Wright
and L.N. Trefethen, IMA Journal of Numerical Analysis, 22, 501&amp;ndash;519, 2002&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Language: Python&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/pygpsa&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;crkmethods&#34;&gt;CRKMethods&lt;/h3&gt;
&lt;p&gt;Runge-Kutta (RK) methods are iterative methods used in the temporal discretization
of (numerical) approximations of Ordinary Differential Equations (ODEs).
CRKMethods is a collection of explicit RK methods: (i) Foreward Eulerâ€™s method,
(ii) RK45, (iii) Refined RK45, and (iv) Fehlbergâ€™s method. Each method is
implemented as a single step, so the end-user has to run over N timesteps to
get the final solution. Furthermore, the end-user is responsible for providing
the time-step (dt). For more details, please have a look at the file &lt;em&gt;main.c&lt;/em&gt; in
the &lt;em&gt;src&lt;/em&gt; directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programming Language: C&lt;/li&gt;
&lt;li&gt;Source Code: &lt;a href=&#34;https://github.com/gdetor/CRKMethods&#34;&gt;&lt;style&gt;
    .inline-svg {
      display: inline-block;
      height: 1.0rem;
      width: 1.0rem;
      top: 0.1rem;
      position: relative;
    }
&lt;/style&gt;

&lt;span class=&#34;inline-svg&#34; &gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 448 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.3 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path fill=&#34;currentColor&#34; d=&#34;M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM277.3 415.7c-8.4 1.5-11.5-3.7-11.5-8 0-5.4.2-33 .2-55.3 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6-14.1 0-28.4 1.9-41.6 5.6 0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 13.9.2 36.5.2 40.6 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3zm-98.1-61.1c-1.9.4-3.7-.4-3.9-1.7-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3zm-9.5-.9c0 1.3-1.5 2.4-3.5 2.4-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4zm-13.7-1.1c-.4 1.3-2.4 1.9-4.1 1.3-1.9-.4-3.2-1.9-2.8-3.2.4-1.3 2.4-1.9 4.1-1.5 2 .6 3.3 2.1 2.8 3.4zm-12.3-5.4c-.9 1.1-2.8.9-4.3-.6-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1zm-9.1-9.1c-.9.6-2.6 0-3.7-1.5s-1.1-3.2 0-3.9c1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1zm-6.5-9.7c-.9.9-2.4.4-3.5-.6-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5zm-6.7-7.4c-.4.9-1.7 1.1-2.8.4-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;


&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
